{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arashfahim/Deep-Schemes-for-Control/blob/main/Dr_Fahim_Second_Approach_(Final_Version).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hhguwHlWj02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1267fa5-a130-48d4-fa4f-5c26ae4cdfc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive,files\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPXG1OIOocCx"
      },
      "source": [
        "# Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoRmv9NiS4nZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import misc\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR, CyclicLR, ReduceLROnPlateau, LinearLR, ExponentialLR\n",
        "import random\n",
        "from torch.autograd.functional import jacobian, hessian\n",
        "# import AUTOGRAD.FUNCTIONAL.JACOBIAN as jacobian\n",
        "import time\n",
        "import math\n",
        "\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rcParams['legend.fontsize'] = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYTV5FY_ol3m"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "r6ZnH8s-lrom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of time intervals"
      ],
      "metadata": {
        "id": "RaezjJgt33pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# num_time_interval=torch.tensor([10]).to(device)    #N\n",
        "# start_list=[4,2,0]\n",
        "# end_list=[6,8,10]"
      ],
      "metadata": {
        "id": "3VbAoZJ8lH2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_time_interval=torch.tensor([20]).to(device)    #N\n",
        "start_list=[4,3,2,1,0]\n",
        "end_list=[6,7,8,9,10]"
      ],
      "metadata": {
        "id": "eC5RgLbqlVzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCjiUfN0oiF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85bf7861-f591-476e-a6b0-57f61ef3af88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0000, 0.0500, 0.1000, 0.1500, 0.2000, 0.2500, 0.3000, 0.3500, 0.4000,\n",
            "        0.4500, 0.5000, 0.5500, 0.6000, 0.6500, 0.7000, 0.7500, 0.8000, 0.8500,\n",
            "        0.9000, 0.9500, 1.0000], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-d5c802e3e3eb>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  delta_t = torch.tensor(T/ num_time_interval).to(device)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# num_sample=torch.tensor([2**14]).to(device) #M\n",
        "# dim=torch.tensor([3]).to(device)          #D\n",
        "\n",
        "# T=torch.tensor([1]).to(device)\n",
        "T=1\n",
        "# T=torch.tensor([1])\n",
        "delta_t = torch.tensor(T/ num_time_interval).to(device)\n",
        "# t_steps = torch.linspace(0,1,num_time_interval).to(device)\n",
        "# sqrt_delta_t = torch.sqrt(delta_t).to(device)\n",
        "# sqrt_delta_t = torch.sqrt(delta_t).to(device)\n",
        "sqrt_delta_t = torch.sqrt(delta_t).to(device)\n",
        "# x_init = torch.zeros(dim)\n",
        "# sigma=torch.sqrt(torch.tensor([2.0]))\n",
        "# lamb=torch.tensor([1.0]).to(device)\n",
        "mu=torch.tensor([1.0]).to(device)\n",
        "sigma=torch.tensor([1.0]).to(device)\n",
        "alpha=torch.tensor([1.0]).to(device)\n",
        "start = torch.tensor([0]).to(device)\n",
        "end = torch.tensor([1]).to(device)\n",
        "num_steps=5\n",
        "num_ite=int(num_time_interval/num_steps)\n",
        "# kappa=torch.tensor([1]).to(device)\n",
        "# theta=torch.tensor([0.4]).to(device)\n",
        "# nu=torch.tensor([0.5]).to(device)\n",
        "# lamb=torch.tensor([0.6]).to(device)\n",
        "# eta=torch.tensor([0.5]).to(device)\n",
        "p=torch.tensor([0.95]).to(device)\n",
        "# rho=torch.tensor([0]).to(device)\n",
        "# num_runs=3\n",
        "# x0=torch.ones([1,dim]).to(device)+4\n",
        "# print(x0)\n",
        "\n",
        "t_steps = torch.linspace(0,T,num_time_interval.item()+1).to(device)\n",
        "print(t_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10a7hGrLUnKm"
      },
      "source": [
        "##$n=1  \\quad (d=2)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhcEi1KtUmUI"
      },
      "outputs": [],
      "source": [
        "# dim=torch.tensor([2]).to(device)\n",
        "# kappa=torch.tensor([1]).to(device)\n",
        "# theta=torch.tensor([0.4]).to(device)\n",
        "# nu=torch.tensor([1]).to(device)\n",
        "# lamb=torch.tensor([0.3]).to(device)\n",
        "# eta=torch.tensor([1]).to(device)\n",
        "# rho=torch.tensor([0.0]).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpXPObUtUeXw"
      },
      "source": [
        "##$n=2 \\quad (d=3)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Se6ZDJGvccW"
      },
      "outputs": [],
      "source": [
        "dim=torch.tensor([3]).to(device)\n",
        "num_sample=torch.tensor([2**14]).to(device) #M\n",
        "kappa=torch.tensor([1, 0.8]).to(device)\n",
        "theta=torch.tensor([0.1,0.2]).to(device)\n",
        "nu=torch.tensor([0.2, 0.15]).to(device)\n",
        "lamb=torch.tensor([0.1, 0.15]).to(device)\n",
        "\n",
        "# lamb=torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.4, 0.3,0.2,0.1]).to(device)\n",
        "\n",
        "\n",
        "eta=torch.tensor([1]).to(device)\n",
        "rho=torch.tensor([0.0, 0.0]).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMn4tu4LpeMG"
      },
      "source": [
        "##$n=4 \\quad (d=5) $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACll0xERxKlQ"
      },
      "outputs": [],
      "source": [
        "# dim=torch.tensor([5]).to(device)\n",
        "# num_sample=torch.tensor([2**16]).to(device) #M\n",
        "# kappa=torch.tensor([1, 0.8, 1.1 ,1.3]).to(device)\n",
        "# theta=torch.tensor([0.1,0.2,0.3,0.4]).to(device)\n",
        "# nu=torch.tensor([0.2, 0.15, 0.25, 0.31]).to(device)\n",
        "# lamb=torch.tensor([0.1, 0.15, 0.2,0.25]).to(device)\n",
        "# eta=torch.tensor([1]).to(device)\n",
        "# rho=torch.tensor([0.0, 0.0, 0.0, 0.0]).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-V42vfIpnwy"
      },
      "source": [
        "##$n=9 \\quad (d=10)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CU_HeMdqFaE"
      },
      "outputs": [],
      "source": [
        "# dim=torch.tensor([10]).to(device)\n",
        "# num_sample=torch.tensor([2**16]).to(device)\n",
        "# kappa=torch.tensor([1, 0.8,1.1, 1.3, 0.95, 0.99,1.02, 1.06, 1.6]).to(device)\n",
        "# theta=torch.tensor([0.1, 0.2, 0.3, 0.4, 0.25, 0.15, 0.18, 0.08, 0.91]).to(device)\n",
        "# nu=torch.tensor([0.2, 0.15, 0.25, 0.2, 0.2, 0.1, 0.22, 0.2, 0.15]).to(device)\n",
        "\n",
        "# lamb=torch.tensor([0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25,0.275,0.3]).to(device)\n",
        "# # lamb=torch.tensor([0.1, 0.125, 0.15, 0.175, 0.2, 0.1, 0.12,0.12,0.12]).to(device)\n",
        "\n",
        "# eta=torch.tensor([3]).to(device)\n",
        "# rho=torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]).to(device)\n",
        "# start_list=[5,3.5,3,1.5,0]\n",
        "# end_list=[6,7.5,9,10.5,12]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3feRG9_9PKl2"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTguvp5GPHkf"
      },
      "outputs": [],
      "source": [
        "def F(x,z,a): #### out_shape =([M]) |  input:  x_shape=[M,D,1],  z shape = [M,D,1],a_shape= [M,D,D]  #This is for rho=0\n",
        "  result=torch.zeros([x.shape[0]]).to(device)\n",
        "  for i in range(dim-1):\n",
        "    result+=-kappa[i]*(theta[i]-x[:,i+1,0])*z[:,i+1,0]\n",
        "    # print(result)\n",
        "  result1=result-torch.sqrt(torch.sum(torch.square(lamb)))*torch.abs(z[:,0,0])*torch.sqrt(a[:,0,0])\n",
        "  return result1\n",
        "def H(x,z,gamma): #### out_shape =([M]) |  input:  x_shape=[M,D,1],  z shape = [M,D,1],gamma shape= [M,dim,dim]\n",
        "  result=torch.zeros([x.shape[0]]).to(device)\n",
        "  for i in range(dim-1):\n",
        "    result+=kappa[i]*(theta[i]-x[:,i+1,0])*z[:,i+1,0]+0.5*torch.pow(nu[i],2)*gamma[:,i+1,i+1]-rho[i]*lamb[i]*nu[i]*(z[:,0,0]*gamma[:,0,i+1])/gamma[:,0,0] \\\n",
        "    + 0.5*torch.pow(rho[i]*nu[i]*gamma[:,0,i+1],2)/gamma[:,0,0]\n",
        "    # print(result)\n",
        "  result1=result-0.5*torch.sum(torch.square(lamb))*torch.pow(z[:,0,0],2)/gamma[:,0,0]\n",
        "  return result1\n",
        "\n",
        "def g1(x): #out_shape= [M,1,1]   | input:  x_shape=[M,D,1]\n",
        "  # result=1-torch.exp(-eta*x[:,0])\n",
        "  result=-torch.exp(-eta*x[:,0])\n",
        "  result=torch.reshape(result,(result.shape[0],1,1))\n",
        "  return result\n",
        "\n",
        "def update(data,delta_w,L):#output: data=(M,D,1) ,  #input data=(M,D,1), delta_w=[M,D,1]\n",
        "    dx = torch.bmm(L,delta_w)\n",
        "    data = data + dx\n",
        "    data=data\n",
        "    return data\n",
        "\n",
        "def der1(x,f):  #output= [M,D,1], input: x=[M,D,1]\n",
        "  Du=torch.zeros(x.shape[0],dim).to(device)\n",
        "  xin=x.clone().detach()\n",
        "  xin.requires_grad=True\n",
        "  u=f(xin)\n",
        "  print(u)\n",
        "  print('u shape', u.shape)\n",
        "  Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
        "                          allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
        "  print('Du shape', Du.shape)\n",
        "  Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
        "  print('Du shape', Du.shape)\n",
        "  return Du\n",
        "\n",
        "def grad_hessian1(x,f_,c=False): #output= [M,D,D], #input: x=[M,D,1]   #c=False => data=[M,D]   #c=True =>data=[M,D,1]\n",
        "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
        "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
        "\n",
        "    if c==False:\n",
        "      # xin=x.unsqueeze(2).clone().detach()\n",
        "      xin=x.clone().detach()\n",
        "      xin.requires_grad=True\n",
        "      # print(xin.shape)\n",
        "      # xin.requires_grad=True\n",
        "      u=f_(xin).squeeze(2)\n",
        "      # print(u.shape)\n",
        "      # xin=xin.unsqueeze(2)\n",
        "      # print(xin.shape)\n",
        "      Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
        "                           allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
        "      hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
        "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
        "    else:\n",
        "      xin=x.clone().detach()\n",
        "      xin.requires_grad=True\n",
        "      u=f_(xin.squeeze(2))\n",
        "    # print(\"xin shape: \",xin.shape)\n",
        "      Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
        "                           allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
        "    # print(\"Du shape:\",Du.shape)\n",
        "    # print(\"-----\")\n",
        "    # print(torch.autograd.grad(outputs=[Du[:,0,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,0,:]),\n",
        "    #                        allow_unused=True,retain_graph=True,create_graph=True)[0].shape)\n",
        "      hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
        "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
        "    # print(\"D2u shape:\",hess_temp.shape)\n",
        "    Du = torch.reshape(Du,(Du.shape[0],dim,1))\n",
        "    # print(\"Du shape:\",Du.shape)\n",
        "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
        "    # print(\"D2u shape:\",hess_temp.shape)\n",
        "\n",
        "    # print(hess_temp)\n",
        "    return Du, hess_temp\n",
        "def hessian1(x,model2z):  #x=[M,D,1]  output= [M,D,1]\n",
        "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
        "    # print(\"D2u shape:\",hess_temp.shape)\n",
        "\n",
        "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
        "    xin=x.clone().detach()\n",
        "    # print('this is xin')\n",
        "    # print(xin)\n",
        "    xin.requires_grad=True\n",
        "    Du = model2z(xin.squeeze(2))\n",
        "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
        "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
        "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
        "\n",
        "    return hess_temp\n",
        "\n",
        "\n",
        "def V(t,x,v): #output           |#input:\n",
        "  # result=1-torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
        "  # result=1-torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
        "  result=-torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
        "  return result\n",
        "def L_matrix(ite,x,model):  #x=[M,D]\n",
        "  L=torch.zeros(x.shape[0],dim,dim).to(device)\n",
        "\n",
        "  L[:,0,0]=model[ite](x)\n",
        "  for i in range(dim-1):\n",
        "    L[:,i+1,i+1]=nu[i]\n",
        "  return L\n",
        "def opt_quad():\n",
        "  return torch.pow(torch.sum(torch.pow(lamb,2)),0.5)/eta\n",
        "\n",
        "def L_matrix_test(ite,x,model1):  #This matrix is a matrix with L00 = 0.6 and L11=nu\n",
        "  L=torch.zeros(x.shape[0],dim,dim).to(device)\n",
        "  L[:,0,0]=opt_quad().to(device)\n",
        "  for i in range(dim-1):\n",
        "    L[:,i+1,i+1]=nu[i]\n",
        "  return L\n",
        "\n",
        "def gradient(x,f): #input=  [M,D,1]    #output= [M,D,1]\n",
        "  xin=x.clone().detach()\n",
        "  xin.requires_grad=True\n",
        "  u=f(xin.squeeze(2));\n",
        "  Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
        "  return Du"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaCXHlZRw0EE"
      },
      "source": [
        "## Plot Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWlB_kjZw13a"
      },
      "outputs": [],
      "source": [
        "def plot(ite,t1,steps, model1,model2,model11=False,model22=False):\n",
        "  t_test=t1\n",
        "  # xs = torch.linspace(start_list[0], end_list[0], steps=steps)\n",
        "  # ys = torch.linspace(start_list[0],end_list[0], steps=steps)\n",
        "  xs = torch.linspace(start_list[ite-1], end_list[ite-1], steps=steps)\n",
        "  ys = torch.linspace(start_list[ite-1],end_list[ite-1], steps=steps)\n",
        "  xgrid, ygrid = torch.meshgrid(xs, ys, indexing='xy')\n",
        "  sss=torch.stack((xgrid.flatten(),ygrid.flatten()),1).to(device)\n",
        "  print(sss.shape)\n",
        "\n",
        "  # print(z1)\n",
        "\n",
        "###############################################################################################################\n",
        "\n",
        "  if model11==True:\n",
        "    fig = plt.figure(figsize=plt.figaspect(0.5))\n",
        "    # ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
        "    # z1=torch.abs(model1[ite-1](sss).clone().detach().cpu())\n",
        "    z1=torch.abs(model1(sss).clone().detach().cpu())\n",
        "    z1=torch.reshape(z1,(steps,steps))\n",
        "    ax1=plt.axes(projection='3d')\n",
        "    ax1.plot_surface(xgrid.numpy(), ygrid.numpy(), z1.numpy(),label = \"trained_model1\")\n",
        "    lamb_value=torch.reshape(lamb/eta,(1,1)).cpu()\n",
        "    lamb_matrix=np.ones((steps,steps))*lamb_value.numpy()\n",
        "    # ax1=plt.axes(projection='3d')\n",
        "    ax1.plot_surface(xgrid.numpy(), ygrid.numpy(), lamb_value.numpy())\n",
        "    ax1.set_title(\"RMSE of the trained model at time step {}: {:.7f}\".format(t_test,np.sqrt(np.mean(np.square(lamb_matrix-z1.numpy())))))\n",
        "    ax1.set_xlabel(r'x', fontsize=15, rotation=10)\n",
        "    ax1.set_ylabel(r'v', fontsize=15, rotation=10)\n",
        "    # ax1.legend()\n",
        "    # ax1.set_zlim(1.1, 1.3)\n",
        "    plt.gcf().set_dpi(100)\n",
        "    plt.show()\n",
        "\n",
        "###########################################################################################################\n",
        "  if model22==True:\n",
        "    fig = plt.figure(figsize=plt.figaspect(0.5))\n",
        "    # ax2 = fig.add_subplot(1, 2, 1, projection='3d')\n",
        "    z2 = modely2[t_test](sss).clone().detach().cpu()\n",
        "    # z2 = model2(sss).clone().detach().cpu()\n",
        "    z2=torch.reshape(z2,(steps,steps))\n",
        "    # print(z2)\n",
        "    # print(z.shape)\n",
        "    ax2 = plt.axes(projection='3d')\n",
        "    ax2.plot_surface(xgrid.numpy(), ygrid.numpy(), z2.numpy(),label = \"trained_modely2\")\n",
        "\n",
        "    truey = V(t_steps[t_test],xgrid.to(device),ygrid.to(device)).clone().detach().cpu()\n",
        "    # print(\"This is theoretical solution\")\n",
        "    # print(truey)\n",
        "    true_matrixy=torch.reshape(truey,(steps,steps))\n",
        "    # print(true_matrix.shape)\n",
        "    # ax2 = plt.axes(projection='3d')\n",
        "    ax2.plot_surface(xgrid.numpy(), ygrid.numpy(), true_matrixy.numpy(),label = \"True\")\n",
        "\n",
        "\n",
        "    # ax2.set_title(\"RMSE of the trained model Y at timestep {}: {:.7f}\".format(t_test,np.sqrt(np.mean(np.square(true_matrixy.numpy()-z2.numpy())))))\n",
        "    ax2.set_title(\"MSE of the trained model Y at timestep {}: {:.7f}\".format(t_test,np.mean(np.square(true_matrixy.numpy()-z2.numpy()))))\n",
        "    # ax2.set_xlabel(r'x', fontsize=15, rotation=60)\n",
        "    # ax2.set_ylabel(r'v', fontsize=15, rotation=60)\n",
        "    ax2.set_xlabel(r'x', fontsize=15, rotation=60)\n",
        "    ax2.set_ylabel(r'v', fontsize=15, rotation=60)\n",
        "    # ax2.set_zlim(-0.008, 0.004)\n",
        "    # ax2.view_init(10, 10)\n",
        "    plt.gcf().set_dpi(100)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "###########################################################################################################\n",
        "#   ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
        "#   z3 = modelz2[t_test](sss).clone().detach().cpu()\n",
        "#   trained_modelzx=z3.squeeze(2)[:,0].reshape(steps,steps)\n",
        "#   # print(trained_modelzx)\n",
        "#   trained_modelzv=z3.squeeze(2)[:,1].reshape(steps,steps)\n",
        "#   print(trained_modelzv)\n",
        "#   # # print(z.shape)\n",
        "#   ax = plt.axes(projection='3d')\n",
        "#   # ax.plot_surface(xgrid.numpy(), ygrid.numpy(), trained_modelzx.numpy(),label = \"trained_modelzx\")\n",
        "#   ax.plot_surface(xgrid.numpy(), ygrid.numpy(), trained_modelzv.numpy(),label = \"trained_modelzv\")\n",
        "#   truederx = derVx(t_steps[t_test],xgrid.to(device),ygrid.to(device)).clone().detach().cpu()\n",
        "#   # print(truederx)\n",
        "#   truederv=derVv(t_steps[t_test],xgrid.to(device),ygrid.to(device)).clone().detach().cpu()\n",
        "#   print(truederv)\n",
        "#   # ax.plot_surface(xgrid.numpy(), ygrid.numpy(), truederx.numpy(),label = \"True derivative with respect to x\")\n",
        "#   ax.plot_surface(xgrid.numpy(), ygrid.numpy(), truederv.numpy(),label = \"True derivative with respect to v\")\n",
        "\n",
        "# # ax3.set_title(\"MSE of the trained model Z at timestep {}: {:.7f}\".format(t_test,np.mean(np.square(true_derx_matrix.numpy()-z2.numpy()))))\n",
        "\n",
        "\n",
        "\n",
        "#   # ax.set_title(\"MSE of the trained model Y at timestep {}: {:.7f}\".format(t_test,np.mean(np.square(true_matrix.numpy()-z3.numpy()))))\n",
        "#   ax.set_xlabel(r'x', fontsize=15, rotation=70)\n",
        "#   ax.set_ylabel(r'v', fontsize=15, rotation=60)\n",
        "#   # ax.legend()\n",
        "\n",
        "#   plt.gcf().set_dpi(100)\n",
        "#   plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k5b4A20OsL0"
      },
      "source": [
        "## Generating Data Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(ite): #output: [M,D]  #input: a scalar\n",
        "    # x=(torch.rand(num_sample,1))\n",
        "    # delta_w=torch.rand(num_sample,dim).to(device)*delta_t\n",
        "    # x=(torch.rand(num_sample,1))\n",
        "    if ite==num_ite:\n",
        "      x=(start_list[ite-1])+(end_list[ite-1]-start_list[ite-1])*torch.rand(num_sample,dim)\n",
        "      # x=(start_list[ite])+(end_list[ite]-start_list[ite])*delta_w\n",
        "    else:\n",
        "      x=(start_list[ite-1])+((end_list[ite-1])-(start_list[ite-1]))*torch.rand(num_sample,dim)\n",
        "    return x\n",
        "\n",
        "def data_generator2(ite,t1,t2,delta_w): #input: ite=scalar, t1=scalar, t2=scalar     #output: [M,D,1]\n",
        "  count=0\n",
        "  l=[]\n",
        "  k=[]\n",
        "  xx=torch.zeros([num_sample,dim,num_time_interval+1]).to(device)\n",
        "  # xx=torch.zeros([num_sample,dim,t2-t1+2]).to(device)\n",
        "  # xx[:,:,t1]=start_list[ite-1]+(end_list[ite-1]-start_list[ite-1])*(torch.rand(num_sample,dim))\n",
        "  xx[:,:,t1]=start_list[ite-1]+(end_list[ite-1]-start_list[ite-1])*delta_w[:,:,t1-1]\n",
        "\n",
        "  # if ite==num_ite:\n",
        "  for i in range(t1, t2+1):\n",
        "    L=L_matrix(ite-1,xx[:,:,i].to(device),model1)\n",
        "    # L=L_matrix_test(ite-1,xx[:,:,i].to(device),model1)\n",
        "    # print(L)\n",
        "    xx[:,:,i+1]=(xx[:,:,i]+torch.bmm(L,delta_w[:,:,i].unsqueeze(2)).squeeze(2)).clone().detach()\n",
        "  path_min = torch.min(xx[:,:,t1:t2+2].flatten(start_dim=1), dim=1)[0]\n",
        "  # print(path_min)\n",
        "  path_max = torch.max(xx[:,:,t1:t2+2].flatten(start_dim=1), dim=1)[0]\n",
        "  # print(path_max)\n",
        "  # pick_sample = torch.logical_and(path_max < end_list[ite]-2,path_min > start_list[ite]+2)\n",
        "  pick_sample = torch.logical_and(path_max < end_list[ite],path_min > start_list[ite])\n",
        "  # print('this is start_list[ite]',start_list[ite])\n",
        "  # print('this is end_list[ite]',end_list[ite])\n",
        "  # print(pick_sample)\n",
        "  xx_new=xx[pick_sample,:,:]\n",
        "  delta_w_new=delta_w[pick_sample,:,:]\n",
        "\n",
        "  return xx_new.cpu(),delta_w_new.cpu()"
      ],
      "metadata": {
        "id": "8zf_QhyxtJnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Outlier Function"
      ],
      "metadata": {
        "id": "Ul1D6A_O6dPI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG9GSSyZaELQ"
      },
      "outputs": [],
      "source": [
        "def remove_outliers(ite,x):\n",
        "    path_min = torch.min(x.flatten(start_dim=1), dim=1)[0]\n",
        "    # print(path_min)\n",
        "    path_max = torch.max(x.flatten(start_dim=1), dim=1)[0]\n",
        "    # print(path_max)\n",
        "    pick_sample = torch.logical_and(path_max < end_list[ite],path_min > start_list[ite])\n",
        "    # print(pick_sample)\n",
        "    x_new=x[pick_sample,:]\n",
        "    # print('the percent of paths removed is {}%'.format((x.shape[0]-x_new.shape[0])/(x.shape[0])*100))\n",
        "    return x_new"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Error Function"
      ],
      "metadata": {
        "id": "cS2NH-MHu5qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size=1000\n",
        "test_samples=[(start_list[0]+(end_list[0]-start_list[0])*torch.rand(size).to(device)) for i in range(dim)]\n",
        "\n",
        "def mean_error(t_test,ite,model):\n",
        "  # test_samples=[(start+(end-start)*torch.rand(size).to(device)) for i in range(dim)]\n",
        "  sss = torch.stack(test_samples,dim=1)\n",
        "  z2=model[ite](sss).clone().detach().cpu()\n",
        "  truey = V(t_test,sss,1).clone().detach().cpu()\n",
        "  error=np.mean(np.square(truey.numpy()-z2.numpy()))\n",
        "  return error\n",
        "\n",
        "# def mean_error(size,start,end,t_test,ite,model):\n",
        "#   test_samples=[(start+(end-start)*torch.rand(size).to(device)) for i in range(dim)]\n",
        "#   sss = torch.stack(test_samples,dim=1)\n",
        "#   z2=model[ite](sss).clone().detach().cpu()\n",
        "#   truey = V(t_test,sss,1).clone().detach().cpu()\n",
        "#   error=np.mean(np.square(truey.numpy()-z2.numpy()))\n",
        "#   return error"
      ],
      "metadata": {
        "id": "66GX9Ih8u5BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-ZHHU-S79jN"
      },
      "source": [
        "# Initilizing Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1I44Ai9uXEc"
      },
      "outputs": [],
      "source": [
        "class ann1(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ann1, self).__init__()\n",
        "        self.linear_stack = nn.Sequential(\n",
        "\n",
        "            nn.Linear(dim, 128),\n",
        "            # nn.BatchNorm1d(256),\n",
        "            nn.BatchNorm1d(num_features=128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128,128),\n",
        "            nn.BatchNorm1d(num_features=128),\n",
        "            # nn.BatchNorm1d(256),\n",
        "            nn.Tanh(),\n",
        "            # nn.Linear(256,256),\n",
        "            # nn.Tanh(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.BatchNorm1d(num_features=128),\n",
        "            # nn.BatchNorm1d(128),\n",
        "            nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            nn.Linear(128,1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_stack(x)\n",
        "        logits=torch.reshape(logits,(x.shape[0],))\n",
        "        # logits=torch.flatten(logits)\n",
        "        # logits=torch.reshape(logits,[x.shape[0]])\n",
        "        # logits=nn.ELU()(logits)+1.5\n",
        "        # logits=torch.reshape(logits,(x.shape[0],1,1))\n",
        "        return torch.max(logits,-logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPCzSVnUzh1C"
      },
      "outputs": [],
      "source": [
        "model1= nn.ModuleList([ann1() for i in range(num_ite)]).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwNCvaaR8ROE"
      },
      "source": [
        "# Loss function 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NQIIEAVDuDI"
      },
      "outputs": [],
      "source": [
        "######## TEST#######################\n",
        "def loss_fn1(ite,x,delta_w,model):  #x=[M,D,1]  delta_w = [M,D,1]\n",
        "    L=torch.zeros(x.shape[0],dim,dim).to(device)\n",
        "    L[:,0,0]=model(x.squeeze(2))\n",
        "    for i in range(dim-1):\n",
        "      L[:,i+1,i+1]=nu[i]\n",
        "    n=update(x,delta_w,L)\n",
        "    if ite==num_ite:\n",
        "      Y=g1(n).to(device)\n",
        "      # print(Y.shape)\n",
        "      # print(\"Y= \")\n",
        "      # print(Y)\n",
        "      Z,hessian=grad_hessian1(n,g1)\n",
        "      # print(\"Z= \")\n",
        "      # print(Z)\n",
        "      # print(\"Hessian= \")\n",
        "      # print(hessian)\n",
        "    else:\n",
        "      n=remove_outliers(ite,n)\n",
        "      L=L[0:n.shape[0],:,:]\n",
        "      Y=modely2[t2+1](n.squeeze(2))\n",
        "      Z,hessian=grad_hessian1(n.squeeze(2),modely2[t2+1])\n",
        "      Z=Z+0.001\n",
        "      hessian=hessian+0.001\n",
        "      # Y=modely2[t2+1](n.squeeze(2))\n",
        "      # Z=modelz2[t2+1](n.squeeze(2))+0.0001\n",
        "      # hessian=hessian1(n,modelz2[t2+1])+0.0001\n",
        "      # print(\"Z= \", Z)\n",
        "      # print(\"hessian= \",hessian)\n",
        "    LTL=torch.bmm(torch.transpose(L,1,2),L)\n",
        "    # print(\"LTL =\", LTL)\n",
        "    LTLhessian=torch.bmm(LTL,hessian)\n",
        "    # print(\"LTLhessian =\", LTLhessian)\n",
        "    Tr=LTLhessian.diagonal(offset=0,dim1=-1,dim2=-2).sum(-1)\n",
        "    # print(\"Trace =\", Tr)\n",
        "    H_=H(n,Z,hessian)\n",
        "    # print(\"H = \", H_)\n",
        "    F_=F(n,Z,LTL)\n",
        "    # print(\"F= \", F_)\n",
        "    # loss=torch.mean(torch.pow(H_-0.5*Tr+F_,2))\n",
        "    loss=torch.mean(torch.pow(H_-0.5*Tr+F_,2))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN9-HECRf3d8"
      },
      "source": [
        "# Initializing Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwqKlJjgf26X"
      },
      "outputs": [],
      "source": [
        "class anny2(nn.Module): #input [M,D], #output [M,1,1]\n",
        "    def __init__(self):\n",
        "        super(anny2, self).__init__()\n",
        "        self.linear_stack = nn.Sequential(\n",
        "            nn.Linear(dim, 128),\n",
        "            # nn.BatchNorm1d(256),\n",
        "            # nn.BatchNorm1d(num_features=128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128,128),\n",
        "            # nn.BatchNorm1d(num_features=128),\n",
        "            nn.Tanh(),\n",
        "            # nn.Linear(256, 256),\n",
        "            # nn.Tanh(),\n",
        "            nn.Linear(128,128),\n",
        "            # nn.BatchNorm1d(num_features=128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128,128),\n",
        "            # nn.BatchNorm1d(num_features=128),\n",
        "            nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            nn.Linear(128,1),\n",
        "\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_stack(x)\n",
        "\n",
        "        logits=torch.reshape(logits,(x.shape[0],1,1))\n",
        "        return logits\n",
        "        # return torch.min(logits,-logits)\n",
        "\n",
        "class annz2(nn.Module): #input [M,D], #output [M,D,1]\n",
        "    def __init__(self):\n",
        "        super(annz2, self).__init__()\n",
        "        self.linear_stack = nn.Sequential(\n",
        "            nn.Linear(dim, 128),\n",
        "            nn.BatchNorm1d(num_features=128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128,256),\n",
        "            nn.BatchNorm1d(num_features=128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            nn.Linear(128,dim),\n",
        "            # nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_stack(x)\n",
        "\n",
        "        logits=torch.reshape(logits,(x.shape[0],dim,1))\n",
        "        # return logits\n",
        "        return torch.min(logits,-logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeiTG1Amy8Ze"
      },
      "outputs": [],
      "source": [
        "modely= anny2().to(device)\n",
        "modelz= annz2().to(device)\n",
        "modely2= nn.ModuleList([anny2() for i in range(num_time_interval+1)]).to(device)\n",
        "modelz2= nn.ModuleList([annz2() for i in range(num_time_interval+1)]).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNOtxgVuXtMO"
      },
      "source": [
        "# Loss Function 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgIiN2dC9ZXS"
      },
      "outputs": [],
      "source": [
        "def loss_fn2(ite,data,delta_w):\n",
        "\n",
        "  bb=L_matrix(ite-1,data[:,:,t1],model1).to(device)\n",
        "  # bb=L_matrix_test(ite-1,data[:,:,t1],model1).to(device)\n",
        "  ss=torch.pow(bb,2)\n",
        "  loss=0\n",
        "  Y=modely2[t1](data[:,:,t1])\n",
        "  # print(data[:,:,t1].shape)\n",
        "  # gradY,hessianY=grad_hessian1(data[:,:,t1],modely2[t1])\n",
        "  # Z=modelz2[t1](data[:,:,t1])\n",
        "  Z=gradient(data[:,:,t1].unsqueeze(2),modely2[t1]).to(device)\n",
        "  # for ite in range(t1,t2):\n",
        "  #   Y=Y.flatten()+F(data[:,:,ite].unsqueeze(2),Z,ss)*delta_t+torch.bmm(torch.bmm(torch.transpose(Z,1,2),ss),delta_w[:,:,ite].unsqueeze(2)).flatten()\n",
        "  #   # bbb=L_matrix(num_ite-1,data[:,:,ite+1],model1)\n",
        "  #   bbb=L_matrix_test(num_ite-1,data[:,:,ite+1],model1)\n",
        "  #   ss=torch.pow(bbb,2)\n",
        "  #   Z=modelz2[ite+1](data[:,:,ite+1])\n",
        "  # Y=Y.flatten()+F(data[:,:,t2].unsqueeze(2),Z,ss)*delta_t+torch.bmm(torch.bmm(torch.transpose(Z,1,2),ss),delta_w[:,:,t2].unsqueeze(2)).flatten()\n",
        "\n",
        "  # loss+=torch.mean(torch.square(Y-modely2[t2+1](data[:,:,t2+1]).flatten()))\n",
        "  # # loss+=torch.mean(torch.square(Y-g1(data[:,:,t2+1]).flatten()))\n",
        "  # return loss\n",
        "  for i in range(t1,t2):\n",
        "    Y=Y.flatten()+F(data[:,:,i].unsqueeze(2),Z,ss)*delta_t+torch.bmm(torch.bmm(torch.transpose(Z,1,2),bb),delta_w[:,:,i].unsqueeze(2)).flatten()\n",
        "    bb=L_matrix(ite-1,data[:,:,i+1],model1).to(device)\n",
        "    # bb=L_matrix_test(ite-1,data[:,:,i+1],model1).to(device)\n",
        "    ss=torch.pow(bb,2)\n",
        "    # Z=modelz2[i+1](data[:,:,i+1])\n",
        "    Z=gradient(data[:,:,i+1].unsqueeze(2),modely2[i+1])\n",
        "    # loss+=torch.mean(torch.mean(torch.square(gradient(data[:,:,i+1].unsqueeze(2),Y)-Z).squeeze(2),1))\n",
        "    loss+=torch.mean(torch.square(Y-modely2[i+1](data[:,:,i+1]).flatten()))\n",
        "  Y=Y.flatten()+F(data[:,:,t2].unsqueeze(2),Z,ss)*delta_t+torch.bmm(torch.bmm(torch.transpose(Z,1,2),bb),delta_w[:,:,t2].unsqueeze(2)).flatten()\n",
        "  if ite==num_ite:\n",
        "    loss+=torch.mean(torch.square(Y-g1(data[:,:,t2+1]).flatten()))\n",
        "  else:\n",
        "    loss+=torch.mean(torch.square(Y-modely2[t2+1](data[:,:,t2+1]).flatten()))\n",
        "  return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUIa09BDi01P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw28xC2BJZa_"
      },
      "source": [
        "#Training Both Model1 And Model2 Simultaneously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RONIDEfie4Uo"
      },
      "outputs": [],
      "source": [
        "from IPython.core.pylabtools import figsize\n",
        "batch_size=int(num_sample/16)\n",
        "num_epoch1 =100\n",
        "num_epoch2=200\n",
        "total_batch=int((num_sample/batch_size))\n",
        "index=0\n",
        "last_loss1 = 0\n",
        "last_loss2=0\n",
        "loss_delta1 = 0\n",
        "loss_delta2=0\n",
        "loss_treshold1 = 0\n",
        "loss_treshold2=0\n",
        "loss_tmp1=[]\n",
        "loss_tmp2=[]\n",
        "# my_index=torch.arange(num_sample)\n",
        "# num_test_pts=500\n",
        "num_runs=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EWn3QGm9GC9"
      },
      "source": [
        "## Optimizer 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e0RAX97VjkS"
      },
      "outputs": [],
      "source": [
        "# lr1=[0.1, 0.1, 0.1, 0.1, 0.1]\n",
        "# lr1=[0.000000015, 0.05, 0.01, 0.01, 0.01]\n",
        "lr1=[0.001, 0.001, 0.001, 0.001, 0.001]\n",
        "def opt1(ite, Adam=False,SGD=False):\n",
        "  l=[]\n",
        "  lr_=lr1[ite]\n",
        "\n",
        "  if Adam==True:\n",
        "    optimizer=optim.Adam(model1[ite-1].parameters(),lr=0.001)\n",
        "  # optimizer=optim.Adam(model1[ite-1].parameters(),lr=lr_,weight_decay=1e-5)\n",
        "  if SGD==True:\n",
        "      optimizer=optim.SGD(model1[ite-1].parameters(), lr=lr_, momentum=0.9)\n",
        "  # scheduler = ExponentialLR(optimizer, gamma =0.9)\n",
        "  # optimizer = torch.optim.Adam(model1[ite-1].parameters(), lr=lr_, weight_decay=1e-5)\n",
        "\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pn7bWn19OH6"
      },
      "source": [
        "## Optimizer 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UpXLsui5rPS"
      },
      "outputs": [],
      "source": [
        "lr2=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
        "# lr2=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
        "# lr2=[0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015,0.015, 0.015, 0.015, 0.015, 0.015, 0.015]\n",
        "# lr2=[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001,0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
        "def opt2(ite,t1,t2,Adam=False,SGD=False):\n",
        "  l=[]\n",
        "  lr_=lr2[ite]\n",
        "\n",
        "  # for i in range(t1,t2+1):\n",
        "  #   l+=list(modely2[i].parameters())+list(modelz2[i].parameters())\n",
        "\n",
        "  # l=list(modely2[t1].parameters())\n",
        "  for i in range(t1,t2+1):\n",
        "    l+=list(modely2[i].parameters())\n",
        "  # optimizer=optim.Adam(l,lr=0.005)\n",
        "  if Adam==True:\n",
        "    optimizer=optim.Adam(l, lr=lr_)\n",
        "  if SGD==True:\n",
        "    optimizer=optim.SGD(l, lr=lr_, momentum=0.9)\n",
        "  # optimizer = torch.optim.Adam(l, lr=lr_, weight_decay=1e-5)\n",
        "  # scheduler = ExponentialLR(optimizer, gamma =0.9)\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6ucFMmeGwbM"
      },
      "source": [
        "## Value function\n",
        "$u(t,x,v)=-e^{-\\eta x-0.5(T-t)\\lambda^2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "JBxEo6-aur1Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1L7bU-QD6Pk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73fa7ea3-0442-4014-c33c-67c882028b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "This is 1th run\n",
            "this is ite= 4\n",
            "t1= 15\n",
            "t2= 19\n",
            "Loss function 1 at epoch 0 is 2.158955794584e-06  |  Loss delta: 2.1589557945844717e-06   |    Training time: 1.19700s\n",
            "Loss function 1 at epoch 40 is -4.244803903930e-07  |  Loss delta: 1.058130123965384e-06   |    Training time: 0.15348s\n",
            "Loss function 1 at epoch 80 is 7.358539733104e-08  |  Loss delta: 1.6392505131079815e-06   |    Training time: 0.15440s\n",
            "Loss function 2 at epoch 0 is 1.761971414089e-01  |  Loss delta: 0.1761971414089203   |    Training time: 0.19217s\n",
            "Loss function 2 at epoch 40 is -4.375521093607e-03  |  Loss delta: 0.04109642654657364   |    Training time: 0.18787s\n",
            "Loss function 2 at epoch 80 is -1.189118251204e-03  |  Loss delta: 0.031222598627209663   |    Training time: 0.18332s\n",
            "Loss function 2 at epoch 120 is -7.436983287334e-04  |  Loss delta: 0.024899393320083618   |    Training time: 0.18002s\n",
            "Loss function 2 at epoch 160 is -8.643772453070e-04  |  Loss delta: 0.022013595327734947   |    Training time: 0.18512s\n",
            "this is ite= 3\n",
            "t1= 10\n",
            "t2= 14\n",
            "Loss function 1 at epoch 100 is 8.801375325106e-03  |  Loss delta: 0.008802590891718864   |    Training time: 0.26465s\n",
            "Loss function 1 at epoch 140 is -7.490799180232e-04  |  Loss delta: 0.00036785349948331714   |    Training time: 0.27170s\n",
            "Loss function 1 at epoch 180 is 7.127926655812e-04  |  Loss delta: 0.0008836593478918076   |    Training time: 0.26388s\n",
            "Loss function 2 at epoch 200 is 2.834388613701e-02  |  Loss delta: 0.04787789285182953   |    Training time: 0.25762s\n",
            "Loss function 2 at epoch 240 is -4.153214395046e-04  |  Loss delta: 0.007988875731825829   |    Training time: 0.22329s\n",
            "Loss function 2 at epoch 280 is -6.223423406482e-05  |  Loss delta: 0.005407425574958324   |    Training time: 0.22636s\n",
            "Loss function 2 at epoch 320 is -4.856218583882e-05  |  Loss delta: 0.0037703358102589846   |    Training time: 0.23905s\n",
            "Loss function 2 at epoch 360 is -2.994132228196e-05  |  Loss delta: 0.0028691617771983147   |    Training time: 0.22308s\n",
            "this is ite= 2\n",
            "t1= 5\n",
            "t2= 9\n",
            "Loss function 1 at epoch 200 is 4.517277903506e-02  |  Loss delta: 0.04541132599115372   |    Training time: 0.27599s\n",
            "Loss function 1 at epoch 240 is 6.728242151439e-03  |  Loss delta: 0.012718706391751766   |    Training time: 0.26662s\n",
            "Loss function 1 at epoch 280 is -4.864743828773e+00  |  Loss delta: 1.1272319555282593   |    Training time: 0.26823s\n",
            "Loss function 2 at epoch 400 is 2.394305565394e-02  |  Loss delta: 0.02625288814306259   |    Training time: 0.32729s\n",
            "Loss function 2 at epoch 440 is 7.990328595042e-05  |  Loss delta: 0.002659643767401576   |    Training time: 0.33135s\n",
            "Loss function 2 at epoch 480 is -4.362082108855e-07  |  Loss delta: 0.00168725592084229   |    Training time: 0.32712s\n",
            "Loss function 2 at epoch 520 is -4.870828706771e-05  |  Loss delta: 0.0011225712951272726   |    Training time: 0.33212s\n",
            "Loss function 2 at epoch 560 is 3.825296880677e-05  |  Loss delta: 0.0007940545328892767   |    Training time: 0.34041s\n",
            "this is ite= 1\n",
            "t1= 0\n",
            "t2= 4\n",
            "Loss function 1 at epoch 300 is -3.185849823058e-02  |  Loss delta: 0.0055304113775491714   |    Training time: 0.27924s\n",
            "Loss function 1 at epoch 340 is 5.091342050582e-02  |  Loss delta: 0.057466212660074234   |    Training time: 0.26710s\n",
            "Loss function 1 at epoch 380 is 2.963599748909e-02  |  Loss delta: 0.035276640206575394   |    Training time: 0.26307s\n",
            "Loss function 2 at epoch 600 is 1.191001269035e-03  |  Loss delta: 0.0017398868221789598   |    Training time: 0.49984s\n",
            "Loss function 2 at epoch 640 is 1.293141394854e-05  |  Loss delta: 0.0005665653152391315   |    Training time: 0.50702s\n",
            "Loss function 2 at epoch 680 is -1.390723627992e-05  |  Loss delta: 0.00044602766865864396   |    Training time: 0.52315s\n",
            "Loss function 2 at epoch 720 is 4.099274519831e-07  |  Loss delta: 0.00034885533386841416   |    Training time: 0.49978s\n",
            "Loss function 2 at epoch 760 is -2.831046003848e-06  |  Loss delta: 0.0002821969974320382   |    Training time: 0.50393s\n",
            "this is accumulated mean square error from each run [0.0015963502]\n",
            "----------------------------------------------------------------------------------------------------------------------------------------\n",
            "This is 2th run\n",
            "this is ite= 4\n",
            "t1= 15\n",
            "t2= 19\n",
            "Loss function 1 at epoch 400 is -1.371702185543e-02  |  Loss delta: 4.021388576802565e-06   |    Training time: 0.16453s\n",
            "Loss function 1 at epoch 440 is 7.282915248652e-08  |  Loss delta: 3.6582391658157576e-06   |    Training time: 0.16548s\n",
            "Loss function 1 at epoch 480 is 1.695684659353e-07  |  Loss delta: 2.266208866785746e-06   |    Training time: 0.16034s\n",
            "Loss function 2 at epoch 800 is 1.241046601353e-01  |  Loss delta: 0.12434583157300949   |    Training time: 0.18312s\n",
            "Loss function 2 at epoch 840 is 7.355585694313e-05  |  Loss delta: 0.03942420706152916   |    Training time: 0.18418s\n",
            "Loss function 2 at epoch 880 is -1.488955691457e-03  |  Loss delta: 0.02896302007138729   |    Training time: 0.19556s\n",
            "Loss function 2 at epoch 920 is -6.207376718521e-04  |  Loss delta: 0.02475065179169178   |    Training time: 0.18488s\n",
            "Loss function 2 at epoch 960 is 4.701875150204e-04  |  Loss delta: 0.021518860012292862   |    Training time: 0.18916s\n",
            "this is ite= 3\n",
            "t1= 10\n",
            "t2= 14\n",
            "Loss function 1 at epoch 500 is 1.834601739438e-02  |  Loss delta: 0.01834842748939991   |    Training time: 0.27781s\n",
            "Loss function 1 at epoch 540 is 2.033977168612e-01  |  Loss delta: 0.2085503488779068   |    Training time: 0.26626s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-2f26cbd83ddc>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# loss1=loss_fn1(ite,data.unsqueeze(2)[temp_index,:].to(device),delta_w[temp_index,:,t2].unsqueeze(2).to(device),model1[ite-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelta_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mite\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0moptimizer_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0mloss_tmp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#### TESTTTTTTTTTT ##################\n",
        "model1results=[]\n",
        "modely2results=[]\n",
        "modelz2results=[]\n",
        "for i in range(num_runs): #numer of runs\n",
        "\n",
        "  print('----------------------------------------------------------------------------------------------------------------------------------------')\n",
        "  print(\"This is {}th run\".format(i+1))\n",
        "  model1= nn.ModuleList([ann1() for i in range(num_ite)]).to(device)\n",
        "  modely2= nn.ModuleList([anny2() for i in range(num_time_interval+1)]).to(device)\n",
        "  modelz2= nn.ModuleList([annz2() for i in range(num_time_interval+1)]).to(device)\n",
        "  # delta_w1 = torch.randn(size=[num_sample, dim]).to(device) * sqrt_delta_t\n",
        "  for ite in range(num_ite,0,-1):\n",
        "    # model1results=[]\n",
        "    # modely2results=[]\n",
        "    # modelz2results=[]\n",
        "    # \"This is {}th run\".format(i+1)\n",
        "    print(\"this is ite=\",ite)\n",
        "    t_0 = time.time()\n",
        "    # x0=torch.ones([1,dim]).to(device)+start_list[ite-1]\n",
        "    # x01=torch.ones([100,dim]).to(device)+start_list[ite-1]\n",
        "    # print(\"this is testing point x0\",x0)\n",
        "    # print(\"At iteration {}:\".format( ite))\n",
        "    t1=ite*num_steps-num_steps\n",
        "    print(\"t1=\", t1)\n",
        "    t2=ite*num_steps-1\n",
        "    print(\"t2=\",t2)\n",
        "    # print(\"this is t_test[t1]\",t_steps[t1])\n",
        "    # print(\"this is lambda\", lamb)\n",
        "    # print(\"this is eta\", eta)\n",
        "    # print(\"this is start_list[ite-1]+1\", start_list[ite-1]+1)\n",
        "    # print(\"this is T\", T)\n",
        "    # print(\"this is theortical solution\", V(t_steps[t1],start_list[ite-1]+1,1))\n",
        "    optimizer_1=opt1(ite,SGD=True)\n",
        "    # if ite==num_ite:\n",
        "    #   num_epoch1=150\n",
        "    # else:\n",
        "    #   num_epoch1=150\n",
        "    # num_epoch1=0\n",
        "    delta_w = torch.randn(size=[num_sample, dim]).to(device) * sqrt_delta_t\n",
        "    for epoch in range(num_epoch1):\n",
        "\n",
        "      # delta_w = torch.randn(size=[num_sample, dim]).to(device) * sqrt_delta_t\n",
        "      data=sample(ite)\n",
        "      my_index1=torch.arange(data.shape[0])\n",
        "      total_batch1=int((data.shape[0]/batch_size))\n",
        "      t_1=time.time()\n",
        "      for batch in range(total_batch1):\n",
        "        if batch==total_batch1-1:\n",
        "              temp_index=my_index1[batch*batch_size : ]\n",
        "        else:\n",
        "              temp_index=my_index1[batch*batch_size : batch*batch_size + batch_size]\n",
        "\n",
        "        optimizer_1.zero_grad()\n",
        "        # loss1=loss_fn1(ite,data.unsqueeze(2)[temp_index,:].to(device),delta_w[temp_index,:,t2].unsqueeze(2).to(device),model1[ite-1])\n",
        "        loss1=loss_fn1(ite,data.unsqueeze(2)[temp_index,:].to(device),delta_w[temp_index,:].unsqueeze(2).to(device),model1[ite-1])\n",
        "        loss1.backward()\n",
        "        optimizer_1.step()\n",
        "      loss_tmp1.append(loss1.item())\n",
        "      loss_delta1 = loss1.item() - last_loss1\n",
        "      last_loss1 = loss1.item()\n",
        "      if epoch %40 == 0:\n",
        "        print(\"Loss function 1 at epoch {:.0f} is {:.12e}  |  Loss delta: {}   |    Training time: {:.5f}s\".format(index*(num_epoch1)+epoch, loss_delta1,loss1.item(), time.time()-t_1))\n",
        "        # print(\"Total time for training {:.0f} epochs is {:.4e}s\".format(index*(num_epoch1)+epoch+1,time.time()-t_0))\n",
        "\n",
        "    # print(\"The value of model1 at time step {} is {}\".format(t2,torch.mean(model1[ite-1](x01)).item()))\n",
        "    # print(\"The true optimal quadratic is {}\".format(opt_quad().item()))\n",
        "    # print(\"The  relative percentage error is {}: \".format((torch.abs((torch.mean(model1[ite-1](x01)))-opt_quad())).item()/torch.abs(model1[ite-1](x01)).item()))\n",
        "    # index+=1\n",
        "    # plot(ite,t1,100,model1[ite-1],modely2[t1],model11=True)\n",
        "\n",
        "    optimizer_2=opt2(ite,t1,t2, SGD=True)\n",
        "    for epoch in range(num_epoch2):\n",
        "    # for epoch in range(100):\n",
        "      t_2=time.time()\n",
        "      delta_w = torch.randn(size=[num_sample, dim, num_time_interval]).to(device) * sqrt_delta_t\n",
        "      # x_sample = torch.zeros([num_sample,dim, num_time_interval + 1])\n",
        "      # x_sample[:, :, 0] = torch.zeros([num_sample, dim]) * x_init\n",
        "\n",
        "      x_new,delta_w_new=data_generator2(ite,t1,t2,delta_w)\n",
        "      # print('this is x_new',x_new.shape)\n",
        "      my_index2=np.arange(x_new.shape[0])\n",
        "      # print('this is my_index2',my_index2)\n",
        "      total_batch2=int((x_new.shape[0]/batch_size))\n",
        "      # print('this is batch_size',batch_size)\n",
        "      # print('this is total_batch2',total_batch2)\n",
        "\n",
        "      for batch in range(total_batch2):\n",
        "        if batch==total_batch2-1:\n",
        "              temp_index=my_index2[batch*batch_size : ]\n",
        "        else:\n",
        "              temp_index=my_index2[batch*batch_size : batch*batch_size + batch_size]\n",
        "        data2=x_new[temp_index,:]\n",
        "        # print(data2.shape)\n",
        "        delta_w2=delta_w_new[temp_index,:]\n",
        "        optimizer_2.zero_grad()\n",
        "        loss2=loss_fn2(ite,data2.to(device),delta_w2.to(device))\n",
        "        # print(loss2)\n",
        "        loss2.backward()\n",
        "        optimizer_2.step()\n",
        "      loss_tmp2.append(loss2.item())\n",
        "      loss_delta2 = loss2.item() - last_loss2\n",
        "      last_loss2 = loss2.item()\n",
        "      if epoch %40 == 0:\n",
        "        print(\"Loss function 2 at epoch {:.0f} is {:.12e}  |  Loss delta: {}   |    Training time: {:.5f}s\".format(index*(num_epoch2)+epoch, loss_delta2,loss2.item(), time.time()-t_2))\n",
        "        # print(\"Total time for training {:.0f} epochs is {:.4e}s\".format(index*(num_epoch1)+epoch+1,time.time()-t_0))\n",
        "    index+=1\n",
        "\n",
        "    # plot(ite,t1,100,model1[ite-1],modely2[t1],model22=True)\n",
        "\n",
        "    # print(\"The value of modely2 at time step {} is {}\".format(t1,modely2[t1](x0).item()))\n",
        "    # print(\"The theoretical solution is {}\".format(V(t_steps[t1],start_list[ite-1]+1,start_list[ite-1]+1).item()))\n",
        "    # print(\"The relative percentage error is {}: \".format(torch.abs(modely2[t1](x0)-V(t_steps[t1],start_list[ite-1]+1,start_list[ite-1]+1)).item()/torch.abs(modely2[t1](x0)).item()))\n",
        "  # model1results.append(model1[ite-1](x0))\n",
        "  # modely2results.append(modely2[t1](x0))\n",
        "    if ite==1:\n",
        "      # x0=test_function(t1,dim,num_test_pts,start_list[t1],end_list[t1])\n",
        "      # modely2results.append(test_function(t1,dim,num_test_pts,start_list[t1],end_list[t1]))\n",
        "      # modely2results.append(mean_error(1000,start_list[0],end_list[0],0,0,modely2))\n",
        "      modely2results.append(mean_error(0,0,modely2))\n",
        "# print(model1results)\n",
        "  print('this is accumulated mean square error from each run', modely2results)\n",
        "# print(\"The average value of {} independent runs of modely2 at time step {} is {}\".format(num_runs,t1,torch.mean(torch.tensor(model1results))))\n",
        "print(\"The average value of {} independent runs of mse at time step {} is {}\".format(num_runs,t1,torch.mean(torch.tensor(modely2results))))\n",
        "print(\"The average std of {} independent runs of mse at time step {} is {}\".format(num_runs,t1,torch.std(torch.tensor(modely2results))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I82biHCdU8R2"
      },
      "outputs": [],
      "source": [
        "# x0=torch.ones([1,dim]).to(device)+4   #our testing region is [4,6] so x0=5\n",
        "# print(x0)\n",
        "# print(x0.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugaABDZrzlvX"
      },
      "outputs": [],
      "source": [
        "# aaa=[torch.rand(5)+5 for i in range(dim)]\n",
        "# print(aaa)\n",
        "# bbb = torch.stack(aaa,dim=1)  #50 randomly generated points for testing\n",
        "# print(bbb)\n",
        "# print(bbb.shape)\n",
        "# print(bbb[:,0])\n",
        "# ccc=modely2[0](bbb.to(device))  #evaluate these 50 random points using modely2 at time step 0\n",
        "# print(ccc)\n",
        "# print(ccc.shape)\n",
        "# # ddd=V1(0,bbb[:,0].to(device))   #evaluate these 50 random point using true value function\n",
        "# dddd=V(0,bbb[:,0].to(device),bbb[:,1].to(device))\n",
        "# # print(ddd)\n",
        "# print(dddd)\n",
        "# eee=torch.mean(torch.square(ccc.flatten()-dddd))  #take square difference between true and approximated values\n",
        "# print(eee)\n",
        "# # print(V1(t_steps[t1],5.3743))\n",
        "# # print(modely2[0](x0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqj8p6zhQlHk"
      },
      "outputs": [],
      "source": [
        "# print(dim)\n",
        "# print(start_list[0])\n",
        "# print(start_list[0],end_list[0])\n",
        "# print((start_list[0]+end_list[0])/2)\n",
        "# # print(torch.mean(torch.tensor(3),torch.tensor(5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVE (Path + Check Points)"
      ],
      "metadata": {
        "id": "pFcJWGW7dr40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PATH = \"/content/gdrive//My Drive/'testsample.pt\"\n",
        "PATH = \"/content/gdrive//My Drive/dim({})_num_sample({})_num_epoch({})_learning_rate1({})_learning_rate2({})_batchsize({}).pt\".format(dim.item(),num_sample.item(),num_epoch2,lr1[1],lr2[1],batch_size)\n",
        "torch.save({\n",
        "            'epoch1': num_epoch1,\n",
        "            'epoch2': num_epoch2,\n",
        "            'model_state_dict1': model1[0].state_dict(),\n",
        "            'model_state_dict2': modely2[0].state_dict(),\n",
        "            'optimizer_state_dict1': optimizer_1.state_dict(),\n",
        "            'optimizer_state_dict2': optimizer_2.state_dict(),\n",
        "            # 'loss1':loss1.item(),\n",
        "            # 'loss2': loss2.item(),\n",
        "            'loss1_array': loss_tmp1,\n",
        "            'loss2_array': loss_tmp2,\n",
        "            'modely2results':modely2results,\n",
        "            }, PATH)"
      ],
      "metadata": {
        "id": "3VJCKjr0u8e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PATH)"
      ],
      "metadata": {
        "id": "kIbMvJhOErT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD (Path + Check Points)"
      ],
      "metadata": {
        "id": "HqyepZFmvZ7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(PATH)\n",
        "loaded_model1=ann1().to(device)\n",
        "loaded_model1.load_state_dict(checkpoint['model_state_dict1'])\n",
        "loaded_model2=anny2().to(device)\n",
        "loaded_model2.load_state_dict(checkpoint['model_state_dict2'])\n",
        "\n",
        "loaded_optimizer1=opt1(1,SGD=True)\n",
        "loaded_optimizer1.load_state_dict(checkpoint['optimizer_state_dict1'])\n",
        "\n",
        "\n",
        "loaded_optimizer2=opt2(1,0,4,SGD=True)\n",
        "loaded_optimizer2.load_state_dict(checkpoint['optimizer_state_dict2'])\n",
        "epoch1 = checkpoint['epoch1']\n",
        "# loss1 = checkpoint['loss1']\n",
        "epoch2 = checkpoint['epoch2']\n",
        "# loss2 = checkpoint['loss2']\n",
        "\n",
        "loss1_array = checkpoint['loss1_array']\n",
        "loss2_array = checkpoint['loss2_array']\n",
        "\n",
        "modely2results=checkpoint['modely2results']"
      ],
      "metadata": {
        "id": "Y1eR_d_Cw2Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMWPUvIzpXOZ"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(modely2results)"
      ],
      "metadata": {
        "id": "t_GFhPbtiFAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot(1,0,1000,loaded_model1,loaded_model2,model22=True)"
      ],
      "metadata": {
        "id": "fJ5EbyR_JvRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(epoch2)\n",
        "plt.plot(loss2_array)\n",
        "plt.xlim(0, 499)\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Function Value')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L6hcvDS73T9S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}