{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, CyclicLR, ReduceLROnPlateau, LinearLR, ExponentialLR\n",
    "import random\n",
    "from torch.autograd.functional import jacobian#, hessian\n",
    "# import AUTOGRAD.FUNCTIONAL.JACOBIAN as jacobian\n",
    "import time\n",
    "import os\n",
    "path = os.getcwd()\n",
    "import json\n",
    "\n",
    "import matplotlib as mpl\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import ChesneyScott\n",
    "from derivation import Grad, Grad_Hess\n",
    "from equation import semilinear\n",
    "from coeff import OU_drift_semi, custom_diff, zero_discount, f_driver, exponential_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pde_params={'dim':2,\n",
    "            'kappa':torch.tensor([0.,1.,0.8,0.6,0.4,0.5,0.3,0.2,0.1,0.7]).to(device), # The first kappa=0 because the drift of wealth process is zero\n",
    "            'theta':torch.tensor([0.,0.1,0.2,0.3,0.4,0.5,0.4,0.3,0.2,0.1]).to(device),\n",
    "            # 'nu':torch.tensor([0.02,0.015,0.11,0.12,0.01,0.013,0.14,0.14,0.01]).to(device), #Hung's coefficient\n",
    "            'nu':torch.tensor([0.2,0.15,0.11,0.12,0.1,0.13,0.14,0.14,0.1]).to(device), # we do not like vanishing diffusion coefficient\n",
    "            # 'lb':torch.tensor([0.,0.15,0.11,0.12,0.13,0.15,0.11,0.12,0.13,0.15]).to(device),   Hung's params\n",
    "            'lb':torch.tensor([0.,.6,1.11,0.12,0.13,0.15,0.11,0.12,0.13,0.15]).to(device), # New params Make closed form solution more sensitive to time\n",
    "            'rho':torch.tensor([0.,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]).to(device),\n",
    "            'eta':torch.tensor([.5]).to(device),\n",
    "            'T': 1.,#torch.tensor([1.]).to(device),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = ChesneyScott(pde_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.2000])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.Tensor([[0.0,1.,0.5],[0.2,.5,0.2]])\n",
    "input[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6077, -1.1051])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 198 ms to generate 19,660,800 iid samples.\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "num_samples = 2**16\n",
    "num_time_intervals = 30\n",
    "max_dim = 10\n",
    "size = num_samples* max_dim * num_time_intervals\n",
    "iid = torch.randn(size=[size]).to(device)\n",
    "print(\"It takes {:.0f} ms to generate {:,} iid samples.\".format(round(1000*(time.time()-t0),6),size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_params={'num_samples':2**12,\n",
    "        'num_time_intervals': 10,\n",
    "        'iid':iid,\n",
    "        'start' : torch.tensor([.9]),  \n",
    "        'end' : torch.tensor([1.1]),\n",
    "        'num_neurons':4\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = OU_drift_semi(pde_params)\n",
    "optimal_diff = m.lb_norm/pde_params['eta']\n",
    "semi_diff_opt = custom_diff(pde_params,optimal_diff)\n",
    "k = zero_discount(pde_params)\n",
    "g = exponential_terminal(pde_params)\n",
    "F = f_driver(pde_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi = semilinear(semi_diff_opt,m,F,k,g,pde_params,sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1, mean loss is 8.61E-01.\n",
      "Training this epoch takes 22.06 ms. So far: 22.91 ms in training.\n",
      "At epoch 1666, mean loss is 2.98E-03.\n",
      "Training this epoch takes 12.52 ms. So far: 21,091.88 ms in training.\n",
      "At epoch 3332, mean loss is 2.86E-03.\n",
      "Training this epoch takes 10.75 ms. So far: 40,164.98 ms in training.\n",
      "Training took 4081 epochs and 48,333.55 ms and the final loss is 2.85E-03.\n"
     ]
    }
   ],
   "source": [
    "semi.train(lr=1e-2,delta_loss=1e-10,max_num_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5169]], grad_fn=<SubBackward0>), tensor([0.4934]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi.Y0(torch.tensor([[1.,0.]]))-1,1-torch.exp(-m.eta-0.5*m.lb_norm**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = OU_drift_semi(pde_params)\n",
    "optimal_cs_diff = lambda x: torch.sqrt(torch.pow(m.lb*x[:,1:],2).sum(axis=1))/pde_params['eta']\n",
    "semi_diff_opt = custom_diff(pde_params,optimal_cs_diff)\n",
    "k = zero_discount(pde_params)\n",
    "g = exponential_terminal(pde_params)\n",
    "F = f_driver(pde_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "semics = semilinear(semi_diff_opt,m,F,k,g,pde_params,sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 1, mean loss is 6.63E-01.\n",
      "Training this epoch takes 37.92 ms. So far: 38.16 ms in training.\n",
      "At epoch 1666, mean loss is 7.55E-04.\n",
      "Training this epoch takes 10.33 ms. So far: 19,859.96 ms in training.\n",
      "At epoch 3332, mean loss is 7.27E-04.\n",
      "Training this epoch takes 14.6 ms. So far: 39,219.6 ms in training.\n",
      "At epoch 4998, mean loss is 7.21E-04.\n",
      "Training this epoch takes 11.79 ms. So far: 58,926.96 ms in training.\n",
      "Training took 5000 epochs and 58,947.09 ms and the final loss is 7.21E-04.\n"
     ]
    }
   ],
   "source": [
    "semics.train(lr=1e-2,delta_loss=1e-10,max_num_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4695]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semics.Y0(torch.tensor([[1.,2.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
