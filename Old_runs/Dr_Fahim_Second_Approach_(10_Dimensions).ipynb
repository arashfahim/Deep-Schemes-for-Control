{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arashfahim/Deep-Schemes-for-Control/blob/main/Dr_Fahim_Second_Approach_(10_Dimensions).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hhguwHlWj02"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive,files\n",
        "# drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPXG1OIOocCx"
      },
      "source": [
        "# Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoRmv9NiS4nZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import misc\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR, CyclicLR, ReduceLROnPlateau, LinearLR, ExponentialLR\n",
        "import random\n",
        "from torch.autograd.functional import jacobian, hessian\n",
        "# import AUTOGRAD.FUNCTIONAL.JACOBIAN as jacobian\n",
        "import time\n",
        "import math\n",
        "\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rcParams['legend.fontsize'] = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYTV5FY_ol3m"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCjiUfN0oiF9",
        "outputId": "416fc0d2-e959-4d72-892f-f48fa591dfb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
            "        0.9000, 1.0000], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-4fab70f4d414>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  delta_t = torch.tensor(T/ num_time_interval).to(device)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_sample=torch.tensor([600000]).to(device) #M\n",
        "# dim=torch.tensor([3]).to(device)          #D\n",
        "num_time_interval=torch.tensor([10]).to(device)    #N\n",
        "# T=torch.tensor([1]).to(device)\n",
        "T=1\n",
        "# T=torch.tensor([1])\n",
        "delta_t = torch.tensor(T/ num_time_interval).to(device)\n",
        "# t_steps = torch.linspace(0,1,num_time_interval).to(device)\n",
        "# sqrt_delta_t = torch.sqrt(delta_t).to(device)\n",
        "# sqrt_delta_t = torch.sqrt(delta_t).to(device)\n",
        "sqrt_delta_t = torch.sqrt(delta_t).to(device)\n",
        "# x_init = torch.zeros(dim)\n",
        "# sigma=torch.sqrt(torch.tensor([2.0]))\n",
        "# lamb=torch.tensor([1.0]).to(device)\n",
        "mu=torch.tensor([1.0]).to(device)\n",
        "sigma=torch.tensor([1.0]).to(device)\n",
        "alpha=torch.tensor([1.0]).to(device)\n",
        "start = torch.tensor([0]).to(device)\n",
        "end = torch.tensor([1]).to(device)\n",
        "num_steps=5\n",
        "num_ite=int(num_time_interval/num_steps)\n",
        "# kappa=torch.tensor([1]).to(device)\n",
        "# theta=torch.tensor([0.4]).to(device)\n",
        "# nu=torch.tensor([0.5]).to(device)\n",
        "# lamb=torch.tensor([0.6]).to(device)\n",
        "# eta=torch.tensor([0.5]).to(device)\n",
        "p=torch.tensor([0.95]).to(device)\n",
        "# rho=torch.tensor([0]).to(device)\n",
        "num_runs=1\n",
        "# x0=torch.ones([1,dim]).to(device)+4\n",
        "# print(x0)\n",
        "\n",
        "t_steps = torch.linspace(0,T,num_time_interval.item()+1).to(device)\n",
        "print(t_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10a7hGrLUnKm"
      },
      "source": [
        "##$n=1  \\quad (d=2)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhcEi1KtUmUI"
      },
      "outputs": [],
      "source": [
        "#dim=torch.tensor([2]).to(device)\n",
        "#kappa=torch.tensor([1]).to(device)\n",
        "# theta=torch.tensor([0.4]).to(device)\n",
        "# nu=torch.tensor([0.2]).to(device)\n",
        "# lamb=torch.tensor([1.5]).to(device)\n",
        "# eta=torch.tensor([0.5]).to(device)\n",
        "# rho=torch.tensor([0.0]).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpXPObUtUeXw"
      },
      "source": [
        "##$n=2 \\quad (d=3)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Se6ZDJGvccW"
      },
      "outputs": [],
      "source": [
        "# dim=torch.tensor([3]).to(device)\n",
        "# kappa=torch.tensor([1, 0.8]).to(device)\n",
        "# theta=torch.tensor([0.1,0.2]).to(device)\n",
        "# nu=torch.tensor([0.2, 0.15]).to(device)\n",
        "# lamb=torch.tensor([0.1, 0.2]).to(device)\n",
        "\n",
        "# # lamb=torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.4, 0.3,0.2,0.1]).to(device)\n",
        "\n",
        "\n",
        "# eta=torch.tensor([0.5]).to(device)\n",
        "# rho=torch.tensor([0.0, 0.0]).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMn4tu4LpeMG"
      },
      "source": [
        "##$n=4 \\quad (d=5) $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACll0xERxKlQ"
      },
      "outputs": [],
      "source": [
        "# dim=torch.tensor([5]).to(device)\n",
        "# kappa=torch.tensor([1, 0.8, 1.1 ,1.3]).to(device)\n",
        "# theta=torch.tensor([0.1,0.2,0.3,0.4]).to(device)\n",
        "# nu=torch.tensor([0.2, 0.15, 0.25, 0.31]).to(device)\n",
        "# lamb=torch.tensor([0.1, 0.2, 0.3,0.4]).to(device)\n",
        "\n",
        "\n",
        "\n",
        "# # lamb=torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.4, 0.3,0.2,0.1]).to(device)\n",
        "# eta=torch.tensor([0.5]).to(device)\n",
        "# rho=torch.tensor([0.0, 0.0, 0.0, 0.0]).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-V42vfIpnwy"
      },
      "source": [
        "##$n=9 \\quad (d=10)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CU_HeMdqFaE"
      },
      "outputs": [],
      "source": [
        "dim=torch.tensor([10]).to(device)\n",
        "kappa=torch.tensor([1, 0.8,1.1, 1.3, 0.95, 0.99,1.02, 1.06, 1.6]).to(device)\n",
        "theta=torch.tensor([0.1, 0.2, 0.3, 0.4, 0.25, 0.15, 0.18, 0.08, 0.91]).to(device)\n",
        "nu=torch.tensor([0.2, 0.15, 0.25, 0.2, 0.2, 0.1, 0.22, 0.2, 0.15]).to(device)\n",
        "# lamb=torch.tensor([1.5, 1.1, 2, 0.8, 0.5, 1.7, 0.9, 1, 0.9]).to(device)\n",
        "\n",
        "lamb=torch.tensor([0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4,0.45,0.5]).to(device)\n",
        "\n",
        "# # lamb=torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5, 0.4, 0.3,0.2,0.1]).to(device)\n",
        "\n",
        "eta=torch.tensor([0.5]).to(device)\n",
        "rho=torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1u6Df1WxADx"
      },
      "outputs": [],
      "source": [
        "def H(x,z,gamma): #### out_shape =([M]) |  input:  x_shape=[M,D,1],  z shape = [M,D,1],gamma shape= [M,dim,dim]\n",
        "  result=torch.zeros([x.shape[0]]).to(device)\n",
        "  for i in range(dim-1):\n",
        "    result+=kappa[i]*(theta[i]-x[:,i+1,0])*z[:,i+1,0]+0.5*torch.pow(nu[i],2)*gamma[:,i+1,i+1]-rho[i]*lamb[i]*nu[i]*(z[:,0,0]*gamma[:,0,i+1])/gamma[:,0,0] \\\n",
        "    + 0.5*torch.pow(rho[i]*nu[i]*gamma[:,0,i+1],2)/gamma[:,0,0]\n",
        "    # print(result)\n",
        "  result1=result-0.5*torch.sum(torch.square(lamb))*torch.pow(z[:,0,0],2)/gamma[:,0,0]\n",
        "  return result1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "284WpU907Cii"
      },
      "outputs": [],
      "source": [
        "def F(x,z,a): #### out_shape =([M]) |  input:  x_shape=[M,D,1],  z shape = [M,D,1],a_shape= [M,D,D]  #This is for rho=0\n",
        "  result=torch.zeros([x.shape[0]]).to(device)\n",
        "  for i in range(dim-1):\n",
        "    result+=-kappa[i]*(theta[i]-x[:,i+1,0])*z[:,i+1,0]\n",
        "    # print(result)\n",
        "  result1=result-torch.sqrt(torch.sum(torch.square(lamb)))*torch.abs(z[:,0,0])*torch.sqrt(a[:,0,0])\n",
        "  return result1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3feRG9_9PKl2"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTguvp5GPHkf"
      },
      "outputs": [],
      "source": [
        "def g1(x): #out_shape= [M,1,1]   | input:  x_shape=[M,D,1]\n",
        "  result=1-torch.exp(-eta*x[:,0])\n",
        "  result=torch.reshape(result,(result.shape[0],1,1))\n",
        "  return result\n",
        "\n",
        "def update(data,delta_w,L):#output: data=(M,D,1) ,  #input data=(M,D,1), delta_w=[M,D,1]\n",
        "    dx = torch.bmm(L,delta_w)\n",
        "    data = data + dx\n",
        "    data=data\n",
        "    return data\n",
        "\n",
        "def der1(x,f):  #output= [M,D,1], input: x=[M,D,1]\n",
        "  Du=torch.zeros(x.shape[0],dim).to(device)\n",
        "  xin=x.clone().detach()\n",
        "  xin.requires_grad=True\n",
        "  u=f(xin)\n",
        "  print(u)\n",
        "  print('u shape', u.shape)\n",
        "  Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
        "                          allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
        "  print('Du shape', Du.shape)\n",
        "  Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
        "  print('Du shape', Du.shape)\n",
        "  return Du\n",
        "\n",
        "\n",
        "def grad_hessian1(x,f_,c=False): #output= [M,D,D], #input: x=[M,D,1]   #c=False => data=[M,D]   #c=True =>data=[M,D,1]\n",
        "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
        "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
        "\n",
        "    if c==False:\n",
        "      # xin=x.unsqueeze(2).clone().detach()\n",
        "      xin=x.clone().detach()\n",
        "      xin.requires_grad=True\n",
        "      # print(xin.shape)\n",
        "      # xin.requires_grad=True\n",
        "      u=f_(xin).squeeze(2)\n",
        "      # print(u.shape)\n",
        "      # xin=xin.unsqueeze(2)\n",
        "      # print(xin.shape)\n",
        "      Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
        "                           allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
        "      hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
        "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
        "    else:\n",
        "      xin=x.clone().detach()\n",
        "      xin.requires_grad=True\n",
        "      u=f_(xin.squeeze(2))\n",
        "    # print(\"xin shape: \",xin.shape)\n",
        "      Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
        "                           allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
        "    # print(\"Du shape:\",Du.shape)\n",
        "    # print(\"-----\")\n",
        "    # print(torch.autograd.grad(outputs=[Du[:,0,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,0,:]),\n",
        "    #                        allow_unused=True,retain_graph=True,create_graph=True)[0].shape)\n",
        "      hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
        "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
        "    # print(\"D2u shape:\",hess_temp.shape)\n",
        "    Du = torch.reshape(Du,(Du.shape[0],dim,1))\n",
        "    # print(\"Du shape:\",Du.shape)\n",
        "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
        "    # print(\"D2u shape:\",hess_temp.shape)\n",
        "\n",
        "    # print(hess_temp)\n",
        "    return Du, hess_temp\n",
        "def hessian1(x,model2z):  #x=[M,D,1]  output= [M,D,1]\n",
        "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
        "    # print(\"D2u shape:\",hess_temp.shape)\n",
        "\n",
        "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
        "    xin=x.clone().detach()\n",
        "    # print('this is xin')\n",
        "    # print(xin)\n",
        "    xin.requires_grad=True\n",
        "    Du = model2z(xin.squeeze(2))\n",
        "    # print('this is Du')\n",
        "    # print(Du)\n",
        "    # print('this is Du shape')\n",
        "    # print(Du.shape)\n",
        "    # print('this is Du 1')\n",
        "    # print(Du[:,1,:])\n",
        "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
        "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
        "\n",
        "    # print(\"D2u shape:\",hess_temp.shape)\n",
        "\n",
        "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
        "\n",
        "    # print(hess_temp)\n",
        "    return hess_temp\n",
        "def L_matrix(ite,x,model):  #x=[M,D]\n",
        "  L=torch.zeros(x.shape[0],dim,dim).to(device)\n",
        "\n",
        "  L[:,0,0]=model[ite](x)\n",
        "  for i in range(dim-1):\n",
        "    L[:,i+1,i+1]=nu[i]\n",
        "  return L\n",
        "def opt_quad():\n",
        "  return torch.pow(torch.sum(torch.pow(lamb,2)),0.5)/eta\n",
        "\n",
        "def L_matrix_test(ite,x,model1):  #This matrix is a matrix with L00 = 0.6 and L11=nu\n",
        "  L=torch.zeros(x.shape[0],dim,dim)\n",
        "  L[:,0,0]=opt_quad()\n",
        "  for i in range(dim-1):\n",
        "    L[:,i+1,i+1]=nu[i]\n",
        "  return L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xD2CDmDzeyBO"
      },
      "outputs": [],
      "source": [
        "def sample(ite): #output: [M,D]  #input: a scalar\n",
        "    # x=(torch.rand(num_sample,1))\n",
        "    # delta_w=torch.rand(num_sample,dim).to(device)*delta_t\n",
        "    # x=(torch.rand(num_sample,1))\n",
        "    if ite==num_ite:\n",
        "      x=(start_list[ite-1])+(end_list[ite-1]-start_list[ite-1])*torch.rand(num_sample,dim)\n",
        "      # x=(start_list[ite])+(end_list[ite]-start_list[ite])*delta_w\n",
        "    else:\n",
        "      x=(start_list[ite-1])+((end_list[ite-1])-(start_list[ite-1]))*torch.rand(num_sample,dim)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkOz0tC-CAv5"
      },
      "outputs": [],
      "source": [
        "def gradient(x,f): #input=  [M,D,1]    #output= [M,D,1]\n",
        "  xin=x.clone().detach()\n",
        "  xin.requires_grad=True\n",
        "  u=f(xin.squeeze(2));\n",
        "  Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
        "  return Du"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k5b4A20OsL0"
      },
      "source": [
        "# Generating Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTEIDG15ZauM"
      },
      "source": [
        "## List of Intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s-WORBH5mLE"
      },
      "outputs": [],
      "source": [
        "# start_list=[2.5,2,1.5,1,0.5,0]\n",
        "# end_list=[3.5,4,4.5,5,5.5,6]\n",
        "\n",
        "# start_list=[10,5,0]\n",
        "# end_list=[12,17,22]\n",
        "# start_list=[10,5,0]\n",
        "# end_list=[12,17,22]\n",
        "\n",
        "# start_list=[3,1,-1]\n",
        "# end_list=[5,7,9]\n",
        "\n",
        "\n",
        "# start_list=[2,1,0]\n",
        "# end_list=[4,5,6]\n",
        "\n",
        "# start_list=[16,8,0]\n",
        "# end_list=[18,26,34]\n",
        "\n",
        "# start_list=[-1,-3,-5]\n",
        "# end_list=[1,3,5]\n",
        "\n",
        "\n",
        "# start_list=[3,1.5,0]\n",
        "# end_list=[5,6.5,8]\n",
        "\n",
        "start_list=[4,2,0]\n",
        "end_list=[6,8,10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm4dIgPf9Aiz"
      },
      "outputs": [],
      "source": [
        "#M = number of samples\n",
        "#D = number of dimension\n",
        "#N = num_time_interval\n",
        "\n",
        "# delta_w=sqrt_delta_t.cpu()*torch.randn(num_sample,dim,num_time_interval)   #[M,D,N]\n",
        "\n",
        "# print(delta_w.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG9GSSyZaELQ"
      },
      "outputs": [],
      "source": [
        "def remove_outliers(ite,x):\n",
        "    path_min = torch.min(x.flatten(start_dim=1), dim=1)[0]\n",
        "    # print(path_min)\n",
        "    path_max = torch.max(x.flatten(start_dim=1), dim=1)[0]\n",
        "    # print(path_max)\n",
        "    pick_sample = torch.logical_and(path_max < end_list[ite],path_min > start_list[ite])\n",
        "    # print(pick_sample)\n",
        "    x_new=x[pick_sample,:]\n",
        "    # print('the percent of paths removed is {}%'.format((x.shape[0]-x_new.shape[0])/(x.shape[0])*100))\n",
        "    return x_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKZ_q2P-od4c"
      },
      "outputs": [],
      "source": [
        "def data_generator2(ite,t1,t2,delta_w): #input: ite=scalar, t1=scalar, t2=scalar     #output: [M,D,1]\n",
        "  count=0\n",
        "  l=[]\n",
        "  k=[]\n",
        "  # start_=start_list[ite-1]\n",
        "  # print(start_)\n",
        "  # end_=end_list[ite-1]\n",
        "  xx=torch.zeros([num_sample,dim,num_time_interval+1]).to(device)\n",
        "  # xx=torch.zeros([num_sample,dim,t2-t1+2]).to(device)\n",
        "  # xx[:,:,t1]=start_list[ite-1]+(end_list[ite-1]-start_list[ite-1])*(torch.rand(num_sample,dim))\n",
        "  xx[:,:,t1]=start_list[ite-1]+(end_list[ite-1]-start_list[ite-1])*delta_w[:,:,t1-1]\n",
        "\n",
        "  # if ite==num_ite:\n",
        "  for i in range(t1, t2+1):\n",
        "    L=L_matrix(ite-1,xx[:,:,i].to(device),model1)\n",
        "    # L=L_matrix_test(ite-1,xx[:,:,i].to(device),model1)\n",
        "    # print(L)\n",
        "    xx[:,:,i+1]=(xx[:,:,i]+torch.bmm(L,delta_w[:,:,i].unsqueeze(2)).squeeze(2)).clone().detach()\n",
        "  path_min = torch.min(xx[:,:,t1:t2+2].flatten(start_dim=1), dim=1)[0]\n",
        "  # print(path_min)\n",
        "  path_max = torch.max(xx[:,:,t1:t2+2].flatten(start_dim=1), dim=1)[0]\n",
        "  # print(path_max)\n",
        "  pick_sample = torch.logical_and(path_max < end_list[ite],path_min > start_list[ite])\n",
        "  # print(pick_sample)\n",
        "  xx_new=xx[pick_sample,:,:]\n",
        "  delta_w_new=delta_w[pick_sample,:,:]\n",
        "\n",
        "  return xx_new.cpu(),delta_w_new.cpu()\n",
        "  # return xx.cpu(),delta_w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-ZHHU-S79jN"
      },
      "source": [
        "# Initilizing Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1I44Ai9uXEc"
      },
      "outputs": [],
      "source": [
        "class ann1(nn.Module):\n",
        "    # def __init__(self):\n",
        "        # super(ann1, self).__init__()\n",
        "        # self.linear_stack = nn.Sequential(\n",
        "        #     nn.Linear(dim, 128),\n",
        "        #     nn.LogSigmoid(),\n",
        "        #     nn.Linear(128, 128),\n",
        "        #     nn.LogSigmoid(),\n",
        "        #     nn.Linear(128, 128),\n",
        "        #     nn.LogSigmoid(),\n",
        "        #     nn.Linear(128, 1),\n",
        "        #     nn.LogSigmoid(),\n",
        "        #     # nn.ReLU()\n",
        "        # )\n",
        "    def __init__(self):\n",
        "        super(ann1, self).__init__()\n",
        "        self.linear_stack = nn.Sequential(\n",
        "\n",
        "            nn.Linear(dim, 256),\n",
        "            # nn.BatchNorm1d(256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256,256),\n",
        "            # nn.BatchNorm1d(256),\n",
        "            nn.Tanh(),\n",
        "            # nn.Linear(256,256),\n",
        "            # nn.Tanh(),\n",
        "            nn.Linear(256, 128),\n",
        "            # nn.BatchNorm1d(128),\n",
        "            nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            nn.Linear(128,1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_stack(x)\n",
        "        logits=torch.reshape(logits,(x.shape[0],))\n",
        "        # logits=torch.flatten(logits)\n",
        "        # logits=torch.reshape(logits,[x.shape[0]])\n",
        "        # logits=nn.ELU()(logits)+1.5\n",
        "        # logits=torch.reshape(logits,(x.shape[0],1,1))\n",
        "        return torch.max(logits,-logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPCzSVnUzh1C"
      },
      "outputs": [],
      "source": [
        "model1= nn.ModuleList([ann1() for i in range(num_ite)]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8hxiaRN3o_o",
        "outputId": "18711a1d-d220-41a4-d8a0-3e04a1d2410c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.0440, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(torch.mean(model1[0](torch.ones(100,dim).to(device))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwNCvaaR8ROE"
      },
      "source": [
        "# Loss function 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NQIIEAVDuDI"
      },
      "outputs": [],
      "source": [
        "######## TEST#######################\n",
        "\n",
        "def loss_fn1(ite,x,delta_w,model):  #x=[M,D,1]  delta_w = [M,D,1]\n",
        "    L=torch.zeros(x.shape[0],dim,dim).to(device)\n",
        "    L[:,0,0]=model(x.squeeze(2))\n",
        "    for i in range(dim-1):\n",
        "      L[:,i+1,i+1]=nu[i]\n",
        "    n=update(x,delta_w,L)\n",
        "    if ite==num_ite:\n",
        "      Y=g1(n).to(device)\n",
        "      # print(Y.shape)\n",
        "      # print(\"Y= \")\n",
        "      # print(Y)\n",
        "      Z,hessian=grad_hessian1(n,g1)\n",
        "      # print(\"Z= \")\n",
        "      # print(Z)\n",
        "      # print(\"Hessian= \")\n",
        "      # print(hessian)\n",
        "    else:\n",
        "      n=remove_outliers(ite,n)\n",
        "      L=L[0:n.shape[0],:,:]\n",
        "      Y=modely2[t2+1](n.squeeze(2))\n",
        "      Z,hessian=grad_hessian1(n.squeeze(2),modely2[t2+1])\n",
        "      Z=Z+0.0001\n",
        "      hessian=hessian+0.0001\n",
        "      # print(\"Z= \", Z)\n",
        "      # print(\"hessian= \",hessian)\n",
        "    LTL=torch.bmm(torch.transpose(L,1,2),L)\n",
        "    # print(\"LTL =\", LTL)\n",
        "    LTLhessian=torch.bmm(LTL,hessian)\n",
        "    # print(\"LTLhessian =\", LTLhessian)\n",
        "    Tr=LTLhessian.diagonal(offset=0,dim1=-1,dim2=-2).sum(-1)\n",
        "    # print(\"Trace =\", Tr)\n",
        "    H_=H(n,Z,hessian)\n",
        "    # print(\"H = \", H_)\n",
        "    F_=F(n,Z,LTL)\n",
        "    # print(\"F= \", F_)\n",
        "    # loss=torch.mean(torch.pow(H_-0.5*Tr+F_,2))\n",
        "    loss=torch.mean(torch.pow(H_-0.5*Tr+F_,2))\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN9-HECRf3d8"
      },
      "source": [
        "# Initializing Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwqKlJjgf26X"
      },
      "outputs": [],
      "source": [
        "class anny2(nn.Module): #input [M,D], #output [M,1,1]\n",
        "    def __init__(self):\n",
        "        super(anny2, self).__init__()\n",
        "        self.linear_stack = nn.Sequential(\n",
        "            nn.Linear(dim, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128,256),\n",
        "            nn.Tanh(),\n",
        "            # nn.Linear(256, 256),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(256,256),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(256,256),\n",
        "            # nn.Tanh(),\n",
        "            nn.Linear(256,128),\n",
        "            nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            nn.Linear(128,1),\n",
        "\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_stack(x)\n",
        "\n",
        "        logits=torch.reshape(logits,(x.shape[0],1,1))\n",
        "        return logits\n",
        "        # return torch.min(logits,-logits)\n",
        "\n",
        "class annz2(nn.Module): #input [M,D], #output [M,D,1]\n",
        "    def __init__(self):\n",
        "        super(annz2, self).__init__()\n",
        "        self.linear_stack = nn.Sequential(\n",
        "            nn.Linear(dim, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128,256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(256,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            # nn.Linear(128,128),\n",
        "            # nn.Tanh(),\n",
        "            nn.Linear(128,dim),\n",
        "            # nn.ReLU(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_stack(x)\n",
        "\n",
        "        logits=torch.reshape(logits,(x.shape[0],dim,1))\n",
        "        return logits\n",
        "        # return torch.min(logits,-logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeiTG1Amy8Ze"
      },
      "outputs": [],
      "source": [
        "modely= anny2().to(device)\n",
        "modelz= annz2().to(device)\n",
        "modely2= nn.ModuleList([anny2() for i in range(num_time_interval+1)]).to(device)\n",
        "modelz2= nn.ModuleList([annz2() for i in range(num_time_interval+1)]).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNOtxgVuXtMO"
      },
      "source": [
        "# Loss Function 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgIiN2dC9ZXS"
      },
      "outputs": [],
      "source": [
        "def loss_fn2(ite,data,delta_w):\n",
        "\n",
        "  bb=L_matrix(ite-1,data[:,:,t1],model1).to(device)\n",
        "  ss=torch.pow(bb,2)\n",
        "  loss=0\n",
        "  Y=modely2[t1](data[:,:,t1])\n",
        "  # print(data[:,:,t1].shape)\n",
        "  # gradY,hessianY=grad_hessian1(data[:,:,t1],modely2[t1])\n",
        "  Z=gradient(data[:,:,t1].unsqueeze(2),modely2[t1]).to(device)\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(t1,t2):\n",
        "    Y=Y.flatten()+F(data[:,:,i].unsqueeze(2),Z,ss)*delta_t+torch.bmm(torch.bmm(torch.transpose(Z,1,2),bb),delta_w[:,:,i].unsqueeze(2)).flatten()\n",
        "    bb=L_matrix(ite-1,data[:,:,i+1],model1).to(device)\n",
        "    # bb=L_matrix_test(ite-1,data[:,:,i+1],model1).to(device)\n",
        "    ss=torch.pow(bb,2)\n",
        "    # Z=modelz2[i+1](data[:,:,i+1])\n",
        "    Z=gradient(data[:,:,i+1].unsqueeze(2),modely2[i+1])\n",
        "    # loss+=torch.mean(torch.mean(torch.square(gradient(data[:,:,i+1].unsqueeze(2),Y)-Z).squeeze(2),1))\n",
        "  Y=Y.flatten()+F(data[:,:,t2].unsqueeze(2),Z,ss)*delta_t+torch.bmm(torch.bmm(torch.transpose(Z,1,2),bb),delta_w[:,:,t2].unsqueeze(2)).flatten()\n",
        "\n",
        "\n",
        "  if ite==num_ite:\n",
        "\n",
        "    loss+=torch.mean(torch.square(Y-g1(data[:,:,t2+1]).flatten()))\n",
        "\n",
        "  else:\n",
        "\n",
        "    loss+=torch.mean(torch.square(Y-modely2[t2+1](data[:,:,t2+1]).flatten()))\n",
        "  return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw28xC2BJZa_"
      },
      "source": [
        "#Training Both Model1 And Model2 Simultaneously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RONIDEfie4Uo"
      },
      "outputs": [],
      "source": [
        "from IPython.core.pylabtools import figsize\n",
        "batch_size=64*2*2*2*2\n",
        "num_epoch1 =200\n",
        "num_epoch2=200\n",
        "total_batch=int((num_sample/batch_size))\n",
        "index=0\n",
        "last_loss1 = 0\n",
        "last_loss2=0\n",
        "loss_delta1 = 0\n",
        "loss_delta2=0\n",
        "loss_treshold1 = 0\n",
        "loss_treshold2=0\n",
        "loss_tmp1=[]\n",
        "loss_tmp2=[]\n",
        "# my_index=torch.arange(num_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_-Z_M5r_68k",
        "outputId": "595f15bb-51ea-46df-cd7c-085122d118d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1024\n"
          ]
        }
      ],
      "source": [
        "print(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EWn3QGm9GC9"
      },
      "source": [
        "# Optimizer 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e0RAX97VjkS"
      },
      "outputs": [],
      "source": [
        "# lr1=[0.1, 0.1, 0.1, 0.1, 0.1]\n",
        "# lr1=[0.000000015, 0.05, 0.01, 0.01, 0.01]\n",
        "lr1=[0.00005, 0.00001, 0.0005, 0.0005, 0.0005]\n",
        "def opt1(ite):\n",
        "  l=[]\n",
        "  lr_=lr1[ite]\n",
        "  # optimizer=optim.SGD(model1[ite-1].parameters(), lr=lr_, momentum=0.9)\n",
        "  optimizer=optim.Adam(model1[ite-1].parameters(),lr=0.000005)\n",
        "  # optimizer=optim.Adam(model1[ite-1].parameters(),lr=lr_,weight_decay=1e-5)\n",
        "  scheduler = ExponentialLR(optimizer, gamma =0.9)\n",
        "  # optimizer = torch.optim.Adam(model1[ite-1].parameters(), lr=lr_, weight_decay=1e-5)\n",
        "\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pn7bWn19OH6"
      },
      "source": [
        "#Optimizer 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UpXLsui5rPS"
      },
      "outputs": [],
      "source": [
        "# lr2=[0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
        "# lr2=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
        "# lr2=[0.015, 0.015, 0.015, 0.015, 0.015, 0.015, 0.015,0.015, 0.015, 0.015, 0.015, 0.015, 0.015]\n",
        "lr2=[0.000001, 0.000001, 0.0001, 0.00001, 0.00001, 0.00001, 0.00001,0.00001, 0.00001, 0.00001, 0.00001, 0.00001, 0.00001]\n",
        "def opt2(ite,t1,t2):\n",
        "  l=[]\n",
        "  lr_=lr2[ite]\n",
        "\n",
        "  # for i in range(t1,t2+1):\n",
        "  #   l+=list(modely2[i].parameters())+list(modelz2[i].parameters())\n",
        "\n",
        "  # l=list(modely2[t1].parameters())\n",
        "  for i in range(t1,t2+1):\n",
        "    l+=list(modely2[i].parameters())\n",
        "  # optimizer=optim.Adam(l,lr=0.005)\n",
        "  optimizer=optim.SGD(l, lr=lr_, momentum=0.9)\n",
        "  # optimizer = torch.optim.Adam(l, lr=lr_, weight_decay=1e-5)\n",
        "  scheduler = ExponentialLR(optimizer, gamma =0.9)\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6ucFMmeGwbM"
      },
      "source": [
        "# Value function\n",
        "$u(t,x,v)=-e^{-\\eta x-0.5(T-t)\\lambda^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLCE-5aBctkP"
      },
      "outputs": [],
      "source": [
        "def V1(t,x):\n",
        "  result=1-torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1L7bU-QD6Pk",
        "outputId": "886eeb72-171b-4877-c6f6-b3f34a775225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is 1th run\n",
            "tensor([[3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], device='cuda:0')\n",
            "At iteration 2:\n",
            "5\n",
            "9\n",
            "Loss function 1 at epoch 0 is 2.240188041469e-05  |  Loss delta: 2.2401880414690822e-05   |    Training time: 14.84069s\n",
            "Loss function 1 at epoch 10 is 2.496941320373e-09  |  Loss delta: 2.823869493795428e-08   |    Training time: 14.52255s\n",
            "Loss function 1 at epoch 20 is 1.692157525213e-10  |  Loss delta: 1.2270406912762155e-08   |    Training time: 14.61558s\n",
            "Loss function 1 at epoch 30 is -1.330624499474e-11  |  Loss delta: 1.9495642877842556e-09   |    Training time: 14.58236s\n",
            "Loss function 1 at epoch 40 is -7.924999545494e-11  |  Loss delta: 4.3705505881064255e-10   |    Training time: 14.57955s\n",
            "Loss function 1 at epoch 50 is -1.131859467263e-10  |  Loss delta: 2.0173689108116122e-10   |    Training time: 14.60033s\n",
            "Loss function 1 at epoch 60 is -1.844058239442e-11  |  Loss delta: 1.5262180408370796e-10   |    Training time: 14.34933s\n",
            "Loss function 1 at epoch 70 is -6.121932127900e-11  |  Loss delta: 9.467947259533815e-11   |    Training time: 14.88900s\n",
            "Loss function 1 at epoch 80 is 2.163331389027e-10  |  Loss delta: 3.043986451789493e-10   |    Training time: 14.29007s\n",
            "Loss function 1 at epoch 90 is -1.138079908092e-11  |  Loss delta: 8.731616124579844e-11   |    Training time: 14.63550s\n",
            "Loss function 1 at epoch 100 is -3.662112280090e-11  |  Loss delta: 6.918237510644687e-11   |    Training time: 14.05090s\n",
            "Loss function 1 at epoch 110 is -5.749824227852e-12  |  Loss delta: 4.767109623715626e-11   |    Training time: 14.35964s\n",
            "Loss function 1 at epoch 120 is -1.239289920685e-12  |  Loss delta: 4.6399460662538416e-11   |    Training time: 14.41295s\n",
            "Loss function 1 at epoch 130 is 2.800682602500e-11  |  Loss delta: 8.656685090979721e-11   |    Training time: 14.57063s\n",
            "Loss function 1 at epoch 140 is -1.956023537586e-11  |  Loss delta: 4.2094841501416624e-11   |    Training time: 14.42306s\n",
            "Loss function 1 at epoch 150 is -3.309019724895e-13  |  Loss delta: 3.8564134524632365e-11   |    Training time: 14.59753s\n",
            "Loss function 1 at epoch 160 is 6.214646852687e-13  |  Loss delta: 2.863464673708016e-11   |    Training time: 14.65261s\n",
            "Loss function 1 at epoch 170 is -2.576008850674e-12  |  Loss delta: 3.5187575075923405e-11   |    Training time: 14.22505s\n",
            "Loss function 1 at epoch 180 is 2.496711171140e-12  |  Loss delta: 5.0612701529839654e-11   |    Training time: 14.22671s\n",
            "Loss function 1 at epoch 190 is 2.693612694005e-12  |  Loss delta: 3.514961932626903e-11   |    Training time: 14.56948s\n",
            "The value of model1 at time step 9 is 1.9751245975494385\n",
            "The true optimal quadratic is 1.9595918655395508\n",
            "Loss function 2 at epoch 0 is 6.169984117150e-02  |  Loss delta: 0.06169984117150307   |    Training time: 4.51299s\n",
            "Loss function 2 at epoch 10 is -1.828705891967e-03  |  Loss delta: 0.0283069871366024   |    Training time: 4.46642s\n",
            "Loss function 2 at epoch 20 is -7.404573261738e-04  |  Loss delta: 0.02249913103878498   |    Training time: 4.61014s\n",
            "Loss function 2 at epoch 30 is -2.477625384927e-03  |  Loss delta: 0.020083149895071983   |    Training time: 4.55977s\n",
            "Loss function 2 at epoch 40 is 9.886566549540e-04  |  Loss delta: 0.01946699619293213   |    Training time: 4.38192s\n",
            "Loss function 2 at epoch 50 is 5.932860076427e-04  |  Loss delta: 0.01718379184603691   |    Training time: 4.54157s\n",
            "Loss function 2 at epoch 60 is 6.473381072283e-04  |  Loss delta: 0.015411593951284885   |    Training time: 4.36348s\n",
            "Loss function 2 at epoch 70 is 5.467142909765e-05  |  Loss delta: 0.013491392135620117   |    Training time: 4.46959s\n",
            "Loss function 2 at epoch 80 is -1.153784804046e-03  |  Loss delta: 0.012250424362719059   |    Training time: 4.62477s\n",
            "Loss function 2 at epoch 90 is 1.968713477254e-04  |  Loss delta: 0.012002925388514996   |    Training time: 4.59828s\n",
            "Loss function 2 at epoch 100 is 1.722639426589e-03  |  Loss delta: 0.01254966575652361   |    Training time: 4.51124s\n",
            "Loss function 2 at epoch 110 is 1.320587471128e-04  |  Loss delta: 0.011182373389601707   |    Training time: 4.74313s\n",
            "Loss function 2 at epoch 120 is -9.244913235307e-04  |  Loss delta: 0.010815596207976341   |    Training time: 4.90247s\n",
            "Loss function 2 at epoch 130 is 1.523131504655e-04  |  Loss delta: 0.00965598039329052   |    Training time: 4.39454s\n",
            "Loss function 2 at epoch 140 is 4.402538761497e-04  |  Loss delta: 0.01030315924435854   |    Training time: 4.26836s\n",
            "Loss function 2 at epoch 150 is -9.682578966022e-04  |  Loss delta: 0.00980708934366703   |    Training time: 4.37866s\n",
            "Loss function 2 at epoch 160 is -5.097705870867e-04  |  Loss delta: 0.009514180943369865   |    Training time: 4.51284s\n",
            "Loss function 2 at epoch 170 is -1.098650507629e-03  |  Loss delta: 0.009663000702857971   |    Training time: 4.54084s\n",
            "Loss function 2 at epoch 180 is -5.471045151353e-04  |  Loss delta: 0.008950821124017239   |    Training time: 4.55347s\n",
            "Loss function 2 at epoch 190 is 2.786777913570e-04  |  Loss delta: 0.00998260173946619   |    Training time: 4.55431s\n",
            "The value of modely2 at time step 5 is 0.9901426434516907\n",
            "The theoretical solution is 0.8244795799255371\n",
            "The  relative percentage error is 0.16731232072647956: \n",
            "tensor([[5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]], device='cuda:0')\n",
            "At iteration 1:\n",
            "0\n",
            "4\n",
            "Loss function 1 at epoch 200 is 2.304694795338e-02  |  Loss delta: 0.02304694801568985   |    Training time: 22.19886s\n",
            "Loss function 1 at epoch 210 is -4.856197629124e-04  |  Loss delta: 0.0017169935163110495   |    Training time: 22.12698s\n",
            "Loss function 1 at epoch 220 is -6.844188319519e-04  |  Loss delta: 0.0017558556282892823   |    Training time: 22.24173s\n",
            "Loss function 1 at epoch 230 is 1.188367605209e-06  |  Loss delta: 0.0022819244768470526   |    Training time: 21.63957s\n",
            "Loss function 1 at epoch 240 is -6.839637644589e-04  |  Loss delta: 0.0021404915023595095   |    Training time: 21.75978s\n",
            "Loss function 1 at epoch 250 is 1.429812982678e-05  |  Loss delta: 0.001965505303815007   |    Training time: 21.75322s\n",
            "Loss function 1 at epoch 260 is -5.848438013345e-04  |  Loss delta: 0.0019247326999902725   |    Training time: 21.51891s\n",
            "Loss function 1 at epoch 270 is -4.241470014676e-04  |  Loss delta: 0.0019177248468622565   |    Training time: 21.15591s\n",
            "Loss function 1 at epoch 280 is 1.897392561659e-03  |  Loss delta: 0.004253269173204899   |    Training time: 20.77760s\n",
            "Loss function 1 at epoch 290 is 7.247754838318e-04  |  Loss delta: 0.0022962866351008415   |    Training time: 21.18713s\n",
            "Loss function 1 at epoch 300 is 1.735216937959e-05  |  Loss delta: 0.0019756960682570934   |    Training time: 20.69948s\n",
            "Loss function 1 at epoch 310 is -9.230794385076e-04  |  Loss delta: 0.001696484163403511   |    Training time: 20.97809s\n",
            "Loss function 1 at epoch 320 is -3.676054766402e-04  |  Loss delta: 0.001333631225861609   |    Training time: 20.94979s\n",
            "Loss function 1 at epoch 330 is 1.212890492752e-04  |  Loss delta: 0.0017867456190288067   |    Training time: 20.65853s\n",
            "Loss function 1 at epoch 340 is 6.135371513665e-04  |  Loss delta: 0.003167131682857871   |    Training time: 20.55571s\n",
            "Loss function 1 at epoch 350 is -6.784233264625e-04  |  Loss delta: 0.0017140698619186878   |    Training time: 20.57842s\n",
            "Loss function 1 at epoch 360 is -2.918614773080e-04  |  Loss delta: 0.0018223276128992438   |    Training time: 20.61628s\n",
            "Loss function 1 at epoch 370 is -2.898287493736e-04  |  Loss delta: 0.002350500086322427   |    Training time: 20.63943s\n",
            "Loss function 1 at epoch 380 is -5.820111837238e-04  |  Loss delta: 0.0016288545448333025   |    Training time: 20.65934s\n",
            "Loss function 1 at epoch 390 is -4.610125906765e-04  |  Loss delta: 0.00217943312600255   |    Training time: 19.34255s\n",
            "The value of model1 at time step 4 is 8.222860336303711\n",
            "The true optimal quadratic is 1.9595918655395508\n",
            "Loss function 2 at epoch 200 is 1.229067204520e+00  |  Loss delta: 1.2385942935943604   |    Training time: 3.90976s\n",
            "Loss function 2 at epoch 210 is -4.385983943939e-02  |  Loss delta: 0.5430324077606201   |    Training time: 3.88719s\n",
            "Loss function 2 at epoch 220 is -3.016966581345e-02  |  Loss delta: 0.2376134991645813   |    Training time: 3.81705s\n",
            "Loss function 2 at epoch 230 is -8.188329637051e-03  |  Loss delta: 0.10884708911180496   |    Training time: 3.82095s\n",
            "Loss function 2 at epoch 240 is -3.165617585182e-03  |  Loss delta: 0.05695284157991409   |    Training time: 3.92830s\n",
            "Loss function 2 at epoch 250 is -6.633065640926e-04  |  Loss delta: 0.03579485043883324   |    Training time: 3.84593s\n",
            "Loss function 2 at epoch 260 is -8.560605347157e-04  |  Loss delta: 0.026396378874778748   |    Training time: 3.80468s\n",
            "Loss function 2 at epoch 270 is 4.477854818106e-04  |  Loss delta: 0.023188740015029907   |    Training time: 3.87266s\n",
            "Loss function 2 at epoch 280 is 1.555779948831e-03  |  Loss delta: 0.02151111513376236   |    Training time: 3.78373s\n",
            "Loss function 2 at epoch 290 is 1.828595995903e-04  |  Loss delta: 0.02076445147395134   |    Training time: 3.78186s\n",
            "Loss function 2 at epoch 300 is -3.238413482904e-04  |  Loss delta: 0.01997292973101139   |    Training time: 3.85448s\n",
            "Loss function 2 at epoch 310 is 7.973797619343e-04  |  Loss delta: 0.02034590393304825   |    Training time: 3.82498s\n",
            "Loss function 2 at epoch 320 is 2.391902729869e-03  |  Loss delta: 0.020671389997005463   |    Training time: 3.85078s\n",
            "Loss function 2 at epoch 330 is -1.298069953918e-03  |  Loss delta: 0.019263384863734245   |    Training time: 3.82349s\n",
            "Loss function 2 at epoch 340 is -2.537667751312e-05  |  Loss delta: 0.019352423027157784   |    Training time: 3.86214s\n",
            "Loss function 2 at epoch 350 is -2.792812883854e-04  |  Loss delta: 0.018845433369278908   |    Training time: 3.91189s\n",
            "Loss function 2 at epoch 360 is 5.437415093184e-04  |  Loss delta: 0.018830662593245506   |    Training time: 3.72646s\n",
            "Loss function 2 at epoch 370 is -1.760998740792e-03  |  Loss delta: 0.01814316399395466   |    Training time: 3.78424s\n",
            "Loss function 2 at epoch 380 is 5.601085722446e-04  |  Loss delta: 0.018676748499274254   |    Training time: 3.81336s\n",
            "Loss function 2 at epoch 390 is 6.541050970554e-05  |  Loss delta: 0.018675897270441055   |    Training time: 3.86110s\n",
            "The value of modely2 at time step 0 is 1.1418577432632446\n",
            "The theoretical solution is 0.9492071866989136\n",
            "The  relative percentage error is 0.16871677553612496: \n",
            "[tensor([8.2229], device='cuda:0', grad_fn=<MaximumBackward0>)]\n",
            "[tensor([[[1.1419]]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)]\n",
            "The average value of 1 independent runs of modely2 at time step 0 is 8.222860336303711\n",
            "The average value of 1 independent runs of modely2 at time step 0 is 1.1418577432632446\n",
            "The training takes 1686848441.93703 seconds.\n"
          ]
        }
      ],
      "source": [
        "#### TESTTTTTTTTTT ##################\n",
        "t1 = time.time()\n",
        "model1results=[]\n",
        "modely2results=[]\n",
        "modelz2results=[]\n",
        "for i in range(1): #numer of runs\n",
        "  # model1= nn.ModuleList([ann1() for i in range(num_ite)]).to(device)\n",
        "  # modely2= nn.ModuleList([anny2() for i in range(num_time_interval+1)]).to(device)\n",
        "  # modelz2= nn.ModuleList([annz2() for i in range(num_time_interval+1)]).to(device)\n",
        "  print(\"This is {}th run\".format(i+1))\n",
        "  # delta_w1 = torch.randn(size=[num_sample, dim]).to(device) * sqrt_delta_t\n",
        "  for ite in range(num_ite,0,-1):\n",
        "    # model1results=[]\n",
        "    # modely2results=[]\n",
        "    # modelz2results=[]\n",
        "    t_0 = time.time()\n",
        "    x0=torch.ones([1,dim]).to(device)+start_list[ite-1]\n",
        "    x01=torch.ones([100,dim]).to(device)+start_list[ite-1]\n",
        "    print(x0)\n",
        "    print(\"At iteration {}:\".format( ite))\n",
        "    t1=ite*num_steps-num_steps\n",
        "    print(t1)\n",
        "    t2=ite*num_steps-1\n",
        "    print(t2)\n",
        "    optimizer_1=opt1(ite)\n",
        "    # if ite==num_ite:\n",
        "    #   num_epoch1=150\n",
        "    # else:\n",
        "    #   num_epoch1=150\n",
        "    # num_epoch1=0\n",
        "    delta_w = torch.randn(size=[num_sample, dim]).to(device) * sqrt_delta_t\n",
        "    for epoch in range(num_epoch1):\n",
        "\n",
        "      # delta_w = torch.randn(size=[num_sample, dim]).to(device) * sqrt_delta_t\n",
        "      data=sample(ite)\n",
        "      my_index1=torch.arange(data.shape[0])\n",
        "      total_batch1=int((data.shape[0]/batch_size))\n",
        "      t_1=time.time()\n",
        "      for batch in range(total_batch1):\n",
        "        if batch==total_batch1-1:\n",
        "              temp_index=my_index1[batch*batch_size : ]\n",
        "        else:\n",
        "              temp_index=my_index1[batch*batch_size : batch*batch_size + batch_size]\n",
        "\n",
        "        optimizer_1.zero_grad()\n",
        "        # loss1=loss_fn1(ite,data.unsqueeze(2)[temp_index,:].to(device),delta_w[temp_index,:,t2].unsqueeze(2).to(device),model1[ite-1])\n",
        "        loss1=loss_fn1(ite,data.unsqueeze(2)[temp_index,:].to(device),delta_w[temp_index,:].unsqueeze(2).to(device),model1[ite-1])\n",
        "        loss1.backward()\n",
        "        optimizer_1.step()\n",
        "      loss_tmp1.append(loss1.item())\n",
        "      loss_delta1 = loss1.item() - last_loss1\n",
        "      last_loss1 = loss1.item()\n",
        "      if epoch %10 == 0:\n",
        "        print(\"Loss function 1 at epoch {:.0f} is {:.12e}  |  Loss delta: {}   |    Training time: {:.5f}s\".format(index*(num_epoch1)+epoch, loss_delta1,loss1.item(), time.time()-t_1))\n",
        "        # print(\"Total time for training {:.0f} epochs is {:.4e}s\".format(index*(num_epoch1)+epoch+1,time.time()-t_0))\n",
        "\n",
        "    print(\"The value of model1 at time step {} is {}\".format(t2,torch.mean(model1[ite-1](x01)).item()))\n",
        "    print(\"The true optimal quadratic is {}\".format(opt_quad().item()))\n",
        "    # print(\"The  relative percentage error is {}: \".format((torch.abs((torch.mean(model1[ite-1](x01)))-opt_quad())).item()/torch.abs(model1[ite-1](x01)).item()))\n",
        "    # index+=1\n",
        "\n",
        "\n",
        "    optimizer_2=opt2(ite,t1,t2)\n",
        "    for epoch in range(num_epoch2):\n",
        "    # for epoch in range(100):\n",
        "      t_2=time.time()\n",
        "      delta_w = torch.randn(size=[num_sample, dim, num_time_interval]).to(device) * sqrt_delta_t\n",
        "      # x_sample = torch.zeros([num_sample,dim, num_time_interval + 1])\n",
        "      # x_sample[:, :, 0] = torch.zeros([num_sample, dim]) * x_init\n",
        "\n",
        "      x_new,delta_w_new=data_generator2(ite,t1,t2,delta_w)\n",
        "      my_index2=np.arange(x_new.shape[0])\n",
        "      total_batch2=int((x_new.shape[0]/batch_size))\n",
        "      for batch in range(total_batch2):\n",
        "        if batch==total_batch2-1:\n",
        "              temp_index=my_index2[batch*batch_size : ]\n",
        "        else:\n",
        "              temp_index=my_index2[batch*batch_size : batch*batch_size + batch_size]\n",
        "        data2=x_new[temp_index,:]\n",
        "        delta_w2=delta_w_new[temp_index,:]\n",
        "        optimizer_2.zero_grad()\n",
        "        loss2=loss_fn2(ite,data2.to(device),delta_w2.to(device))\n",
        "        loss2.backward()\n",
        "        optimizer_2.step()\n",
        "      loss_tmp2.append(loss2.item())\n",
        "      loss_delta2 = loss2.item() - last_loss2\n",
        "      last_loss2 = loss2.item()\n",
        "      if epoch %10 == 0:\n",
        "        print(\"Loss function 2 at epoch {:.0f} is {:.12e}  |  Loss delta: {}   |    Training time: {:.5f}s\".format(index*(num_epoch2)+epoch, loss_delta2,loss2.item(), time.time()-t_2))\n",
        "        # print(\"Total time for training {:.0f} epochs is {:.4e}s\".format(index*(num_epoch1)+epoch+1,time.time()-t_0))\n",
        "    index+=1\n",
        "\n",
        "    print(\"The value of modely2 at time step {} is {}\".format(t1,modely2[t1](x0).item()))\n",
        "    print(\"The theoretical solution is {}\".format(V1(t_steps[t1],start_list[ite-1]+1).item()))\n",
        "    print(\"The  relative percentage error is {}: \".format(torch.abs(modely2[t1](x0)-V1(t_steps[t1],start_list[ite-1]+1)).item()/torch.abs(modely2[t1](x0)).item()))\n",
        "  model1results.append(model1[ite-1](x0))\n",
        "  modely2results.append(modely2[t1](x0))\n",
        "print(model1results)\n",
        "print(modely2results)\n",
        "print(\"The average value of {} independent runs of modely2 at time step {} is {}\".format(num_runs,t1,torch.mean(torch.tensor(model1results))))\n",
        "print(\"The average value of {} independent runs of modely2 at time step {} is {}\".format(num_runs,t1,torch.mean(torch.tensor(modely2results))))\n",
        "print(\"The training takes {:.5f} seconds.\".format(time.time()-t1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VerW2p_xqxXU",
        "outputId": "356c9620-6054-411c-f949-cd9362684df6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The value of model1 at time step 4 is 8.222860336303711\n",
            "The true optimal quadratic is 1.9595918655395508\n"
          ]
        }
      ],
      "source": [
        "  print(\"The value of model1 at time step {} is {}\".format(t2,torch.mean(model1[ite-1](x01)).item()))\n",
        "  print(\"The true optimal quadratic is {}\".format(opt_quad().item()))\n",
        "  # print(\"The  relative percentage error is {}: \".format((torch.abs((torch.mean(model1[ite-1](x01)-opt_quad())).item())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sUwNGW8yK9ku",
        "outputId": "d55d1472-5315-44d6-92ac-8b67394deaa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.9596], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(opt_quad())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMWPUvIzpXOZ"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I82biHCdU8R2",
        "outputId": "57cc33ec-ad29-413a-b6bc-7c8cb7880d35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]], device='cuda:0')\n",
            "torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "x0=torch.ones([1,dim]).to(device)+4   #our testing region is [4,6] so x0=5\n",
        "print(x0)\n",
        "print(x0.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ugaABDZrzlvX",
        "outputId": "735845e1-57f4-4f2a-e897-d1d6444f4a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([50, 10])\n",
            "tensor([5.5818, 5.5383, 5.9696, 5.0786, 5.4429, 5.3071, 5.8215, 5.2751, 5.0937,\n",
            "        5.7772, 5.7208, 5.3802, 5.0397, 5.2278, 5.0029, 5.3253, 5.2268, 5.0811,\n",
            "        5.5271, 5.7614, 5.6169, 5.8119, 5.7462, 5.0296, 5.0745, 5.9486, 5.3999,\n",
            "        5.3581, 5.9133, 5.2910, 5.7718, 5.7537, 5.6673, 5.1418, 5.3531, 5.6126,\n",
            "        5.2714, 5.7668, 5.6234, 5.3635, 5.6950, 5.9439, 5.4636, 5.5131, 5.7722,\n",
            "        5.5309, 5.8349, 5.5721, 5.7536, 5.2846])\n",
            "tensor([[[1.1444]],\n",
            "\n",
            "        [[1.1497]],\n",
            "\n",
            "        [[1.1393]],\n",
            "\n",
            "        [[1.1480]],\n",
            "\n",
            "        [[1.1540]],\n",
            "\n",
            "        [[1.1533]],\n",
            "\n",
            "        [[1.1487]],\n",
            "\n",
            "        [[1.1402]],\n",
            "\n",
            "        [[1.1436]],\n",
            "\n",
            "        [[1.1588]],\n",
            "\n",
            "        [[1.1471]],\n",
            "\n",
            "        [[1.1460]],\n",
            "\n",
            "        [[1.1366]],\n",
            "\n",
            "        [[1.1444]],\n",
            "\n",
            "        [[1.1393]],\n",
            "\n",
            "        [[1.1376]],\n",
            "\n",
            "        [[1.1395]],\n",
            "\n",
            "        [[1.1455]],\n",
            "\n",
            "        [[1.1553]],\n",
            "\n",
            "        [[1.1438]],\n",
            "\n",
            "        [[1.1423]],\n",
            "\n",
            "        [[1.1391]],\n",
            "\n",
            "        [[1.1578]],\n",
            "\n",
            "        [[1.1635]],\n",
            "\n",
            "        [[1.1470]],\n",
            "\n",
            "        [[1.1363]],\n",
            "\n",
            "        [[1.1492]],\n",
            "\n",
            "        [[1.1407]],\n",
            "\n",
            "        [[1.1540]],\n",
            "\n",
            "        [[1.1596]],\n",
            "\n",
            "        [[1.1496]],\n",
            "\n",
            "        [[1.1411]],\n",
            "\n",
            "        [[1.1487]],\n",
            "\n",
            "        [[1.1467]],\n",
            "\n",
            "        [[1.1496]],\n",
            "\n",
            "        [[1.1553]],\n",
            "\n",
            "        [[1.1441]],\n",
            "\n",
            "        [[1.1429]],\n",
            "\n",
            "        [[1.1558]],\n",
            "\n",
            "        [[1.1326]],\n",
            "\n",
            "        [[1.1602]],\n",
            "\n",
            "        [[1.1562]],\n",
            "\n",
            "        [[1.1397]],\n",
            "\n",
            "        [[1.1345]],\n",
            "\n",
            "        [[1.1377]],\n",
            "\n",
            "        [[1.1489]],\n",
            "\n",
            "        [[1.1577]],\n",
            "\n",
            "        [[1.1423]],\n",
            "\n",
            "        [[1.1394]],\n",
            "\n",
            "        [[1.1398]]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
            "tensor([0.9620, 0.9612, 0.9687, 0.9512, 0.9593, 0.9564, 0.9663, 0.9557, 0.9515,\n",
            "        0.9656, 0.9646, 0.9580, 0.9502, 0.9547, 0.9493, 0.9568, 0.9547, 0.9512,\n",
            "        0.9610, 0.9653, 0.9627, 0.9662, 0.9650, 0.9500, 0.9511, 0.9684, 0.9584,\n",
            "        0.9575, 0.9678, 0.9561, 0.9655, 0.9652, 0.9636, 0.9527, 0.9574, 0.9626,\n",
            "        0.9557, 0.9654, 0.9628, 0.9576, 0.9641, 0.9683, 0.9597, 0.9607, 0.9655,\n",
            "        0.9610, 0.9665, 0.9618, 0.9652, 0.9559], device='cuda:0')\n",
            "tensor(0.0349, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "bbb = torch.stack([torch.rand(50)+5 for i in range(dim)],dim=1)  #50 randomly generated points for testing\n",
        "print(bbb.shape)\n",
        "print(bbb[:,0])\n",
        "ccc=modely2[0](bbb.to(device))  #evaluate these 50 random points using modely2 at time step 0\n",
        "print(ccc)\n",
        "ddd=V1(0,bbb[:,0].to(device))   #evaluate these 50 random point using true value function\n",
        "print(ddd)\n",
        "eee=torch.mean(torch.square(ccc.flatten()-ddd))  #take square difference between true and approximated values\n",
        "print(eee)\n",
        "# print(V1(t_steps[t1],5.3743))\n",
        "# print(modely2[0](x0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}