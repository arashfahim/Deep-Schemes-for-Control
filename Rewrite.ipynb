{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, CyclicLR, ReduceLROnPlateau, LinearLR, ExponentialLR\n",
    "import random\n",
    "from torch.autograd.functional import jacobian, hessian\n",
    "# import AUTOGRAD.FUNCTIONAL.JACOBIAN as jacobian\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=2\n",
    "kappa=torch.tensor([1]).to(device)\n",
    "theta=torch.tensor([0.4]).to(device)\n",
    "nu=torch.tensor([0.02]).to(device)\n",
    "lamb=torch.tensor([0.15]).to(device)\n",
    "eta=torch.tensor([0.5]).to(device)\n",
    "rho=torch.tensor([0.0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 2])\n",
      "torch.Size([8, 1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9f/dlwft6dn7hxg65r7xqyxvk6r0000gn/T/ipykernel_20146/224781277.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  delta_t = torch.tensor(T/ num_time_interval)\n"
     ]
    }
   ],
   "source": [
    "num_sample=2**3   #M\n",
    "#dim=5\n",
    "num_time_interval=2    #N\n",
    "T=torch.tensor([1]).to(device)\n",
    "delta_t = torch.tensor(T/ num_time_interval)\n",
    "# t_steps = torch.linspace(0,1,num_time_interval).to(device)\n",
    "# sqrt_delta_t = torch.sqrt(delta_t).to(device)\n",
    "sqrt_delta_t = torch.sqrt(delta_t).to(device)\n",
    "x_init = torch.zeros(dim)\n",
    "# sigma=torch.sqrt(torch.tensor([2.0]))\n",
    "# lamb=torch.tensor([1.0]).to(device)\n",
    "mu=torch.tensor([1.0]).to(device)\n",
    "# sigma=torch.tensor([1.0]).to(device)\n",
    "alpha=torch.tensor([1.0]).to(device)\n",
    "start = torch.tensor([0]).to(device)\n",
    "end = torch.tensor([1]).to(device)\n",
    "num_steps=5\n",
    "num_ite=int(num_time_interval/num_steps)\n",
    "# kappa=torch.tensor([1]).to(device)\n",
    "# theta=torch.tensor([0.4]).to(device)\n",
    "# nu=torch.tensor([0.5]).to(device)\n",
    "# lamb=torch.tensor([0.6]).to(device)\n",
    "# eta=torch.tensor([0.5]).to(device)\n",
    "p=torch.tensor([0.95]).to(device)\n",
    "# rho=torch.tensor([0]).to(device)\n",
    "num_runs=1\n",
    "# x0=torch.ones([1,dim]).to(device)+4\n",
    "# print(x0)\n",
    "\n",
    "t_steps = torch.linspace(0,T.item(),num_time_interval+1).to(device)\n",
    "# print(t_steps)\n",
    "test_interval=[0.5,1.5]\n",
    "# v=(torch.ones(num_sample,dim,dim)-0.3).to(device)\n",
    "# v=(torch.ones(num_sample,dim,dim)).to(device)\n",
    "v=torch.diag(torch.ones(dim)).to(device)\n",
    "# print(v)\n",
    "v=v.unsqueeze(0).repeat(num_sample,1,1)\n",
    "print(v.shape)\n",
    "# print(v)\n",
    "\n",
    "\n",
    "# dt=T/num_time_interval\n",
    "Dt=torch.zeros((num_sample,1,num_time_interval+1)).to(device)\n",
    "print(Dt.shape)\n",
    "Dt[:,:,1:]=delta_t\n",
    "t=torch.cumsum(Dt,axis=2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearity - Full - $h(t,x,p,q,\\gamma)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(x,z,gamma): #### out_shape =([M]) |  input:  x_shape=[M,dim,1],  z shape = [M,dim,1], gamma shape= [M,dim,dim]\n",
    "    result=torch.zeros([x.shape[0]]).to(device)\n",
    "    for i in range(dim-1):\n",
    "      result+=kappa[i]*(theta[i]-x[:,i+1,0])*z[:,i+1,0]+0.5*torch.pow(nu[i],2)*gamma[:,i+1,i+1]-rho[i]*lamb[i]*nu[i]*(z[:,0,0]*gamma[:,0,i+1])/gamma[:,0,0] \\\n",
    "      + 0.5*torch.pow(rho[i]*nu[i]*gamma[:,0,i+1],2)/gamma[:,0,0]\n",
    "      # print(result)\n",
    "    result1=result-0.5*torch.sum(torch.square(lamb))*torch.pow(z[:,0,0],2)/gamma[:,0,0]\n",
    "    return result1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinarity - semi $(t,x,p,q,\\sigma)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def F(x,z,a): #### out_shape =([M]) |  input:  x_shape=[M,D,1],  z shape = [M,D,1],a_shape= [M,D,D]  #This is for rho=0\n",
    "  result=torch.zeros([x.shape[0]]).to(device)\n",
    "  for i in range(dim-1):\n",
    "    result+=-kappa[i]*(theta[i]-x[:,i+1,0])*z[:,i+1,0]\n",
    "    # print(result)\n",
    "  result1=result-torch.sqrt(torch.sum(torch.square(lamb)))*torch.abs(z[:,0,0])*torch.sqrt(a[:,0,0])\n",
    "  return result1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def g1(x): #out_shape= [M,1,1]   | input:  x_shape=[M,D]\n",
    "  result=1-torch.exp(-eta*x[:,0])\n",
    "  # result=torch.reshape(result,(result.shape[0],1,1))\n",
    "  result=torch.reshape(result,(result.shape[0],1))\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic of SDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def update(data,delta_w,sigma):#output: data=(M,D,1) ,  #input data=(M,D,1), delta_w=[M,D,1], sigma=[M,D,1]\n",
    "    dx = torch.bmm(sigma,delta_w)\n",
    "    data = data + dx\n",
    "    data=data\n",
    "    return data\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def der1(x,f):  #output= [M,D,1], input: x=[M,D,1]\n",
    "  Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "  xin=x.clone().detach()\n",
    "  xin.requires_grad=True\n",
    "  # print('xin shape', xin.shape)\n",
    "  u=f(xin)\n",
    "  # print(u)\n",
    "  # print('u/f(xin) shape', u.shape)\n",
    "  Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                          allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "  # print('Du shape before reshaped', Du.shape)\n",
    "  Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "  # print('Du shape after reshaped', Du.shape)\n",
    "  return Du\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First and second derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def grad_hessian1(t, x,f_): #output= [M,D,D], #input: x=[M,D], t=[M,1], xt= [M,D+1]\n",
    "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "\n",
    "    xin=x.clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    # print(\"xin_shape\", xin.shape)\n",
    "    tin=t.clone().detach()\n",
    "    # print(\"tin shape\", tin.shape)\n",
    "    xt_in=torch.cat((xin,tin),1)\n",
    "    # print(\"xt_in shape\", xt_in.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # xt_in.requires_grad=True\n",
    "    u=f_(xt_in)\n",
    "    # print(\"u shape: \",u.shape)\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "    # print(\"Du shape:\",Du.shape)\n",
    "    # print(\"-----\")\n",
    "    # print(torch.autograd.grad(outputs=[Du[:,0,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,0,:]),\n",
    "    #                        allow_unused=True,retain_graph=True,create_graph=True)[0].shape)\n",
    "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
    "    # print(\"D2u shape:\",hess_temp.shape)\n",
    "    Du = torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print(\"Du after reshape:\",Du.shape)\n",
    "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
    "    # print(\"D2u after reshape:\",hess_temp.shape)\n",
    "\n",
    "    # print(hess_temp)\n",
    "    return Du, hess_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def grad_model(t, x,f_): #output= [M,D,1], #input: x=[M,D], t=[M,1], xt= [M,D+1]\n",
    "    # hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "\n",
    "    xin=x.clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    # print(\"xin_shape\", xin.shape)\n",
    "    tin=t.clone().detach()\n",
    "    # print(\"tin shape\", tin.shape)\n",
    "    xt_in=torch.cat((xin,tin),1)\n",
    "    # print(\"xt_in shape\", xt_in.shape)\n",
    "\n",
    "    # xt_in.requires_grad=True\n",
    "    u=f_(xt_in)\n",
    "    # print(\"u shape: \",u.shape)\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "\n",
    "    Du = torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print(\"Du after reshape:\",Du.shape)\n",
    "\n",
    "    return Du\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hessian_model(t,x,model):\n",
    "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "\n",
    "    xin=x.clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    # print(\"xin_shape\", xin.shape)\n",
    "    tin=t.clone().detach()\n",
    "    # print(\"tin shape\", tin.shape)\n",
    "    xt_in=torch.cat((xin,tin),1)\n",
    "    # print(\"xt_in shape\", xt_in.shape)\n",
    "    Du = model(xt_in)\n",
    "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i]),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
    "\n",
    "    # print(\"D2u shape:\",hess_temp.shape)\n",
    "\n",
    "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
    "\n",
    "    # print(hess_temp)\n",
    "    return hess_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Hessian!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def hessian1(x,model2z):  #x=[M,D,1]  output= [M,D,1]\n",
    "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    # print(\"D2u shape:\",hess_temp.shape)\n",
    "\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "    xin=x.clone().detach()\n",
    "    # print('this is xin')\n",
    "    # print(xin)\n",
    "    xin.requires_grad=True\n",
    "    Du = model2z(xin.squeeze(2))\n",
    "    # print('this is Du')\n",
    "    # print(Du)\n",
    "    # print('this is Du shape')\n",
    "    # print(Du.shape)\n",
    "    # print('this is Du 1')\n",
    "    # print(Du[:,1,:])\n",
    "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
    "\n",
    "    # print(\"D2u shape:\",hess_temp.shape)\n",
    "\n",
    "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
    "\n",
    "    # print(hess_temp)\n",
    "    return hess_temp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grad_fn2(x,z,a,f,xx=False,r=False,p=False,alpha=False):  #output= [M,D,1], input: x=[M,D,1] z=[M,D,1], a=[M,D,D]\n",
    "  Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "  xin=x.clone().detach()\n",
    "  xin.requires_grad=True\n",
    "  # print('xin shape', xin.shape)\n",
    "  zin=z.clone().detach()\n",
    "  zin.requires_grad=True\n",
    "  # print('zin shape', zin.shape)\n",
    "  ain=a.clone().detach()\n",
    "  ain.requires_grad=True\n",
    "  # print('ain shape', ain.shape)\n",
    "  # ain=torch.reshape(ain,(num_sample,dim*dim,1))\n",
    "  u=f(xin,zin,ain)\n",
    "  # print(u)\n",
    "  # print('u/f(xin) shape', u.shape)\n",
    "  if xx==True:\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                            allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
    "    # print('Du shape before reshaped', Du.shape)\n",
    "    Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print('Du shape after reshaped', Du.shape)\n",
    "  if r==True:\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[rin],grad_outputs=torch.ones_like(u),\n",
    "                            allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
    "    # print('Du shape before reshaped', Du.shape)\n",
    "    Du=torch.reshape(Du,(Du.shape[0],1,1))\n",
    "    # Du=torch.reshape(Du,(Du.shape[0],dim,dim))\n",
    "    # print('Du shape after reshaped', Du.shape)\n",
    "  if p==True:\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[zin],grad_outputs=torch.ones_like(u),\n",
    "                            allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
    "    # print('Du shape before reshaped', Du.shape)\n",
    "    Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print('Du shape after reshaped', Du.shape)\n",
    "  if alpha==True:\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[ain],grad_outputs=torch.ones_like(u),\n",
    "                            allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
    "    # print('Du shape before reshaped', Du.shape)\n",
    "    Du=torch.reshape(Du,(Du.shape[0],dim,dim))\n",
    "    # print('Du shape after reshaped', Du.shape)\n",
    "\n",
    "  return Du\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def L_matrix(ite,x,model):  #x=[M,D]\n",
    "  L=torch.zeros(x.shape[0],dim,dim)\n",
    "\n",
    "  L[:,0,0]=model[ite](x)\n",
    "  for i in range(dim-1):\n",
    "    L[:,i+1,i+1]=nu[i]\n",
    "  return L\n",
    "\n",
    "\n",
    "def opt_quad():\n",
    "  return torch.pow(torch.sum(torch.pow(lamb,2)),0.5)/eta\n",
    "\n",
    "# def L_matrix_test(ite,x,model1):  #This matrix is a matrix with L00 = 0.6 and L11=nu\n",
    "#   L=torch.zeros(x.shape[0],dim,dim)\n",
    "#   L[:,0,0]=opt_quad()\n",
    "#   for i in range(dim-1):\n",
    "#     L[:,i+1,i+1]=nu[i]\n",
    "#   return L\n",
    "\n",
    "def L_matrix_test(x):  #This matrix is a matrix with L00 = 0.6 and L11=nu\n",
    "  L=torch.zeros(x.shape[0],dim,dim)\n",
    "  L[:,0,0]=opt_quad()\n",
    "  for i in range(dim-1):\n",
    "    L[:,i+1,i+1]=nu[i]\n",
    "  return L\n",
    "\n",
    "def V1(t,x,v):\n",
    "  # result=(1-torch.exp(-eta*x))*(torch.exp(-0.5*(T-t)*torch.sum(torch.pow(lamb,2))))\n",
    "  result=-torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
    "  return result\n",
    "\n",
    "def V(t,x,v): #output           |#input:\n",
    "  result=1-torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
    "  return result\n",
    "\n",
    "\n",
    "def V1(t,x,v):\n",
    "  # result=(1-torch.exp(-eta*x))*(torch.exp(-0.5*(T-t)*torch.sum(torch.pow(lamb,2))))\n",
    "  result=-torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
    "  return result\n",
    "\n",
    "def derV(t,x,v): #output           |#input:\n",
    "  result=eta*torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
    "  return result\n",
    "\n",
    "def hessianV(t,x,v): #output           |#input:\n",
    "  result=-torch.pow(eta,2)*torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving linear equations\n",
    "\n",
    "$\\begin{cases}-\\partial v_t - \\dfrac12 (\\sigma^\\intercal\\sigma): D^2v -\\mu\\cdot\\nabla v + f(t,x) =0\\\\\n",
    "v(T,x)=g(x)\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(t,x)$ has shape [num_samples,dim,dim]\n",
    "$dW_t$  has shape [num_samples,dim]\n",
    "\n",
    "We need to unsqueeze $dW_t$ before batch multiplication.\n",
    "\n",
    "    \n",
    "    torch.bmm(sigma(x[:,:,i]),dw[:, :, i]dw[:, :, i].unsqueeze(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigma(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(sigma, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(dim+1, 4),\n",
    "            nn.BatchNorm1d(num_features=4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4,dim*dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim])\n",
    "\n",
    "class mu(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(mu, self).__init__()\n",
    "        # self.linear_stack = nn.Sequential(\n",
    "        #     nn.Linear(dim+1, 4),\n",
    "        #     nn.BatchNorm1d(num_features=4),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(4,dim),\n",
    "        # )\n",
    "    def forward(self, x):\n",
    "        logits = torch.zeros([x.shape[0],dim])\n",
    "        return logits\n",
    "    \n",
    "class kappa(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(kappa, self).__init__()\n",
    "        # self.linear_stack = nn.Sequential(\n",
    "        #     nn.Linear(dim+1, 4),\n",
    "        #     nn.BatchNorm1d(num_features=4),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(4,dim),\n",
    "        # )\n",
    "    def forward(self, x):\n",
    "        logits = torch.zeros([x.shape[0],1])\n",
    "        return logits\n",
    "    \n",
    "class source(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(source, self).__init__()\n",
    "        # self.linear_stack = nn.Sequential(\n",
    "        #     nn.Linear(dim+1, 4),\n",
    "        #     nn.BatchNorm1d(num_features=4),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(4,dim),\n",
    "        # )\n",
    "    def forward(self, x):\n",
    "        logits = torch.zeros([x.shape[0],1])\n",
    "        return logits   \n",
    "    \n",
    "class terminal(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(terminal, self).__init__()\n",
    "        # self.linear_stack = nn.Sequential(\n",
    "        #     nn.Linear(dim+1, 4),\n",
    "        #     nn.BatchNorm1d(num_features=4),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(4,dim),\n",
    "        # )\n",
    "    def forward(self, x):\n",
    "        logits = -torch.tensor([0.5])*torch.pow(x[:,1:],2)\n",
    "        return logits \n",
    "    \n",
    "class Y(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(Y, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(dim+1, 4),\n",
    "            nn.BatchNorm1d(num_features=4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4,1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim])  \n",
    "    \n",
    "class Z(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(Z, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(dim+1, 4),\n",
    "            nn.BatchNorm1d(num_features=4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4,dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand((8,dim+1))\n",
    "s = sigma()\n",
    "m = mu()\n",
    "k = kappa()\n",
    "f = source()\n",
    "g = terminal()\n",
    "s(x).reshape((8,dim,dim)).shape, m(x).shape, k(x).shape, g(x).shape\n",
    "\n",
    "z = Z()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class equation(object):\n",
    "    def __init__(self,sigma,mu,source,kappa,terminal,T,n,a,b,dim):\n",
    "        num_samples = 2**3\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.kappa = kappa\n",
    "        self.source = source\n",
    "        self.x = torch.zeros((num_samples,dim+1,n+1))\n",
    "        self.Y = Y()\n",
    "        self.Z = Z()\n",
    "        dt = torch.tensor([T/n]).to(device)\n",
    "        dw = torch.randn(size=[num_samples, dim, n]).to(device)* torch.sqrt(dt)\n",
    "        self.x[:,1:,0]= a+(b-a)*torch.rand(num_samples,dim)\n",
    "        self.r = torch.ones((num_samples,1,n+1))\n",
    "        self.c = torch.zeros((num_samples,1,n+1))\n",
    "        for i in range(n):\n",
    "            s_tmp = self.sigma(self.x[:,:,i]).reshape((num_samples,dim,dim))\n",
    "            dw_tmp = dw[:, :, i].unsqueeze(2)\n",
    "            self.x[:,1:,i+1] = self.x[:,1:,i] + self.mu(self.x[:,:,i])*dt + torch.bmm(s_tmp,dw_tmp).squeeze(2)\n",
    "            self.x[:,0,i+1] = self.x[:,0,i]+dt\n",
    "            self.r[:,:,i+1] = self.r[:,:,i]* torch.exp(-self.kappa(self.x[:,:,i])*dt)\n",
    "            self.c[:,:,i+1] = self.c[:,:,i] + self.source(self.x[:,:,i])*dt\n",
    "        kc = (self.r*self.c).sum(-1) + g(self.x[:,1:,-1])\n",
    "        print(kc.mean())\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5830, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([0.0])\n",
    "b = torch.tensor([1.0])\n",
    "heat = equation(s,m,f,k,g,T,num_time_interval,a,b,dim)\n",
    "heat.Z(heat.x[:,:,1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = torch.tensor([T/num_time_interval]).to(device)\n",
    "dw = torch.randn(size=[num_sample, dim, num_time_interval]).to(device)* torch.sqrt(dt)\n",
    "x = torch.zeros((num_sample,dim+1,num_time_interval+1)).to(device)\n",
    "x[:,1:,0]= a+(b-a)*torch.rand(num_sample,dim)\n",
    "for i in range(num_time_interval):\n",
    "    s_tmp = s(x[:,:,i]).reshape((num_sample,dim,dim))\n",
    "    dw_tmp = dw[:, :, i].unsqueeze(2)\n",
    "    # print(s_tmp.shape,dw_tmp.shape)\n",
    "    # print(torch.bmm(s_tmp,dw_tmp).shape)\n",
    "    # print(m(x[:,:,i]).shape)\n",
    "    # print(x[:,:,i].shape)\n",
    "    x[:,1:,i+1] = x[:,1:,i] + m(x[:,:,i])*dt + torch.bmm(s_tmp,dw_tmp).squeeze(2)\n",
    "    x[:,0,i+1] = x[:,0,i]+dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.5000,  1.0000],\n",
       "         [ 0.6045,  0.6313,  0.3523],\n",
       "         [ 0.3737,  0.3689,  0.1290]],\n",
       "\n",
       "        [[ 0.0000,  0.5000,  1.0000],\n",
       "         [ 0.0943,  0.4706,  0.8799],\n",
       "         [ 0.0363,  0.7895,  1.5572]],\n",
       "\n",
       "        [[ 0.0000,  0.5000,  1.0000],\n",
       "         [ 0.9576,  0.9594,  0.9705],\n",
       "         [ 0.3591,  0.3474,  0.2021]],\n",
       "\n",
       "        [[ 0.0000,  0.5000,  1.0000],\n",
       "         [ 0.4682,  0.1024, -1.0904],\n",
       "         [ 0.3310, -0.1241, -1.9073]],\n",
       "\n",
       "        [[ 0.0000,  0.5000,  1.0000],\n",
       "         [ 0.5298,  0.7732,  0.5755],\n",
       "         [ 0.5534,  0.8607,  0.6673]],\n",
       "\n",
       "        [[ 0.0000,  0.5000,  1.0000],\n",
       "         [ 0.9525,  0.7066,  0.4719],\n",
       "         [ 0.7826,  1.2406,  0.8090]],\n",
       "\n",
       "        [[ 0.0000,  0.5000,  1.0000],\n",
       "         [ 0.6376,  1.2435,  1.4026],\n",
       "         [ 0.2813,  0.6142,  1.0592]],\n",
       "\n",
       "        [[ 0.0000,  0.5000,  1.0000],\n",
       "         [ 0.1331,  0.5952,  0.4466],\n",
       "         [ 0.6384,  2.4695,  1.8225]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
