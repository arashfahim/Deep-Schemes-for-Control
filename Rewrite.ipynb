{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, CyclicLR, ReduceLROnPlateau, LinearLR, ExponentialLR\n",
    "import random\n",
    "from torch.autograd.functional import jacobian, hessian\n",
    "# import AUTOGRAD.FUNCTIONAL.JACOBIAN as jacobian\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.7\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters of the PDEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=9\n",
    "params={'dim':dim,\n",
    "          'kappa':torch.tensor([0.,1.,0.8,0.6,0.4,0.5,0.3,0.2,0.1,0.7]).to(device), # The first kappa=0 because the drift of wealth process is zero\n",
    "          'theta':torch.tensor([0.,0.1,0.2,0.3,0.4,0.5,0.4,0.3,0.2,0.1]).to(device),\n",
    "          'nu':torch.tensor([0.02,0.015,0.11,0.12,0.01,0.013,0.14,0.14,0.01]).to(device),\n",
    "          'lb':torch.tensor([0.,0.15,0.11,0.12,0.13,0.15,0.11,0.12,0.13,0.15]).to(device),   \n",
    "          'rho':torch.tensor([0.,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]).to(device),\n",
    "          'eta':torch.tensor([1]).to(device),\n",
    "          'T': torch.tensor([1]).to(device)\n",
    "        }\n",
    "params0={'dim':dim,\n",
    "          'kappa':torch.tensor([0.,0.,0.,0.,0.,0.,0.,0.,0.,0.]).to(device), # The first kappa=0 because the drift of wealth process is zero\n",
    "          'theta':torch.tensor([0.,0.1,0.2,0.3,0.4,0.5,0.4,0.3,0.2,0.1]).to(device),\n",
    "          'nu':torch.tensor([0.02,0.015,0.11,0.12,0.01,0.013,0.14,0.14,0.01]).to(device),\n",
    "          'lb':torch.tensor([0.,0.15,0.11,0.12,0.13,0.15,0.11,0.12,0.13,0.15]).to(device),   \n",
    "          'rho':torch.tensor([0.,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]).to(device),\n",
    "          'eta':torch.tensor([1.0]).to(device),\n",
    "          'T': torch.tensor([1.]).to(device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd for gradient and Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def der1(x,f):  #output= [M,D,1], input: x=[M,D,1]\n",
    "  Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "  xin=x.clone().detach()\n",
    "  xin.requires_grad=True\n",
    "  # print('xin shape', xin.shape)\n",
    "  u=f(xin)\n",
    "  # print(u)\n",
    "  # print('u/f(xin) shape', u.shape)\n",
    "  Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                          allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "  # print('Du shape before reshaped', Du.shape)\n",
    "  Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "  # print('Du shape after reshaped', Du.shape)\n",
    "  return Du\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def grad_model(t, x,f_): #output= [M,D,1], #input: x=[M,D], t=[M,1], xt= [M,D+1]\n",
    "    # hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "\n",
    "    xin=x.clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    # print(\"xin_shape\", xin.shape)\n",
    "    tin=t.clone().detach()\n",
    "    # print(\"tin shape\", tin.shape)\n",
    "    xt_in=torch.cat((xin,tin),1)\n",
    "    # print(\"xt_in shape\", xt_in.shape)\n",
    "\n",
    "    # xt_in.requires_grad=True\n",
    "    u=f_(xt_in)\n",
    "    # print(\"u shape: \",u.shape)\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "\n",
    "    Du = torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print(\"Du after reshape:\",Du.shape)\n",
    "\n",
    "    return Du\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def grad_model(t, x,f_): #output= [M,D,1], #input: x=[M,D], t=[M,1], xt= [M,D+1]\n",
    "    # hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "\n",
    "    xin=x.clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    # print(\"xin_shape\", xin.shape)\n",
    "    tin=t.clone().detach()\n",
    "    # print(\"tin shape\", tin.shape)\n",
    "    xt_in=torch.cat((xin,tin),1)\n",
    "    # print(\"xt_in shape\", xt_in.shape)\n",
    "\n",
    "    # xt_in.requires_grad=True\n",
    "    u=f_(xt_in)\n",
    "    # print(\"u shape: \",u.shape)\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "\n",
    "    Du = torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print(\"Du after reshape:\",Du.shape)\n",
    "\n",
    "    return Du\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def der1(x,f):  #output= [M,D,1], input: x=[M,D,1]\n",
    "  Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "  xin=x.clone().detach()\n",
    "  xin.requires_grad=True\n",
    "  # print('xin shape', xin.shape)\n",
    "  u=f(xin)\n",
    "  # print(u)\n",
    "  # print('u/f(xin) shape', u.shape)\n",
    "  Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                          allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "  # print('Du shape before reshaped', Du.shape)\n",
    "  Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "  # print('Du shape after reshaped', Du.shape)\n",
    "  return Du\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grad_fn2(x,z,a,f,xx=False,r=False,p=False,alpha=False):  #output= [M,D,1], input: x=[M,D,1] z=[M,D,1], a=[M,D,D]\n",
    "  Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "  xin=x.clone().detach()\n",
    "  xin.requires_grad=True\n",
    "  # print('xin shape', xin.shape)\n",
    "  zin=z.clone().detach()\n",
    "  zin.requires_grad=True\n",
    "  # print('zin shape', zin.shape)\n",
    "  ain=a.clone().detach()\n",
    "  ain.requires_grad=True\n",
    "  # print('ain shape', ain.shape)\n",
    "  # ain=torch.reshape(ain,(num_sample,dim*dim,1))\n",
    "  u=f(xin,zin,ain)\n",
    "  # print(u)\n",
    "  # print('u/f(xin) shape', u.shape)\n",
    "  if xx==True:\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                            allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
    "    # print('Du shape before reshaped', Du.shape)\n",
    "    Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print('Du shape after reshaped', Du.shape)\n",
    "  if r==True:\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[rin],grad_outputs=torch.ones_like(u),\n",
    "                            allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
    "    # print('Du shape before reshaped', Du.shape)\n",
    "    Du=torch.reshape(Du,(Du.shape[0],1,1))\n",
    "    # Du=torch.reshape(Du,(Du.shape[0],dim,dim))\n",
    "    # print('Du shape after reshaped', Du.shape)\n",
    "  if p==True:\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[zin],grad_outputs=torch.ones_like(u),\n",
    "                            allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
    "    # print('Du shape before reshaped', Du.shape)\n",
    "    Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print('Du shape after reshaped', Du.shape)\n",
    "  if alpha==True:\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[ain],grad_outputs=torch.ones_like(u),\n",
    "                            allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
    "    # print('Du shape before reshaped', Du.shape)\n",
    "    Du=torch.reshape(Du,(Du.shape[0],dim,dim))\n",
    "    # print('Du shape after reshaped', Du.shape)\n",
    "\n",
    "  return Du\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import jacobian    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class sample_Znet(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(sample_Znet, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(dim+1, 10),\n",
    "            # nn.BatchNorm1d(num_features=20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10,dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim])       \n",
    "dv = sample_Znet()\n",
    "def test_fnc(x):\n",
    "    return x[:,1:]*x[:,1:] + torch.sin(torch.tensor([2])*torch.pi*x[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_fnc(\u001b[38;5;28minput\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m, in \u001b[0;36mtest_fnc\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_fnc\u001b[39m(x):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x[:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m*\u001b[39mx[:,\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m*\u001b[39mx[:,\u001b[38;5;241m1\u001b[39m:])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "test_fnc(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = x_tmp\n",
    "print(input.shape,dim)\n",
    "test_fnc(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sin(2*torch.pi*input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = jacobian(test_fnc, input)\n",
    "print(input.shape, test_fnc(input).shape,output.shape)\n",
    "for i in range(input.shape[0]):\n",
    "    if i == 0:\n",
    "        jac = output[0,:,0,1:].unsqueeze(0)# output[0,:,0,1:] eliminates the derivative wrt the first compinent, i.e., time derivative\n",
    "        # print(output[i,:,i,:])\n",
    "    else:\n",
    "        # print(output[i,:,i,:].unsqueeze(0))\n",
    "        jac = torch.cat((jac,output[i,:,i,1:].unsqueeze(0)))\n",
    "print(\"Jacobians Tensor:\\n\", jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0,:,0,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jacobian of a function from $\\mathbb{R}^d$ to $\\mathbb{R}^d$, excludes time derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqr_jacobian(func,x):#Jacobian of a function from R^d to R^d\n",
    "    xin=x.clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    out = jacobian(func, xin)\n",
    "    for i in range(input.shape[0]):\n",
    "        if i == 0:\n",
    "            jac = out[i,:,i,1:].unsqueeze(0)\n",
    "            # print(output[i,:,i,:])\n",
    "        else:\n",
    "            # print(output[i,:,i,:].unsqueeze(0))\n",
    "            jac = torch.cat((jac,out[i,:,i,1:].unsqueeze(0)))\n",
    "    return jac\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqr_jacobian(test_fnc, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_model(t,x,model):\n",
    "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "\n",
    "    xin=x.clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    # print(\"xin_shape\", xin.shape)\n",
    "    tin=t.clone().detach()\n",
    "    # print(\"tin shape\", tin.shape)\n",
    "    xt_in=torch.cat((xin,tin),1)\n",
    "    # print(\"xt_in shape\", xt_in.shape)\n",
    "    Du = model(xt_in)\n",
    "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i]),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
    "\n",
    "    # print(\"D2u shape:\",hess_temp.shape)\n",
    "\n",
    "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
    "\n",
    "    # print(hess_temp)\n",
    "    return hess_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hessian1(x,model2z):  #x=[M,D,1]  output= [M,D,1]\n",
    "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    # print(\"D2u shape:\",hess_temp.shape)\n",
    "\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "    xin=x.clone().detach()\n",
    "    # print('this is xin')\n",
    "    # print(xin)\n",
    "    xin.requires_grad=True\n",
    "    Du = model2z(xin.squeeze(2))\n",
    "    # print('this is Du')\n",
    "    # print(Du)\n",
    "    # print('this is Du shape')\n",
    "    # print(Du.shape)\n",
    "    # print('this is Du 1')\n",
    "    # print(Du[:,1,:])\n",
    "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
    "\n",
    "    # print(\"D2u shape:\",hess_temp.shape)\n",
    "\n",
    "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
    "\n",
    "    # print(hess_temp)\n",
    "    return hess_temp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First and second derivative together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Hess(x,v): #output= [M,D,D], #input: x=[M,D], t=[M,1], xt= [M,D+1]\n",
    "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "    xin=x.clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    u=v(xin)\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
    "    Du = torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
    "    return Du, hess_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tmp = torch.tensor([[0.6253, 0.2105, 0.6508, 0.8284, 0.4725, 0.6971, 0.4351, 0.8187, 0.4427],\n",
    "        [0.8233, 0.8866, 0.3864, 0.8186, 0.0874, 0.6627, 0.9699, 0.2465, 0.6860]])\n",
    "class sample_Ynet(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(sample_Ynet, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(dim, 10),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(10,1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim])  \n",
    "v = sample_Ynet()\n",
    "Grad_Hess(x_tmp[:,0:dim],v)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NNs and coefficient functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural nets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNs for the solution and derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ynet(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(Ynet, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(dim, 10),\n",
    "            # nn.BatchNorm1d(num_features=10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10,1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim])  \n",
    "    \n",
    "class Znet(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(Znet, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(dim+1, 10),\n",
    "            # nn.BatchNorm1d(num_features=20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10,dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim])         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d NN for the diffusion coefficient of the wealth process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigma1D(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self,dim):\n",
    "        super(sigma1D, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(dim, 4),\n",
    "            nn.BatchNorm1d(num_features=4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4,1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim])\n",
    "    \n",
    "class sigma(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self,dim):\n",
    "        super(sigma, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(dim, 4),\n",
    "            nn.BatchNorm1d(num_features=4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4,dim*dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic class that defines the coefficients of the PDEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Implements the coefficeints based on the parameters '''\n",
    "class coefficient(object):\n",
    "    def __init__(self,params): #### out_shape =([M]) |  input:  x_shape=[M,D,1],  z shape = [M,D,1],a_shape= [M,D,D]  #This is for rho=0\n",
    "        self.dim=params['dim']#2\n",
    "        self.nu = params['nu']\n",
    "        self.kappa = params['kappa']\n",
    "        self.theta = params['theta']\n",
    "        self.eta = params['eta']\n",
    "        self.lb = params['lb']\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffuion coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''constant diffution coefficient'''  \n",
    "\n",
    "class constant_diff(coefficient):\n",
    "    '''This class is a constant diffusion coefficient which is the optimal diffusion'''\n",
    "    def __init__(self,params):\n",
    "        super(constant_diff, self).__init__(params)\n",
    "    def __call__(self,x):\n",
    "        tmp = x.shape[0]\n",
    "        s = torch.sqrt(torch.pow(self.lb,2).sum())/self.eta\n",
    "        return torch.diag(torch.cat((s,self.nu[0:self.dim-1]),axis=0)).repeat(tmp,1,1)\n",
    "\n",
    "'''Random diffusion coefficient for wealth process with diffusion of volatility processes all constant'''   \n",
    "class random_diff(coefficient):\n",
    "    def __init__(self,params):\n",
    "        super(random_diff, self).__init__(params)\n",
    "    def __call__(self,x):\n",
    "        tmp = x.shape[0]\n",
    "        return torch.diag(torch.cat((torch.rand(tmp,1),self.nu[0:self.dim-1].repeat(tmp,1)),axis=0))\n",
    "    \n",
    "'''NN diffusion coefficient for wealth process  with diffusion of volatility processes all constant'''\n",
    "class NN_diff_1D(coefficient):\n",
    "    def __init__(self,params,**kwargs):\n",
    "        super(NN_diff_1D, self).__init__(params)\n",
    "        if kwargs:\n",
    "            if 'diff' in kwargs.keys():\n",
    "                self.s = kwargs['diff']\n",
    "            else:\n",
    "                self.s = sigma1D(self.dim)\n",
    "                print(\"Warning: since the coefficient in not created using diff = diffusion(params), the diffusion coefficient is set to a neural net in first variable.\")\n",
    "            if 'update' in kwargs.keys():\n",
    "                self.update = kwargs['update']\n",
    "            else:\n",
    "                self.update = lambda x: torch.zeros(x.shape[0],1)\n",
    "                print(\"Warning: since the coefficient in not created using update = function, the update function is set to zero.\")\n",
    "        else:\n",
    "            self.s = sigma1D(self.dim)#Define an update function only for this coefficient to change its value.\n",
    "            self.update = lambda x: torch.zeros(x.shape[0],1) \n",
    "    def __call__(self,x):\n",
    "        tmp = self.s(x)+self.update(x)\n",
    "        shp = x.shape[0]\n",
    "        return torch.diag_embed(torch.cat((tmp,self.nu[0:self.dim-1].repeat(shp,1)),axis=1))\n",
    "\n",
    "# class updated(coefficient):\n",
    "#     def __init__(self,params,coef,adjust):\n",
    "#         self.adjust = adjust\n",
    "#         self.coef = coef\n",
    "#         super(update, self).__init__(params)\n",
    "#     def __call__(self,x):\n",
    "#         return self.coef(x) + self.adjust(x)    \n",
    "    \n",
    "'''NN diffusion for all components'''    \n",
    "class NN_diff(coefficient):\n",
    "    def __init__(self,params):\n",
    "        super(NN_diff, self).__init__(params)\n",
    "        self.s = sigma(self.dim)\n",
    "    def __call__(self,x):\n",
    "        return self.s0(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sigma1D(2)\n",
    "u = sigma1D(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = NN_diff_1D(params,diff = s, update = u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t(input[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero drift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Zero drift coefficient for all components'''  \n",
    "class zero_drift(coefficient):\n",
    "    def __init__(self,params):\n",
    "        super(zero_drift, self).__init__(params)\n",
    "    def __call__(self,x):\n",
    "        return torch.zeros(x.shape[0],x.shape[1]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift for the semilinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Drift of semilinear eqn: first component drift=0, others are OU'''\n",
    "    \n",
    "class OU_drift_semi(coefficient):\n",
    "    def __init__(self,params):\n",
    "        super(OU_drift_semi, self).__init__(params)\n",
    "    def __call__(self,x):\n",
    "        num_samples = x.shape[0]\n",
    "        output = torch.zeros(num_samples,dim)\n",
    "        for i in range(0,self.dim):\n",
    "            output[:,i] = self.kappa[i]*(self.theta[i] - x[:,i])\n",
    "        return output\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift for linear equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Drift of linear eqn: first component drift =|lb|*sqn(v_e)sigma_{00}(x), others are OU'''\n",
    "\n",
    "class OU_drift_lin(coefficient):\n",
    "    def __init__(self,params,v,sigma):\n",
    "        self.p = v\n",
    "        self.sigma = sigma\n",
    "        super(OU_drift_lin, self).__init__(params)\n",
    "    def __call__(self,x):\n",
    "        num_samples = x.shape[0]\n",
    "        q = self.p(x)[:,0]\n",
    "        output = torch.zeros(num_samples,dim)\n",
    "        output[:,0] = torch.sqrt(torch.pow(self.lb,2).sum())*torch.sgn(q)*self.sigma(x)[:,0,0]\n",
    "        for i in range(1,self.dim):\n",
    "            output[:,i] = self.kappa[i]*(self.theta[i] - x[:,i])\n",
    "        return output        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver of the semilinear BSDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class f_driver(coefficient):\n",
    "    def __init__(self,params): #### out_shape =([M]) |  input:  x_shape=[M,D,1],  z shape = [M,D,1],a_shape= [M,D,D]  #This is for rho=0\n",
    "        super(f_driver, self).__init__(params)\n",
    "    def __call__(self,x,z,a):\n",
    "        output=torch.zeros([x.shape[0]]).to(device)\n",
    "        for i in range(1,self.dim):\n",
    "            output = output - self.kappa[i]*(self.theta[i]-x[:,i,0])*z[:,i,0]\n",
    "        # print(result)\n",
    "        return output-torch.sqrt(torch.sum(torch.square(self.lb)))*torch.abs(z[:,0,0])*torch.abs(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class zero_source(coefficient):\n",
    "    def __init__(self,params):\n",
    "        super(zero_source, self).__init__(params)\n",
    "    def __call__(self,x):\n",
    "        return torch.zeros(x.shape[0],1)    \n",
    "class ell_source(coefficient):\n",
    "    def __init__(self,params,v,sigma,alpha_n):\n",
    "        self.p = v\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha_n\n",
    "        super(ell_source, self).__init__(params)\n",
    "    def direction(self,x):\n",
    "        num_samples = x.shape[0]        \n",
    "        q = self.p(x)[:,0]\n",
    "        gamma = sqr_jacobian(self.p,x)\n",
    "        x__ = torch.zeros(num_samples,dim,dim)\n",
    "        x__[:,0,0] = torch.sqrt(torch.pow(self.lb,2).sum())*torch.abs(q)\n",
    "        return x__+torch.transpose(self.sigma(x),1,2)*gamma\n",
    "    def __call__(self,x):\n",
    "        return  self.alpha*torch.pow(self.direction(x),2).sum(axis=1).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero discount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Zero drift coefficient for all components'''  \n",
    "class zero_discount(coefficient):\n",
    "    def __init__(self,params):\n",
    "        super(zero_discount, self).__init__(params)\n",
    "    def __call__(self,x):\n",
    "        return torch.zeros(x.shape[0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class terminal(coefficient):\n",
    "    def __init__(self,params):\n",
    "        self.eta = params['eta']\n",
    "    def __call__(self,x):\n",
    "        return (torch.tensor([1.0])-torch.exp(-self.eta*x[:,0])).unsqueeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = x_tmp[:,0:dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sqr_jacobian(test_fnc,input)\n",
    "sigma = constant_diff(params)\n",
    "torch.sqrt(torch.pow(params['lb'],2).sum())*test_fnc(input)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x__ = torch.zeros(2,dim,dim)\n",
    "x__[:,0,0] = -torch.sqrt(torch.pow(params['lb'],2).sum())*torch.abs(test_fnc(input)[:,0])\n",
    "torch.pow(x__-torch.transpose(sigma(input),1,2),2).sum(axis=-1).sum(axis=-1),*sqr_jacobian(test_fnc,input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = ell_source(params,test_fnc,sigma,torch.tensor([1.0]))\n",
    "L(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = OU_drift_lin(params,test_fnc,sigma)\n",
    "D(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tmp = torch.tensor([[0.,0.1,-1.4,.18],[0.,0.2,0.1672,.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = NN_diff_1D(params)#x[:,0]=time x[:,1:] space \n",
    "diff(input[:,1:]).reshape(input.shape[0],dim,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = terminal(params)\n",
    "g(x_tmp[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed-form solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class solution(object):\n",
    "    def __init__(self,params):\n",
    "        self.dim=params['dim']#2\n",
    "        self.nu = params['nu']\n",
    "        self.kappa = params['kappa']\n",
    "        self.theta = params['theta']\n",
    "        self.eta = params['eta']\n",
    "        self.lb = params['lb']\n",
    "        self.T = params['T']\n",
    "    def __call__(self,x):\n",
    "        return 1.-torch.exp(-self.eta*x[:,1]+0.5*torch.sum(torch.pow(self.lb,2))*(self.T-x[:,0])).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_exp = solution(params0)\n",
    "sol_exp(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot closed form solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =100\n",
    "x = torch.linspace(0.0,1.,steps=steps)\n",
    "y = torch.linspace(0.0,1.,steps=steps)\n",
    "xy = torch.cartesian_prod(x,y)\n",
    "x, y = torch.meshgrid(x, y, indexing='ij')\n",
    "v_T = g(xy).detach().reshape(steps,steps).squeeze(-1)\n",
    "v = sol_exp(xy).detach().reshape(steps,steps).squeeze(-1)\n",
    "f = plt.figure(figsize=(10,3),dpi=300);\n",
    "ax0 = f.add_subplot(1, 2, 1, projection='3d')\n",
    "ax0.plot_surface(x.numpy(),y.numpy(),v.numpy(),color='b',alpha=0.5);\n",
    "ax0.set_xlabel('time')\n",
    "ax0.set_ylabel('wealth')\n",
    "\n",
    "ax1 = f.add_subplot(1, 2, 2, projection='3d')\n",
    "ax1.plot_surface(x.numpy(),y.numpy(),v_T.numpy(),color='r',alpha=0.5);\n",
    "ax1.set_xlabel('wealth')\n",
    "ax1.set_ylabel('volatility');\n",
    "plt.tight_layout();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving linear equations\n",
    "\n",
    "$\\begin{cases}-\\partial v_t - \\dfrac12 (\\sigma^\\intercal\\sigma): D^2v -\\mu\\cdot\\nabla v + f(t,x) =0\\\\\n",
    "v(T,x)=g(x)\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(t,x)$ has shape [num_samples,dim,dim]\n",
    "$dW_t$  has shape [num_samples,dim]\n",
    "\n",
    "We need to unsqueeze $dW_t$ before batch multiplication.\n",
    "\n",
    "    \n",
    "    torch.bmm(sigma(x[:,:,i]),dw[:, :, i]dw[:, :, i].unsqueeze(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "size = [2**10, 9, 9]\n",
    "iid = torch.randn(size=size).to(device)\n",
    "print(\"It takes {:.3E} ms to generate iid samples.\".format(round(1000*(time.time()-t0),6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear(object):\n",
    "    def __init__(self,sigma,mu,source,kappa,terminal,T,n,a,b,dim):\n",
    "        num_samples = 2**10\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma \n",
    "        self.kappa = kappa \n",
    "        self.source = source \n",
    "        self.n = n\n",
    "        self.loss_epoch = []\n",
    "        self.num_epochs = 10000\n",
    "        self.epoch=0\n",
    "        self.x = torch.zeros((num_samples,dim+1,n+1))\n",
    "        self.Y0 = Ynet()\n",
    "        self.Z = Znet()\n",
    "        self.terminal = terminal\n",
    "        self.dt = torch.tensor([T/n]).to(device)\n",
    "        dw = torch.randn(size=[num_samples, dim, n]).to(device)* torch.sqrt(self.dt)#you can make randomness a universal variable if \n",
    "        self.x[:,1:,0]= a+(b-a)*torch.rand(num_samples,dim).to(device)\n",
    "        self.r = torch.ones((num_samples,1,n+1)).to(device)\n",
    "        self.c = torch.zeros((num_samples,1,n+1)).to(device)\n",
    "        self.sigmadw = torch.zeros((num_samples,dim,n)).to(device)\n",
    "        for i in range(self.n):\n",
    "            self.sigmadw[:,:,i] = torch.bmm(self.sigma(self.x[:,:,i]).reshape((num_samples,dim,dim)),dw[:, :, i].unsqueeze(2)).squeeze(2)\n",
    "            # print(self.x[:,1:,i].shape,self.mu(self.x[:,:,i]).shape,self.sigmadw[:,:,i].shape)\n",
    "            self.x[:,1:,i+1] = self.x[:,1:,i] + self.mu(self.x[:,:,i])*self.dt + self.sigmadw[:,:,i]\n",
    "            self.x[:,0,i+1] = self.x[:,0,i]+self.dt\n",
    "            self.r[:,:,i+1] = self.r[:,:,i]* torch.exp(-self.kappa(self.x[:,:,i])*self.dt)\n",
    "            if i == self.n -1 :\n",
    "                self.c[:,:,i+1] = self.terminal(self.x[:,1:,i+1])\n",
    "            self.c[:,:,i] = self.source(self.x[:,:,i])\n",
    "        self.x = self.x.clone().detach()\n",
    "        self.r = self.r.clone().detach()\n",
    "        self.sigmadw = self.sigmadw.clone().detach()\n",
    "        self.c = self.c.clone().detach()\n",
    "        \n",
    "    def loss(self):\n",
    "        # self.Zsigmadw = torch.zeros((num_samples,1,n)).to(device)\n",
    "        for i in range(self.n):   \n",
    "            if i == 0:\n",
    "                Y =  self.Y0(self.x[:,1:,0])\n",
    "            else:\n",
    "                # self.Zsigmadw[:,:,i] = torch.bmm(self.Z(self.x[:,:,i]).unsqueeze(1),self.sigmadw[:,:,i].unsqueeze(2)).squeeze(2)\n",
    "                Y = Y*self.r[:,:,i] - self.c[:,:,i]*self.dt + torch.bmm(self.Z(self.x[:,:,i]).unsqueeze(1),self.sigmadw[:,:,i].unsqueeze(2)).squeeze(2)\n",
    "        return torch.pow(self.c[:,:,-1]-Y,2).mean()\n",
    "        \n",
    "    def train(self,lr):\n",
    "        t_0 = time.time()\n",
    "        self.lr = lr\n",
    "        parameters = list(self.Y0.parameters()) + list(self.Z.parameters())\n",
    "        optimizer = optim.Adam(parameters, self.lr)\n",
    "        L_ = torch.Tensor([-2.0])\n",
    "        loss = torch.Tensor([2.0])\n",
    "        while (torch.abs(L_-loss)>1e-10) & (self.epoch < self.num_epochs):# epoch in range(num_epochs):\n",
    "            t_1 = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            loss= self.loss()#self.cost(self.X,self.modelu(X))+ torch.mean(self.terminal(update(self.X,self.modelu(X))))#\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            self.loss_epoch.append(loss)\n",
    "            if self.epoch>0:\n",
    "                L_ = self.loss_epoch[self.epoch-1]\n",
    "            if (self.epoch % int(self.num_epochs/10)== int(self.num_epochs/10)-1):\n",
    "                print(\"At epoch {} the mean error is {:.2E}.\".format(self.epoch+1,loss.detach()))\n",
    "                self.time_display(t_0, t_1)\n",
    "            self.epoch += 1\n",
    "        print(\"Training took {} epochs.\".format(self.epoch))\n",
    "\n",
    "    def time_display(self, t_0, t_1):\n",
    "        print(\"Training this epoch takes {} ms.\".format(round(1000*(time.time()-t_1),2)))\n",
    "        print(\"So far: {} ms in training.\".format(round(1000*(time.time()-t_0),2)))\n",
    "                \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = constant_diff(params0)\n",
    "m = zero_drift(params0)\n",
    "k = zero_discount(params0)\n",
    "f = zero_source(params0)\n",
    "g=terminal(params0)\n",
    "a = torch.tensor([0.0])\n",
    "b = torch.tensor([1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_interval = 20\n",
    "heat = linear(s,m,f,k,g,T,20,a,b,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat.train(lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([l.detach().numpy() for l in heat.loss_epoch]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(np.arange(len(heat.loss_epoch))+1),[np.log(l.detach().numpy()) for l in heat.loss_epoch]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_it(eqn):\n",
    "    sol_exp = solution(params0)\n",
    "    if dim == 2:\n",
    "        steps =100\n",
    "        x = torch.linspace(0.0,1.,steps=steps)\n",
    "        y = torch.linspace(0.0,1.,steps=steps)\n",
    "        xy = torch.cartesian_prod(x,y)\n",
    "        txy = torch.cat((torch.zeros(xy.shape[0],1),xy),axis=1)\n",
    "        v_T = eqn.terminal(xy).detach().reshape(steps,steps).squeeze(-1)\n",
    "        v = eqn.Y0(xy).detach().reshape(steps,steps).squeeze(-1)\n",
    "        v_0 = sol_exp(txy).detach().reshape(steps,steps).squeeze(-1)\n",
    "        f,ax = plt.subplots(1,1,figsize=(15,5),dpi=300);\n",
    "        ax = plt.axes(projection='3d')\n",
    "        X, Y = torch.meshgrid(x, y, indexing='ij')\n",
    "        ax.plot_surface(X.numpy(),Y.numpy(),v.numpy());\n",
    "        ax.plot_surface(X.numpy(),Y.numpy(),v_0.numpy(),color='r',alpha=0.5);\n",
    "        ax.set_xlabel('wealth')\n",
    "        ax.set_ylabel('volatility');\n",
    "        plt.tight_layout();\n",
    "    data = torch.rand(2**14,dim)\n",
    "    v = eqn.Y0(data).detach().squeeze(-1)\n",
    "    tdata = torch.cat((torch.zeros(data.shape[0],1),data),axis=1)\n",
    "    v_0 = sol_exp(tdata).detach().squeeze(-1)\n",
    "    print(\"The MSE is {:.3E}.\".format(pow(v-v_0,2).mean()))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_it(heat);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving semilinear equatoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class semilinear(object):\n",
    "    def __init__(self,sigma,mu,F,kappa,terminal,T,n,a,b,dim):\n",
    "        self.num_samples = 2**10\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.kappa = kappa \n",
    "        self.F = F\n",
    "        self.n = n\n",
    "        self.x = torch.zeros((self.num_samples,dim+1,n+1))\n",
    "        self.Y0 = Ynet()\n",
    "        self.Z = Znet()\n",
    "        self.terminal = terminal\n",
    "        self.epoch=0\n",
    "        self.loss_epoch = []\n",
    "        self.num_epochs = 10000\n",
    "        self.dt = torch.tensor([T/n]).to(device)\n",
    "        dw = torch.randn(size=[self.num_samples, dim, n]).to(device)* torch.sqrt(self.dt)\n",
    "        self.x[:,1:,0]= a+(b-a)*torch.rand(self.num_samples,dim).to(device)\n",
    "        self.r = torch.ones((self.num_samples,1,n+1)).to(device)\n",
    "\n",
    "        self.sigmadw = torch.zeros((self.num_samples,dim,n)).to(device)\n",
    "        for i in range(self.n):\n",
    "            self.sigmadw[:,:,i] = torch.bmm(self.sigma(self.x[:,:,i]).reshape((self.num_samples,dim,dim)),dw[:, :, i].unsqueeze(2)).squeeze(2)\n",
    "            self.x[:,1:,i+1] = self.x[:,1:,i] + self.mu(self.x[:,:,i])*self.dt + self.sigmadw[:,:,i]\n",
    "            self.x[:,0,i+1] = self.x[:,0,i]+self.dt\n",
    "            # print(torch.exp(-self.kappa(self.x[:,:,i])).shape, self.r[:,:,i].shape)\n",
    "            self.r[:,:,i+1] = self.r[:,:,i]* torch.exp(-self.kappa(self.x[:,:,i])*self.dt)\n",
    "        self.x = self.x.clone().detach()\n",
    "        self.sigmadw = self.sigmadw.clone().detach()\n",
    "        self.r = self.r.clone().detach()\n",
    "        \n",
    "    def loss(self):\n",
    "        c = torch.zeros((self.num_samples,1,self.n+1)).to(device)\n",
    "        for i in range(self.n):   \n",
    "            if i == 0:\n",
    "                Y =  self.Y0(self.x[:,1:,0])\n",
    "            else:\n",
    "                if i == self.n - 1:\n",
    "                    c[:,:,i+1] = self.terminal(self.x[:,1:,i+1])\n",
    "                c[:,:,i] = self.F(self.x[:,1:,i].unsqueeze(-1),self.Z(self.x[:,:,i]).unsqueeze(-1),self.sigma(self.x[:,:,i]).reshape((self.num_samples,dim,dim))[:,0,0]).unsqueeze(-1)\n",
    "                Y = Y*self.r[:,:,i] - c[:,:,i]*self.dt + torch.bmm(self.Z(self.x[:,:,i]).unsqueeze(1),self.sigmadw[:,:,i].unsqueeze(2)).squeeze(2)\n",
    "                # print(Y)\n",
    "        return torch.pow(c[:,:,-1]-Y,2).mean()\n",
    "        \n",
    "    def train(self,lr):\n",
    "        t_0 = time.time()\n",
    "        self.lr = lr\n",
    "        parameters = list(self.Y0.parameters()) + list(self.Z.parameters())\n",
    "        optimizer = optim.Adam(parameters, self.lr)\n",
    "        L_ = torch.Tensor([-2.0])\n",
    "        loss = torch.Tensor([2.0])\n",
    "        while  (torch.abs(L_-loss)>1e-10) & (self.epoch < self.num_epochs):# \n",
    "            t_1 = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            loss= self.loss()##\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            self.loss_epoch.append(loss)\n",
    "            if self.epoch>0:\n",
    "                L_ = self.loss_epoch[self.epoch-1]\n",
    "            if (self.epoch % int(self.num_epochs/10)== int(self.num_epochs/10)-1):\n",
    "                print(\"At epoch {} the mean error is {:.2E}.\".format(self.epoch+1,loss.detach()))\n",
    "                self.time_display(t_0,t_1)          \n",
    "            self.epoch += 1\n",
    "                \n",
    "    def time_display(self,t_0,t_1):\n",
    "        print(\"Training this epoch takes {:.3E}s.\".format(time.time()-t_1))\n",
    "        print(\"So far spend {:.3E} in training.\".format(time.time()-t_0))      \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = constant_diff(params0)\n",
    "m = OU_drift_semi(params0)\n",
    "k = zero_discount(params0)\n",
    "f = zero_source(params0)\n",
    "g=terminal(params0)\n",
    "F = f_driver(params0)\n",
    "a = torch.tensor([0.0])\n",
    "b = torch.tensor([1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi = semilinear(s,m,F,k,g,params0['T'],20,a,b,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi.train(lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot value function 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_it(semi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([l.detach().numpy() for l in semi.loss_epoch]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(np.arange(len(semi.loss_epoch))+1),[np.log(l.detach().numpy()) for l in semi.loss_epoch]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine both linear and semilinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eqn(object):\n",
    "    def __init__(self,params,sigma,mu,kappa,terminal): #params=dim,T,n,a,b,num_samples\n",
    "        self.dim = params['dim']\n",
    "        self.T = params['T']\n",
    "        self.num_samples = params['num_samples']\n",
    "        self.n = params['num_steps']\n",
    "        self.a = params['a']\n",
    "        self.b = params['b']\n",
    "        self.loss_epoch = []\n",
    "        self.num_epochs = 10000\n",
    "        self.epoch=0\n",
    "        self.sigma = sigma         \n",
    "        self.mu = mu\n",
    "        self.kappa = kappa \n",
    "        self.terminal = terminal\n",
    "        self.x = torch.zeros((self.num_samples,self.dim+1,self.n+1))\n",
    "        self.dt = torch.tensor([self.T/self.n]).to(device)\n",
    "        self.Y0 = Ynet()\n",
    "        self.Z = Znet()\n",
    "        self.dw = torch.randn(size=[self.num_samples, self.dim, self.n]).to(device)* torch.sqrt(self.dt)\n",
    "        self.x[:,1:,0]= self.a+(self.b-self.a)*torch.rand(self.num_samples,self.dim).to(device)\n",
    "        self.r = torch.ones((self.num_samples,1,self.n+1)).to(device)\n",
    "        self.sigmadw = torch.zeros((self.num_samples,self.dim,self.n)).to(device)\n",
    "        for i in range(self.n):\n",
    "            self.sigmadw[:,:,i] = torch.bmm(self.sigma(self.x[:,:,i]).reshape((self.num_samples,self.dim,self.dim)),self.dw[:, :, i].unsqueeze(2)).squeeze(2)\n",
    "            self.x[:,1:,i+1] = self.x[:,1:,i] + self.mu(self.x[:,:,i])*self.dt + self.sigmadw[:,:,i]\n",
    "            self.x[:,0,i+1] = self.x[:,0,i]+self.dt\n",
    "            self.r[:,:,i+1] = self.r[:,:,i]* torch.exp(-self.kappa(self.x[:,:,i])*self.dt)\n",
    "        self.x = self.x.clone().detach()\n",
    "        self.r = self.r.clone().detach()\n",
    "        self.sigmadw = self.sigmadw.clone().detach()\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        t_0 = time.time()\n",
    "        self.lr = 1e-2\n",
    "        parameters = list(self.Y0.parameters()) + list(self.Z.parameters())\n",
    "        optimizer = optim.Adam(parameters, self.lr)\n",
    "        L_ = torch.Tensor([-2.0])\n",
    "        loss = torch.Tensor([2.0])\n",
    "        while  (torch.abs(L_-loss)>1e-10) & (self.epoch < self.num_epochs):# epoch in range(num_epochs):\n",
    "            t_1 = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            loss= self.loss()##\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            self.loss_epoch.append(loss)\n",
    "            if self.epoch>0:\n",
    "                L_ = self.loss_epoch[self.epoch-1]\n",
    "            if (self.epoch % int(self.num_epochs/10)== int(self.num_epochs/10)-1):\n",
    "                print(\"At epoch {} the mean loss is {:.2E}.\".format(self.epoch+1,loss.detach()))\n",
    "                self.time_display(t_0,t_1)          \n",
    "            self.epoch += 1\n",
    "        print(\"Training took {} epochs.\".format(self.epoch))\n",
    "            \n",
    "                \n",
    "    def time_display(self, t_0, t_1):\n",
    "        print(\"Training this epoch takes {:.3E} ms.\".format(round(1000*(time.time()-t_1),2)))\n",
    "        print(\"So far: {:.3E} ms in training.\".format(round(1000*(time.time()-t_0),2)))    \n",
    "\n",
    "class lin(eqn):\n",
    "    def __init__(self,params,sigma,mu,kappa,terminal,source):\n",
    "        self.source = source    \n",
    "        super(lin,self).__init__(params,sigma,mu,kappa,terminal)\n",
    "        self.c = torch.zeros((self.num_samples,1,self.n+1)).to(device)\n",
    "        for i in range(self.n):\n",
    "            if i == self.n -1 :\n",
    "                self.c[:,:,i+1] = self.terminal(self.x[:,1:,i+1])\n",
    "            self.c[:,:,i] = self.source(self.x[:,:,i])\n",
    "            self.c = self.c.clone().detach()\n",
    "\n",
    "            \n",
    "    def loss(self):\n",
    "        # self.Zsigmadw = torch.zeros((num_samples,1,n)).to(device)\n",
    "        for i in range(self.n):   \n",
    "            if i == 0:\n",
    "                Y =  self.Y0(self.x[:,1:,0])\n",
    "            else:\n",
    "                # self.Zsigmadw[:,:,i] = torch.bmm(self.Z(self.x[:,:,i]).unsqueeze(1),self.sigmadw[:,:,i].unsqueeze(2)).squeeze(2)\n",
    "                Y = Y*self.r[:,:,i] - self.c[:,:,i]*self.dt + torch.bmm(self.Z(self.x[:,:,i]).unsqueeze(1),self.sigmadw[:,:,i].unsqueeze(2)).squeeze(2)\n",
    "                # print(Y)\n",
    "        return torch.pow(self.c[:,:,-1]-Y,2).mean()\n",
    "    \n",
    "class semi(eqn):\n",
    "    def __init__(self,params,sigma,mu,kappa,terminal,F):\n",
    "        self.F = F    \n",
    "        super(semi,self).__init__(params,sigma,mu,kappa,terminal)\n",
    "\n",
    "            \n",
    "    def loss(self):\n",
    "        c = torch.zeros((self.num_samples,1,self.n+1)).to(device)\n",
    "        for i in range(self.n):   \n",
    "            if i == 0:\n",
    "                Y =  self.Y0(self.x[:,1:,0])\n",
    "            else:\n",
    "                if i == self.n - 1:\n",
    "                    c[:,:,i+1] = self.terminal(self.x[:,1:,i+1])\n",
    "                c[:,:,i] = self.F.eval(self.x[:,1:,i].unsqueeze(-1),self.Z(self.x[:,:,i]).unsqueeze(-1),self.sigma(self.x[:,:,i]).reshape((self.num_samples,dim,dim))[:,0,0]).unsqueeze(-1)\n",
    "                Y = Y*self.r[:,:,i] - c[:,:,i]*self.dt + torch.bmm(self.Z(self.x[:,:,i]).unsqueeze(1),self.sigmadw[:,:,i].unsqueeze(2)).squeeze(2)\n",
    "                # print(Y)\n",
    "        return torch.pow(c[:,:,-1]-Y,2).mean()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim':2,\n",
    "        'T':1,\n",
    "        'num_samples':2**10,\n",
    "        'num_steps':20,\n",
    "        'a':torch.tensor([0.0]),\n",
    "        'b':torch.tensor([1.0])}\n",
    "x=torch.rand((8,dim+1))\n",
    "s = sigma()\n",
    "m = mu()\n",
    "k = kappa()\n",
    "f = source()\n",
    "a = torch.tensor([0.0])\n",
    "b = torch.tensor([1.0])\n",
    "num_time_interval = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_eqn = lin(params,s,m,k,expo,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_eqn.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_eqn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =100\n",
    "x = torch.linspace(0.25,0.75,steps=steps)\n",
    "y = torch.linspace(0.25,0.75,steps=steps)\n",
    "xy = torch.cartesian_prod(x,y)\n",
    "x, y = torch.meshgrid(x, y, indexing='ij')\n",
    "z_T = lin.terminal(xy).detach().reshape(steps,steps).squeeze(-1)\n",
    "z = lin.Y0(xy).detach().reshape(steps,steps).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x.numpy(),y.numpy(),z.numpy());\n",
    "ax.plot_surface(x.numpy(),y.numpy(),z_T.numpy());\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =100\n",
    "x = torch.linspace(0,1,steps=steps)\n",
    "y = torch.linspace(0,1,steps=steps)\n",
    "xy = torch.cartesian_prod(x,y)\n",
    "x, y = torch.meshgrid(x, y, indexing='ij')\n",
    "z_T = lin.terminal(xy).detach().reshape(steps,steps).squeeze(-1)\n",
    "txy = torch.cat((torch.zeros((xy.shape[0],1)),xy),axis=1)\n",
    "z = lin.Z(txy).detach().reshape(steps,steps,2).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x.numpy(),y.numpy(),z[:,:,0].numpy());\n",
    "# ax.plot_surface(x.numpy(),y.numpy(),z_T.numpy());\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x.numpy(),y.numpy(),z[:,:,1].numpy());\n",
    "# ax.plot_surface(x.numpy(),y.numpy(),z_T.numpy());\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi1 = semi(params,s,m,k,expo,F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi1.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =100\n",
    "x = torch.linspace(0.25,0.75,steps=steps)\n",
    "y = torch.linspace(0.25,0.75,steps=steps)\n",
    "xy = torch.cartesian_prod(x,y)\n",
    "x, y = torch.meshgrid(x, y, indexing='ij')\n",
    "z_T = semi1.terminal(xy).detach().reshape(steps,steps).squeeze(-1)\n",
    "z = semi1.Y0(xy).detach().reshape(steps,steps).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x.numpy(),y.numpy(),z.numpy());\n",
    "ax.plot_surface(x.numpy(),y.numpy(),z_T.numpy());\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =100\n",
    "x = torch.linspace(0,1,steps=steps)\n",
    "y = torch.linspace(0,1,steps=steps)\n",
    "xy = torch.cartesian_prod(x,y)\n",
    "x, y = torch.meshgrid(x, y, indexing='ij')\n",
    "z_T = semi1.terminal(xy).detach().reshape(steps,steps).squeeze(-1)\n",
    "txy = torch.cat((torch.zeros((xy.shape[0],1)),xy),axis=1)\n",
    "z = semi1.Z(txy).detach().reshape(steps,steps,2).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x.numpy(),y.numpy(),z[:,:,0].numpy());\n",
    "# ax.plot_surface(x.numpy(),y.numpy(),z_T.numpy());\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(x.numpy(),y.numpy(),z[:,:,1].numpy());\n",
    "# ax.plot_surface(x.numpy(),y.numpy(),z_T.numpy());\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding second derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    " \n",
    "def test_fn(x):\n",
    "    return torch.bmm(x.unsqueeze(-1),x.unsqueeze(1)).unsqueeze(-1)\n",
    "\n",
    "x = torch.tensor([[0.0, 1.0],[1.0,0.0]], requires_grad=True)\n",
    " \n",
    "# Compute the gradient of f with respect to x\n",
    "def gradiant(x,f):\n",
    "    return grad(outputs=f(x), inputs=x, grad_outputs=torch.ones_like(f(x)), create_graph=True, retain_graph=True, only_inputs=True)[0]#.unsqueeze(-1)\n",
    " \n",
    "# Print the gradient\n",
    "print(gradiant(x,test_fn).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def der1(x,f):  #output= [M,D,1], input: x=[M,D,1]\n",
    "  Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "  xin=x.clone().detach()\n",
    "  xin.requires_grad=True\n",
    "  # print('xin shape', xin.shape)\n",
    "  u=f(xin)\n",
    "  # print(u)\n",
    "  # print('u/f(xin) shape', u.shape)\n",
    "  Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                          allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(-1)\n",
    "  # print('Du shape before reshaped', Du.shape)\n",
    "  Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "  # print('Du shape after reshaped', Du.shape)\n",
    "  return Du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "der1(x,test_fn).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Hess(x,f_): #output= [M,D,D], #input: x=[M,D], t=[M,1], xt= [M,D+1]\n",
    "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "    xin=x[:,1:].clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    u=f_(xin)\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
    "    Du = torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print(\"Du after reshape:\",Du.shape)\n",
    "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
    "    return Du, hess_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi1.x[:,:,0].squeeze(-1).shape,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.0,0.0,1.0],[0.0,1.0,0.0],[0.0,1.4,0.7]], requires_grad=True)\n",
    "Grad_Hess(semi1.x[:,:,0],semi1.Y0)[1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test second derivative with NN\n",
    "\n",
    "<font color=\"red\" size=\"4\">Batch normaizations srews up the derivative.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hess(x,f_): #output= [M,D,D], #input: x=[M,D]\n",
    "    dim = x.shape[1]\n",
    "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "    xin=x.clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    u=f_(xin)\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
    "    Du = torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print(\"Du after reshape:\",Du.shape)\n",
    "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
    "    return Du, hess_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN1d(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(NN1d, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(1, 20),\n",
    "            # nn.BatchNorm1d(num_features=20),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(20,1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fn = NN1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = torch.rand(100,1)\n",
    "# x_tr = torch.rand(0,1,100).unsqueeze(-1)\n",
    "y_tr = x_tr*x_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100000\n",
    "lr = 1e-2\n",
    "optimizer = optim.Adam(test_fn.parameters(), lr)\n",
    "L_ = torch.Tensor([-2.0])\n",
    "loss = torch.Tensor([2.0])\n",
    "epoch=0\n",
    "while  (torch.abs(L_-loss)>1e-8) & (epoch < num_epochs):# epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss= torch.pow(y_tr-test_fn(x_tr),2).mean()##\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch % int(num_epochs/10)== int(num_epochs/10)-1):\n",
    "        print(\"At epoch {} the mean loss is {}.\".format(epoch+1,loss.detach()))\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ts = torch.linspace(0,1,100).unsqueeze(-1)\n",
    "plt.plot(x_ts.detach().numpy(),test_fn(x_ts).detach().numpy());\n",
    "plt.plot(x_ts.detach().numpy(),x_ts*x_ts.detach().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xin = x_ts.clone().detach()\n",
    "xin.requires_grad=True\n",
    "dy = gradiant(xin,test_fn).squeeze(-1)\n",
    "dy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(x_tr.detach().numpy(),test_fn(x_tr).detach().numpy());\n",
    "plt.plot(x_ts.detach().numpy(),dy.detach().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradiant(torch.tensor([[1.0],[0.5],[0.0]],requires_grad=True),test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddy = Hess(x_ts,test_fn)[1].squeeze(-1)\n",
    "plt.plot(x_ts.detach().numpy(),ddy.detach().numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\color{red}\\text{It does not matter if the second derivative is not accurate. } \\ell^2  \\text{ will be positive and the direction is always a direction of increase.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
