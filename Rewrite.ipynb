{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, CyclicLR, ReduceLROnPlateau, LinearLR, ExponentialLR\n",
    "import random\n",
    "from torch.autograd.functional import jacobian, hessian\n",
    "# import AUTOGRAD.FUNCTIONAL.JACOBIAN as jacobian\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=1\n",
    "kappa=torch.tensor([1]).to(device)\n",
    "theta=torch.tensor([0.4]).to(device)\n",
    "nu=torch.tensor([0.02]).to(device)\n",
    "lamb=torch.tensor([0.15]).to(device)\n",
    "eta=torch.tensor([0.5]).to(device)\n",
    "rho=torch.tensor([0.0]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 1])\n",
      "torch.Size([8, 1, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9f/dlwft6dn7hxg65r7xqyxvk6r0000gn/T/ipykernel_27638/916122852.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  delta_t = torch.tensor(T/ num_time_interval)\n"
     ]
    }
   ],
   "source": [
    "num_samples=2**3   #M\n",
    "#dim=5\n",
    "num_time_interval=2    #N\n",
    "T=torch.tensor([1]).to(device)\n",
    "delta_t = torch.tensor(T/ num_time_interval)\n",
    "# t_steps = torch.linspace(0,1,num_time_interval).to(device)\n",
    "# sqrt_delta_t = torch.sqrt(delta_t).to(device)\n",
    "sqrt_delta_t = torch.sqrt(delta_t).to(device)\n",
    "x_init = torch.zeros(dim)\n",
    "# sigma=torch.sqrt(torch.tensor([2.0]))\n",
    "# lamb=torch.tensor([1.0]).to(device)\n",
    "mu=torch.tensor([1.0]).to(device)\n",
    "# sigma=torch.tensor([1.0]).to(device)\n",
    "alpha=torch.tensor([1.0]).to(device)\n",
    "start = torch.tensor([0]).to(device)\n",
    "end = torch.tensor([1]).to(device)\n",
    "num_steps=5\n",
    "num_ite=int(num_time_interval/num_steps)\n",
    "# kappa=torch.tensor([1]).to(device)\n",
    "# theta=torch.tensor([0.4]).to(device)\n",
    "# nu=torch.tensor([0.5]).to(device)\n",
    "# lamb=torch.tensor([0.6]).to(device)\n",
    "# eta=torch.tensor([0.5]).to(device)\n",
    "p=torch.tensor([0.95]).to(device)\n",
    "# rho=torch.tensor([0]).to(device)\n",
    "num_runs=1\n",
    "# x0=torch.ones([1,dim]).to(device)+4\n",
    "# print(x0)\n",
    "\n",
    "t_steps = torch.linspace(0,T.item(),num_time_interval+1).to(device)\n",
    "# print(t_steps)\n",
    "test_interval=[0.5,1.5]\n",
    "# v=(torch.ones(num_sample,dim,dim)-0.3).to(device)\n",
    "# v=(torch.ones(num_sample,dim,dim)).to(device)\n",
    "v=torch.diag(torch.ones(dim)).to(device)\n",
    "# print(v)\n",
    "v=v.unsqueeze(0).repeat(num_sample,1,1)\n",
    "print(v.shape)\n",
    "# print(v)\n",
    "\n",
    "\n",
    "# dt=T/num_time_interval\n",
    "Dt=torch.zeros((num_samples,1,num_time_interval+1)).to(device)\n",
    "print(Dt.shape)\n",
    "Dt[:,:,1:]=delta_t\n",
    "t=torch.cumsum(Dt,axis=2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearity - Full - $h(t,x,p,q,\\gamma)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(x,z,gamma): #### out_shape =([M]) |  input:  x_shape=[M,dim,1],  z shape = [M,dim,1], gamma shape= [M,dim,dim]\n",
    "    result=torch.zeros([x.shape[0]]).to(device)\n",
    "    for i in range(dim-1):\n",
    "      result+=kappa[i]*(theta[i]-x[:,i+1,0])*z[:,i+1,0]+0.5*torch.pow(nu[i],2)*gamma[:,i+1,i+1]-rho[i]*lamb[i]*nu[i]*(z[:,0,0]*gamma[:,0,i+1])/gamma[:,0,0] \\\n",
    "      + 0.5*torch.pow(rho[i]*nu[i]*gamma[:,0,i+1],2)/gamma[:,0,0]\n",
    "      # print(result)\n",
    "    result1=result-0.5*torch.sum(torch.square(lamb))*torch.pow(z[:,0,0],2)/gamma[:,0,0]\n",
    "    return result1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinarity - semi $(t,x,p,q,\\sigma)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def F(x,z,a): #### out_shape =([M]) |  input:  x_shape=[M,D,1],  z shape = [M,D,1],a_shape= [M,D,D]  #This is for rho=0\n",
    "  result=torch.zeros([x.shape[0]]).to(device)\n",
    "  for i in range(dim-1):\n",
    "    result+=-kappa[i]*(theta[i]-x[:,i+1,0])*z[:,i+1,0]\n",
    "    # print(result)\n",
    "  result1=result-torch.sqrt(torch.sum(torch.square(lamb)))*torch.abs(z[:,0,0])*torch.sqrt(a[:,0,0])\n",
    "  return result1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def g1(x): #out_shape= [M,1,1]   | input:  x_shape=[M,D]\n",
    "  result=1-torch.exp(-eta*x[:,0])\n",
    "  # result=torch.reshape(result,(result.shape[0],1,1))\n",
    "  result=torch.reshape(result,(result.shape[0],1))\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic of SDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def update(data,delta_w,sigma):#output: data=(M,D,1) ,  #input data=(M,D,1), delta_w=[M,D,1], sigma=[M,D,1]\n",
    "    dx = torch.bmm(sigma,delta_w)\n",
    "    data = data + dx\n",
    "    data=data\n",
    "    return data\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def der1(x,f):  #output= [M,D,1], input: x=[M,D,1]\n",
    "  Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "  xin=x.clone().detach()\n",
    "  xin.requires_grad=True\n",
    "  # print('xin shape', xin.shape)\n",
    "  u=f(xin)\n",
    "  # print(u)\n",
    "  # print('u/f(xin) shape', u.shape)\n",
    "  Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                          allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "  # print('Du shape before reshaped', Du.shape)\n",
    "  Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "  # print('Du shape after reshaped', Du.shape)\n",
    "  return Du\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First and second derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def grad_hessian1(t, x,f_): #output= [M,D,D], #input: x=[M,D], t=[M,1], xt= [M,D+1]\n",
    "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "\n",
    "    xin=x.clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    # print(\"xin_shape\", xin.shape)\n",
    "    tin=t.clone().detach()\n",
    "    # print(\"tin shape\", tin.shape)\n",
    "    xt_in=torch.cat((xin,tin),1)\n",
    "    # print(\"xt_in shape\", xt_in.shape)\n",
    "\n",
    "\n",
    "\n",
    "    # xt_in.requires_grad=True\n",
    "    u=f_(xt_in)\n",
    "    # print(\"u shape: \",u.shape)\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "    # print(\"Du shape:\",Du.shape)\n",
    "    # print(\"-----\")\n",
    "    # print(torch.autograd.grad(outputs=[Du[:,0,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,0,:]),\n",
    "    #                        allow_unused=True,retain_graph=True,create_graph=True)[0].shape)\n",
    "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
    "    # print(\"D2u shape:\",hess_temp.shape)\n",
    "    Du = torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print(\"Du after reshape:\",Du.shape)\n",
    "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
    "    # print(\"D2u after reshape:\",hess_temp.shape)\n",
    "\n",
    "    # print(hess_temp)\n",
    "    return Du, hess_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def grad_model(t, x,f_): #output= [M,D,1], #input: x=[M,D], t=[M,1], xt= [M,D+1]\n",
    "    # hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "\n",
    "    xin=x.clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    # print(\"xin_shape\", xin.shape)\n",
    "    tin=t.clone().detach()\n",
    "    # print(\"tin shape\", tin.shape)\n",
    "    xt_in=torch.cat((xin,tin),1)\n",
    "    # print(\"xt_in shape\", xt_in.shape)\n",
    "\n",
    "    # xt_in.requires_grad=True\n",
    "    u=f_(xt_in)\n",
    "    # print(\"u shape: \",u.shape)\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0].unsqueeze(2)\n",
    "\n",
    "    Du = torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print(\"Du after reshape:\",Du.shape)\n",
    "\n",
    "    return Du\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hessian_model(t,x,model):\n",
    "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "\n",
    "    xin=x.clone().detach()\n",
    "    xin.requires_grad=True\n",
    "    # print(\"xin_shape\", xin.shape)\n",
    "    tin=t.clone().detach()\n",
    "    # print(\"tin shape\", tin.shape)\n",
    "    xt_in=torch.cat((xin,tin),1)\n",
    "    # print(\"xt_in shape\", xt_in.shape)\n",
    "    Du = model(xt_in)\n",
    "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i]),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
    "\n",
    "    # print(\"D2u shape:\",hess_temp.shape)\n",
    "\n",
    "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
    "\n",
    "    # print(hess_temp)\n",
    "    return hess_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Hessian!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def hessian1(x,model2z):  #x=[M,D,1]  output= [M,D,1]\n",
    "    hess_temp=torch.zeros(x.shape[0],dim,dim).to(device)\n",
    "    # print(\"D2u shape:\",hess_temp.shape)\n",
    "\n",
    "    Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "    xin=x.clone().detach()\n",
    "    # print('this is xin')\n",
    "    # print(xin)\n",
    "    xin.requires_grad=True\n",
    "    Du = model2z(xin.squeeze(2))\n",
    "    # print('this is Du')\n",
    "    # print(Du)\n",
    "    # print('this is Du shape')\n",
    "    # print(Du.shape)\n",
    "    # print('this is Du 1')\n",
    "    # print(Du[:,1,:])\n",
    "    hess_temp= torch.cat([ torch.autograd.grad(outputs=[Du[:,i,:]],inputs=[xin],grad_outputs=torch.ones_like(Du[:,i,:]),\n",
    "                           allow_unused=True,retain_graph=True,create_graph=True)[0] for i in range(dim)],1)\n",
    "\n",
    "    # print(\"D2u shape:\",hess_temp.shape)\n",
    "\n",
    "    hess_temp=torch.reshape(hess_temp,(hess_temp.shape[0],dim,dim))\n",
    "\n",
    "    # print(hess_temp)\n",
    "    return hess_temp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grad_fn2(x,z,a,f,xx=False,r=False,p=False,alpha=False):  #output= [M,D,1], input: x=[M,D,1] z=[M,D,1], a=[M,D,D]\n",
    "  Du=torch.zeros(x.shape[0],dim).to(device)\n",
    "  xin=x.clone().detach()\n",
    "  xin.requires_grad=True\n",
    "  # print('xin shape', xin.shape)\n",
    "  zin=z.clone().detach()\n",
    "  zin.requires_grad=True\n",
    "  # print('zin shape', zin.shape)\n",
    "  ain=a.clone().detach()\n",
    "  ain.requires_grad=True\n",
    "  # print('ain shape', ain.shape)\n",
    "  # ain=torch.reshape(ain,(num_sample,dim*dim,1))\n",
    "  u=f(xin,zin,ain)\n",
    "  # print(u)\n",
    "  # print('u/f(xin) shape', u.shape)\n",
    "  if xx==True:\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[xin],grad_outputs=torch.ones_like(u),\n",
    "                            allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
    "    # print('Du shape before reshaped', Du.shape)\n",
    "    Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print('Du shape after reshaped', Du.shape)\n",
    "  if r==True:\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[rin],grad_outputs=torch.ones_like(u),\n",
    "                            allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
    "    # print('Du shape before reshaped', Du.shape)\n",
    "    Du=torch.reshape(Du,(Du.shape[0],1,1))\n",
    "    # Du=torch.reshape(Du,(Du.shape[0],dim,dim))\n",
    "    # print('Du shape after reshaped', Du.shape)\n",
    "  if p==True:\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[zin],grad_outputs=torch.ones_like(u),\n",
    "                            allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
    "    # print('Du shape before reshaped', Du.shape)\n",
    "    Du=torch.reshape(Du,(Du.shape[0],dim,1))\n",
    "    # print('Du shape after reshaped', Du.shape)\n",
    "  if alpha==True:\n",
    "    Du=torch.autograd.grad(outputs=[u],inputs=[ain],grad_outputs=torch.ones_like(u),\n",
    "                            allow_unused=True,retain_graph=True,create_graph=True)[0]\n",
    "    # print('Du shape before reshaped', Du.shape)\n",
    "    Du=torch.reshape(Du,(Du.shape[0],dim,dim))\n",
    "    # print('Du shape after reshaped', Du.shape)\n",
    "\n",
    "  return Du\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def L_matrix(ite,x,model):  #x=[M,D]\n",
    "  L=torch.zeros(x.shape[0],dim,dim)\n",
    "\n",
    "  L[:,0,0]=model[ite](x)\n",
    "  for i in range(dim-1):\n",
    "    L[:,i+1,i+1]=nu[i]\n",
    "  return L\n",
    "\n",
    "\n",
    "def opt_quad():\n",
    "  return torch.pow(torch.sum(torch.pow(lamb,2)),0.5)/eta\n",
    "\n",
    "# def L_matrix_test(ite,x,model1):  #This matrix is a matrix with L00 = 0.6 and L11=nu\n",
    "#   L=torch.zeros(x.shape[0],dim,dim)\n",
    "#   L[:,0,0]=opt_quad()\n",
    "#   for i in range(dim-1):\n",
    "#     L[:,i+1,i+1]=nu[i]\n",
    "#   return L\n",
    "\n",
    "def L_matrix_test(x):  #This matrix is a matrix with L00 = 0.6 and L11=nu\n",
    "  L=torch.zeros(x.shape[0],dim,dim)\n",
    "  L[:,0,0]=opt_quad()\n",
    "  for i in range(dim-1):\n",
    "    L[:,i+1,i+1]=nu[i]\n",
    "  return L\n",
    "\n",
    "def V1(t,x,v):\n",
    "  # result=(1-torch.exp(-eta*x))*(torch.exp(-0.5*(T-t)*torch.sum(torch.pow(lamb,2))))\n",
    "  result=-torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
    "  return result\n",
    "\n",
    "def V(t,x,v): #output           |#input:\n",
    "  result=1-torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
    "  return result\n",
    "\n",
    "\n",
    "def V1(t,x,v):\n",
    "  # result=(1-torch.exp(-eta*x))*(torch.exp(-0.5*(T-t)*torch.sum(torch.pow(lamb,2))))\n",
    "  result=-torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
    "  return result\n",
    "\n",
    "def derV(t,x,v): #output           |#input:\n",
    "  result=eta*torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
    "  return result\n",
    "\n",
    "def hessianV(t,x,v): #output           |#input:\n",
    "  result=-torch.pow(eta,2)*torch.exp(-eta*x-0.5*(T-t)*torch.sum(torch.pow(lamb,2))).to(device)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving linear equations\n",
    "\n",
    "$\\begin{cases}-\\partial v_t - \\dfrac12 (\\sigma^\\intercal\\sigma): D^2v -\\mu\\cdot\\nabla v + f(t,x) =0\\\\\n",
    "v(T,x)=g(x)\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(t,x)$ has shape [num_samples,dim,dim]\n",
    "$dW_t$  has shape [num_samples,dim]\n",
    "\n",
    "We need to unsqueeze $dW_t$ before batch multiplication.\n",
    "\n",
    "    \n",
    "    torch.bmm(sigma(x[:,:,i]),dw[:, :, i]dw[:, :, i].unsqueeze(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigma(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(sigma, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(dim+1, 4),\n",
    "            nn.BatchNorm1d(num_features=4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4,dim*dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim])\n",
    "\n",
    "class mu(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(mu, self).__init__()\n",
    "        # self.linear_stack = nn.Sequential(\n",
    "        #     nn.Linear(dim+1, 4),\n",
    "        #     nn.BatchNorm1d(num_features=4),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(4,dim),\n",
    "        # )\n",
    "    def forward(self, x):\n",
    "        logits = torch.zeros([x.shape[0],dim])\n",
    "        return logits\n",
    "    \n",
    "class kappa(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(kappa, self).__init__()\n",
    "        # self.linear_stack = nn.Sequential(\n",
    "        #     nn.Linear(dim+1, 4),\n",
    "        #     nn.BatchNorm1d(num_features=4),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(4,dim),\n",
    "        # )\n",
    "    def forward(self, x):\n",
    "        logits = torch.zeros([x.shape[0],1])\n",
    "        return logits\n",
    "    \n",
    "class source(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(source, self).__init__()\n",
    "        # self.linear_stack = nn.Sequential(\n",
    "        #     nn.Linear(dim+1, 4),\n",
    "        #     nn.BatchNorm1d(num_features=4),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(4,dim),\n",
    "        # )\n",
    "    def forward(self, x):\n",
    "        logits = torch.zeros([x.shape[0],1])\n",
    "        return logits   \n",
    "    \n",
    "class terminal(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(terminal, self).__init__()\n",
    "        # self.linear_stack = nn.Sequential(\n",
    "        #     nn.Linear(dim+1, 4),\n",
    "        #     nn.BatchNorm1d(num_features=4),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(4,dim),\n",
    "        # )\n",
    "    def forward(self, x):\n",
    "        logits = -torch.tensor([0.5])*torch.bmm(x.unsqueeze(1),x.unsqueeze(2)).squeeze(2)\n",
    "        return logits \n",
    "    \n",
    "class Y(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(Y, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(dim, 4),\n",
    "            nn.BatchNorm1d(num_features=4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4,1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim])  \n",
    "    \n",
    "class Z(nn.Module): #input [M,D+1]   #output [M,1]\n",
    "    def __init__(self):\n",
    "        super(Z, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(dim+1, 4),\n",
    "            nn.BatchNorm1d(num_features=4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4,dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits#.reshape([dim,dim])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand((8,dim+1))\n",
    "s = sigma()\n",
    "m = mu()\n",
    "k = kappa()\n",
    "f = source()\n",
    "g = terminal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "class equation(object):\n",
    "    def __init__(self,sigma,mu,source,kappa,terminal,T,n,a,b,dim):\n",
    "        num_samples = 2**3\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma \n",
    "        self.kappa = kappa \n",
    "        self.source = source \n",
    "        self.n = n\n",
    "        self.x = torch.zeros((num_samples,dim+1,n+1))\n",
    "        self.Y0 = Y()\n",
    "        self.Z = Z()\n",
    "        self.dt = torch.tensor([T/n]).to(device)\n",
    "        dw = torch.randn(size=[num_samples, dim, n]).to(device)* torch.sqrt(self.dt)\n",
    "        self.x[:,1:,0]= a+(b-a)*torch.rand(num_samples,dim).to(device)\n",
    "        self.r = torch.ones((num_samples,1,n+1)).to(device)\n",
    "        self.c = torch.zeros((num_samples,1,n+1)).to(device)\n",
    "        self.sigmadw = torch.zeros((num_samples,dim,n)).to(device)\n",
    "        self.Zsigmadw = torch.zeros((num_samples,1,n)).to(device)\n",
    "        for i in range(self.n):\n",
    "            self.sigmadw[:,:,i] = torch.bmm(self.sigma(self.x[:,:,i]).reshape((num_samples,dim,dim)),dw[:, :, i].unsqueeze(2)).squeeze(2)\n",
    "            self.x[:,1:,i+1] = self.x[:,1:,i] + self.mu(self.x[:,:,i])*self.dt + self.sigmadw[:,:,i]\n",
    "            self.x[:,0,i+1] = self.x[:,0,i]+self.dt\n",
    "            self.r[:,:,i+1] = self.r[:,:,i]* torch.exp(-self.kappa(self.x[:,:,i])*self.dt)\n",
    "            if i == n - 1:\n",
    "                self.c[:,:,i+1] = terminal(self.x[:,1:,i+1])\n",
    "            else:\n",
    "                self.c[:,:,i] = self.source(self.x[:,:,i])\n",
    "        self.x = self.x.clone().detach()\n",
    "        self.r = self.r.clone().detach()\n",
    "        self.sigmadw = self.sigmadw.clone().detach()\n",
    "        \n",
    "    def train(self):\n",
    "        lr_ = 0.001\n",
    "        params = list(self.Y0.parameters()) + list(self.Z.parameters()) \n",
    "        opt=optim.Adam(params, lr=lr_)\n",
    "        num_epochs = 30\n",
    "        Y = self.Y0(self.x[:,1:,0])\n",
    "        for ep in range(num_epochs):\n",
    "            for i in range(self.n):    \n",
    "                self.Zsigmadw[:,:,i] = torch.bmm(self.Z(self.x[:,:,i]).unsqueeze(1),self.sigmadw[:,:,i].unsqueeze(2)).squeeze(2)\n",
    "                Y = (1+self.r[:,:,i]*self.dt) - self.c[:,:,i]*self.dt + self.Zsigmadw[:,:,i]\n",
    "            loss =  torch.pow(self.c[:,:,-1]-Y,2).mean()\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            print(loss)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.0])\n",
    "b = torch.tensor([1.0])\n",
    "heat = equation(s,m,f,k,g,T,num_time_interval,a,b,dim)\n",
    "# heat.Zsigmadw[:,:,1].shape#,heat.Zsigmadw[:,:,1].shape,torch.bmm(heat.Z(heat.x[:,:,1]).unsqueeze(1),heat.sigmadw[:,:,1].unsqueeze(2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[365], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mheat\u001b[38;5;241m.\u001b[39mr[:,:,i]\u001b[38;5;241m*\u001b[39mheat\u001b[38;5;241m.\u001b[39mdt) \u001b[38;5;241m-\u001b[39m heat\u001b[38;5;241m.\u001b[39mc[:,:,i]\u001b[38;5;241m*\u001b[39mheat\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m+\u001b[39m heat\u001b[38;5;241m.\u001b[39mZsigmadw[:,:,i]\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mpow(torch\u001b[38;5;241m.\u001b[39mzeros(size\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m-\u001b[39mY,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     12\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "lr_ = 0.001\n",
    "params = list(heat.Y0.parameters()) + list(heat.Z.parameters()) \n",
    "opt=optim.Adam(params, lr=lr_)\n",
    "num_epochs = 30\n",
    "Y = heat.Y0(heat.x[:,1:,0])\n",
    "for ep in range(num_epochs):\n",
    "    for i in range(heat.n):    \n",
    "        heat.Zsigmadw[:,:,i] = torch.bmm(heat.Z(heat.x[:,:,i]).unsqueeze(1),heat.sigmadw[:,:,i].unsqueeze(2)).squeeze(2)\n",
    "        Y = Y*(1+heat.r[:,:,i]*heat.dt) - heat.c[:,:,i]*heat.dt + heat.Zsigmadw[:,:,i]\n",
    "    loss =  torch.pow(torch.zeros(size=[8,1]).to(device)-Y,2).mean()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(heat.x[:,:,-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = heat.x[:,1:,-1]\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6211]],\n",
       "\n",
       "        [[-0.2861]],\n",
       "\n",
       "        [[-0.0533]],\n",
       "\n",
       "        [[-0.0836]],\n",
       "\n",
       "        [[-0.9971]],\n",
       "\n",
       "        [[-0.0415]],\n",
       "\n",
       "        [[-0.3707]],\n",
       "\n",
       "        [[-0.6233]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
